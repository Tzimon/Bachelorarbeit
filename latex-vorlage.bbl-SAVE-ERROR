% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nyt/global//global/global}
    \entry{aaditAcceleratingAdaptiveParallel2023}{inproceedings}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=39681f8bcd036ca61ec4e9bba00c64c5}{%
           family={Aadit},
           familyi={A\bibinitperiod},
           given={Navid\bibnamedelima Anjum},
           giveni={N\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=1,uniquepart=given,hash=3660f6bf9d4821aa4c362d0abd316e4f}{%
           family={Mohseni},
           familyi={M\bibinitperiod},
           given={Masoud},
           giveni={M\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=fa3bcaa71e30612e4ba4ec7f556062ee}{%
           family={Camsari},
           familyi={C\bibinitperiod},
           given={Kerem\bibnamedelima Y.},
           giveni={K\bibinitperiod\bibinitdelim Y\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{901a91fe042285abc38152c0c5800b1e}
      \strng{fullhash}{901a91fe042285abc38152c0c5800b1e}
      \strng{bibnamehash}{901a91fe042285abc38152c0c5800b1e}
      \strng{authorbibnamehash}{901a91fe042285abc38152c0c5800b1e}
      \strng{authornamehash}{901a91fe042285abc38152c0c5800b1e}
      \strng{authorfullhash}{901a91fe042285abc38152c0c5800b1e}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Special-purpose hardware to solve optimization problems formulated as Ising models has generated great excitement recently. Despite a large diversity in hardware, most solvers employ standard variations of the classical (simulated) annealing (CA) algorithm. Here, we show how powerful replica-based Parallel Tempering (PT) algorithms can significantly outperform CA, using FPGA-based probabilistic computers. Using a massively parallel (graph-colored) architecture, we implement the Adaptive PT (APT) algorithm, generating problem-dependent temperature profiles to equalize replica swap probabilities. We benchmark our p-computer against analytical results from classical Ising theory and use our machine to solve spin-glass instances formulated as hard optimization problems. APT outperforms heuristic choices of temperature profiles used in conventional PT and a replica-based version of CA. Our machine provides 6,000X speedup over optimized CPU, with orders of magnitude further speedup projected for scaled implementations. The developed co-design techniques may be useful for a broad range of Ising machines beyond p-computers.}
      \field{booktitle}{2023 {{IEEE Symposium}} on {{VLSI Technology}} and {{Circuits}} ({{VLSI Technology}} and {{Circuits}})}
      \field{eventtitle}{2023 {{IEEE Symposium}} on {{VLSI Technology}} and {{Circuits}} ({{VLSI Technology}} and {{Circuits}})}
      \field{issn}{2158-9682}
      \field{month}{6}
      \field{title}{Accelerating {{Adaptive Parallel Tempering}} with {{FPGA-based}} p-Bits}
      \field{urlday}{10}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 2}
      \range{pages}{2}
      \verb{doi}
      \verb 10.23919/VLSITechnologyandCir57934.2023.10185207
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\UEWGDRCY\\Aadit et al_2023_Accelerating Adaptive Parallel Tempering with FPGA-based p-bits.pdf;C\:\\Users\\simon\\Zotero\\storage\\NMUS4SFU\\10185207.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/10185207
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/10185207
      \endverb
      \keyw{Computational modeling,Computer architecture,Computers,Probabilistic logic,Simulated annealing,Temperature distribution,Very large scale integration}
    \endentry
    \entry{abarAgentBasedModelling2017}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=7e15667358247d5e30b66b340af38fc2}{%
           family={Abar},
           familyi={A\bibinitperiod},
           given={Sameera},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=841e53e83a59cc6ada71e94f1e44e3fb}{%
           family={Theodoropoulos},
           familyi={T\bibinitperiod},
           given={Georgios\bibnamedelima K.},
           giveni={G\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2163deaed8948d64eabfb4ab294a3e00}{%
           family={Lemarinier},
           familyi={L\bibinitperiod},
           given={Pierre},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6b59395f6552bc9ee2c939ed97c5c397}{%
           family={O’Hare},
           familyi={O\bibinitperiod},
           given={Gregory\bibnamedelimb M.\bibnamedelimi P.},
           giveni={G\bibinitperiod\bibinitdelim M\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{6dbd904e53312002147ae9e334a939ed}
      \strng{fullhash}{33e108fe7d838e73acc5873afbcfada1}
      \strng{bibnamehash}{33e108fe7d838e73acc5873afbcfada1}
      \strng{authorbibnamehash}{33e108fe7d838e73acc5873afbcfada1}
      \strng{authornamehash}{6dbd904e53312002147ae9e334a939ed}
      \strng{authorfullhash}{33e108fe7d838e73acc5873afbcfada1}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The key intent of this work is to present a comprehensive comparative literature survey of the state-of-art in software agent-based computing technology and its incorporation within the modelling and simulation domain. The original contribution of this survey is two-fold: (1) Present a concise characterization of almost the entire spectrum of agent-based modelling and simulation tools, thereby highlighting the salient features, merits, and shortcomings of such multi-faceted application software; this article covers eighty five agent-based toolkits that may assist the system designers and developers with common tasks, such as constructing agent-based models and portraying the real-time simulation outputs in tabular/graphical formats and visual recordings. (2) Provide a usable reference that aids engineers, researchers, learners and academicians in readily selecting an appropriate agent-based modelling and simulation toolkit for designing and developing their system models and prototypes, cognizant of both their expertise and those requirements of their application domain. In a nutshell, a significant synthesis of Agent Based Modelling and Simulation (ABMS) resources has been performed in this review that stimulates further investigation into this topic.}
      \field{day}{1}
      \field{issn}{1574-0137}
      \field{journaltitle}{Computer Science Review}
      \field{month}{5}
      \field{shortjournal}{Computer Science Review}
      \field{shorttitle}{Agent {{Based Modelling}} and {{Simulation}} Tools}
      \field{title}{Agent {{Based Modelling}} and {{Simulation}} Tools: {{A}} Review of the State-of-Art Software}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{volume}{24}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{13\bibrangedash 33}
      \range{pages}{21}
      \verb{doi}
      \verb 10.1016/j.cosrev.2017.03.001
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\JPPXWS87\S1574013716301198.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S1574013716301198
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S1574013716301198
      \endverb
      \keyw{Agent Based Modelling and Simulation (ABMS) tools,Artificial life / social science simulations,Modelling complex systems,Multi-agent computing,Software agent,Swarm intelligence}
    \endentry
    \entry{ackleyLearningAlgorithmBoltzmann1985}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=d75e80fc3f251b0d87ace30e48140f35}{%
           family={Ackley},
           familyi={A\bibinitperiod},
           given={David\bibnamedelima H.},
           giveni={D\bibinitperiod\bibinitdelim H\bibinitperiod},
           givenun=0}}%
        {{un=1,uniquepart=given,hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E.},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
           givenun=1}}%
        {{un=1,uniquepart=given,hash=2df660039cf6a702dffb22b64106829f}{%
           family={Sejnowski},
           familyi={S\bibinitperiod},
           given={Terrence\bibnamedelima J.},
           giveni={T\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=1}}%
      }
      \strng{namehash}{3bec7e9a6f8b0ff103785ca67fc3866c}
      \strng{fullhash}{3bec7e9a6f8b0ff103785ca67fc3866c}
      \strng{bibnamehash}{3bec7e9a6f8b0ff103785ca67fc3866c}
      \strng{authorbibnamehash}{3bec7e9a6f8b0ff103785ca67fc3866c}
      \strng{authornamehash}{3bec7e9a6f8b0ff103785ca67fc3866c}
      \strng{authorfullhash}{3bec7e9a6f8b0ff103785ca67fc3866c}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.}
      \field{day}{1}
      \field{issn}{0364-0213}
      \field{journaltitle}{Cognitive Science}
      \field{month}{1}
      \field{number}{1}
      \field{shortjournal}{Cognitive Science}
      \field{title}{A Learning Algorithm for Boltzmann Machines}
      \field{urlday}{16}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{9}
      \field{year}{1985}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{147\bibrangedash 169}
      \range{pages}{23}
      \verb{doi}
      \verb 10.1016/S0364-0213(85)80012-4
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\42UB7SBL\Ackley et al_1985_A learning algorithm for boltzmann machines.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0364021385800124
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0364021385800124
      \endverb
      \keyw{ungelesen,zitiert}
    \endentry
    \entry{ahadNeuralNetworksWireless2016}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=5a5a37313a709f9c58f99cf499f342b5}{%
           family={Ahad},
           familyi={A\bibinitperiod},
           given={Nauman},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f55026359def906ce7eb1096c07d28a2}{%
           family={Qadir},
           familyi={Q\bibinitperiod},
           given={Junaid},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b614ce435177fff6212f97962007591a}{%
           family={Ahsan},
           familyi={A\bibinitperiod},
           given={Nasir},
           giveni={N\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{005bd4d8b782b81c413df24260bad960}
      \strng{fullhash}{005bd4d8b782b81c413df24260bad960}
      \strng{bibnamehash}{005bd4d8b782b81c413df24260bad960}
      \strng{authorbibnamehash}{005bd4d8b782b81c413df24260bad960}
      \strng{authornamehash}{005bd4d8b782b81c413df24260bad960}
      \strng{authorfullhash}{005bd4d8b782b81c413df24260bad960}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The design of modern wireless networks, which involves decision making and parameter optimization, is quite challenging due to the highly dynamic, and often unknown, environmental conditions that characterize wireless networks. There is a common trend in modern networks to incorporate artificial intelligence (AI) techniques to cope with this design complexity. While a number of AI techniques have been profitably employed in the wireless networks community, the well-established AI framework of neural networks (NNs), well known for their remarkable generality and versatility, has been applied in a wide variety of settings in wireless networks. In particular, NNs are especially popular for tasks involving classification, learning, or optimization. In this paper, we provide both an exposition of common NN models and a comprehensive survey of the applications of NNs in wireless networks. We also identify pitfalls and challenges of implementing NNs especially when we consider alternative AI models and techniques. While various surveys on NNs exist in the literature, our paper is the first paper, to the best of our knowledge, which focuses on the applications of NNs in wireless networks.}
      \field{day}{1}
      \field{issn}{1084-8045}
      \field{journaltitle}{Journal of Network and Computer Applications}
      \field{month}{6}
      \field{shortjournal}{Journal of Network and Computer Applications}
      \field{shorttitle}{Neural Networks in Wireless Networks}
      \field{title}{Neural Networks in Wireless Networks: {{Techniques}}, Applications and Guidelines}
      \field{urlday}{28}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{68}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 27}
      \range{pages}{27}
      \verb{doi}
      \verb 10.1016/j.jnca.2016.04.006
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\I74EW2SD\S1084804516300492.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S1084804516300492
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S1084804516300492
      \endverb
      \keyw{Artificial intelligence,Computational intelligence,Neural networks,Wireless networks}
    \endentry
    \entry{ahmadOptimizingHardwareAccelerated2020}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=724826babf3d432a024d73f2ae839a72}{%
           family={Ahmad},
           familyi={A\bibinitperiod},
           given={Afzal},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d1b941da543b8d0a3e4ff6582138c4a7}{%
           family={Pasha},
           familyi={P\bibinitperiod},
           given={Muhammad\bibnamedelima Adeel},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{e396f238573ba0523c425470084c3c77}
      \strng{fullhash}{e396f238573ba0523c425470084c3c77}
      \strng{bibnamehash}{e396f238573ba0523c425470084c3c77}
      \strng{authorbibnamehash}{e396f238573ba0523c425470084c3c77}
      \strng{authornamehash}{e396f238573ba0523c425470084c3c77}
      \strng{authorfullhash}{e396f238573ba0523c425470084c3c77}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convolution is inarguably the most complex operation utilized in Convolutional Neural Networks (convnets). Owing to the billions of independent multiply-adds involved, convolution is being massively parallelized by the simultaneous utilization of many cores of Graphical Processing Units (GPUs). Although GPUs have shown significant performance improvements in both training and inference stages, they are not well-suited for mobile vision applications where both energy and real-time constraints need to be satisfied. In contrast, Field Programmable Gate Arrays (FPGAs) have demonstrated massive parallelization capabilities, with fast DSPs and on-chip memory, at a lower energy cost than GPUs. Hence, they are being utilized to design convnet accelerators for embedded applications. In this brief, we design an FPGA-based accelerator for general matrix-matrix multiplication (GeMM) to improve the efficiency of convolutional layers of Shufflenet, an efficient convnet architecture. Experimental results show significant performance improvements against the state-of-the-art FPGA-based implementations of both efficient convnets that are tailored towards mobile vision applications, and complex convnets that are used in traditional applications.}
      \field{eventtitle}{{{IEEE Transactions}} on {{Circuits}} and {{Systems II}}: {{Express Briefs}}}
      \field{issn}{1558-3791}
      \field{journaltitle}{IEEE Transactions on Circuits and Systems II: Express Briefs}
      \field{month}{11}
      \field{number}{11}
      \field{title}{Optimizing {{Hardware Accelerated General Matrix-Matrix Multiplication}} for {{CNNs}} on {{FPGAs}}}
      \field{urlday}{18}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{67}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{2692\bibrangedash 2696}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1109/TCSII.2020.2965154
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\RWTCIR59\\Ahmad_Pasha_2020_Optimizing Hardware Accelerated General Matrix-Matrix Multiplication for CNNs.pdf;C\:\\Users\\simon\\Zotero\\storage\\GN8KR79F\\8954788.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/8954788?casa_token=FflZ-99s30MAAAAA:l-4hcDRIsH2x9gRN3bGMy8BAo1nbQbrJEhqZpdRnAR5IJSe2naviSLmKFiAuYV_yuWVlAPPPdbs
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/8954788?casa_token=FflZ-99s30MAAAAA:l-4hcDRIsH2x9gRN3bGMy8BAo1nbQbrJEhqZpdRnAR5IJSe2naviSLmKFiAuYV_yuWVlAPPPdbs
      \endverb
      \keyw{Complexity theory,Computer architecture,convnets,Convolution,FPGAs,Hardware,Hardware acceleration,Kernel,Shape,Tensors}
    \endentry
    \entry{ahmedPrototypingFrameworkHumanCentered2021}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=3132e35c1d9fb8e6c977b3606c9b7b81}{%
           family={Ahmed},
           familyi={A\bibinitperiod},
           given={Salman},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8c517e434c8e3d03710b7ffaef8c51d3}{%
           family={Demirel},
           familyi={D\bibinitperiod},
           given={H.\bibnamedelimi Onan},
           giveni={H\bibinitperiod\bibinitdelim O\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{3}{}{%
        {{hash=01a73a441b7ee66c61f291cfe4d34cb1}{%
           family={Soares},
           familyi={S\bibinitperiod},
           given={Marcelo\bibnamedelima M.},
           giveni={M\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=2e44e3517d7c279fb135a104cefeb0d2}{%
           family={Rosenzweig},
           familyi={R\bibinitperiod},
           given={Elizabeth},
           giveni={E\bibinitperiod}}}%
        {{hash=28e4f412995cd32168f059866616faf0}{%
           family={Marcus},
           familyi={M\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{cfa5c45c622b49a867ecc95d1e609f11}
      \strng{fullhash}{cfa5c45c622b49a867ecc95d1e609f11}
      \strng{bibnamehash}{cfa5c45c622b49a867ecc95d1e609f11}
      \strng{authorbibnamehash}{cfa5c45c622b49a867ecc95d1e609f11}
      \strng{authornamehash}{cfa5c45c622b49a867ecc95d1e609f11}
      \strng{authorfullhash}{cfa5c45c622b49a867ecc95d1e609f11}
      \strng{editorbibnamehash}{402b09c2d1579225f4aaddcc6ea13e19}
      \strng{editornamehash}{402b09c2d1579225f4aaddcc6ea13e19}
      \strng{editorfullhash}{402b09c2d1579225f4aaddcc6ea13e19}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Prototyping is a crucial aspect of the product design and development process. Successful development and deployment of products hinge on building correct prototypes. However, current prototyping frameworks often do not provide specific prototyping guidelines that are practical and readily applicable and do not consider Human Factors Engineering (HFE) design guidelines thoroughly. This paper presents a novel prototyping framework that integrates prototyping and HFE guidelines to address the research gap in the prototyping literature. The prototyping framework presents a database in the form of a graphical user interface (GUI), which contains userform data entry. The GUI userform helps the designer to develop prototyping strategies based on the prototyping best practices and HFE principles. Further, the GUI userform suggests tools and technologies that can be used to fabricate the prototype. Thus, the framework helps designers plan prototyping activities and reduces the reliance on intuition when fabricating prototypes. This paper also presents a preliminary validation study that focuses on exploring whether there is a statistical difference between the intervention and control group in developing prototyping strategies for various prototyping problems. The intervention and control group are tested using twelve prototyping problems, and the Prototyping Success score is measured for each group. An independent sample t-test is performed. From the statistical analysis, it can be inferred that the participants who use the prototyping framework produce mean Prototyping Success scores that are higher than that of the control group.}
      \field{booktitle}{Design, {{User Experience}}, and {{Usability}}: {{UX Research}} and {{Design}}}
      \field{isbn}{978-3-030-78221-4}
      \field{langid}{english}
      \field{shorttitle}{A {{Prototyping Framework}} for {{Human-Centered Product Design}}}
      \field{title}{A {{Prototyping Framework}} for {{Human-Centered Product Design}}: {{Preliminary Validation Study}}}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{pages}{3\bibrangedash 14}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1007/978-3-030-78221-4_1
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\GM5EJ3MG\Ahmed_Demirel_2021_A Prototyping Framework for Human-Centered Product Design.pdf
      \endverb
      \keyw{Design,Digital human modeling,Ergonomics,Human factors engineering,Human-centered design,Prototyping framework,Validation}
    \endentry
    \entry{amariInformationGeometryBoltzmann1992}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=38a01cc459979a21bdbfea2e3460c5a6}{%
           family={Amari},
           familyi={A\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a6f327746616b456d289390e2f1aa9f8}{%
           family={Kurata},
           familyi={K\bibinitperiod},
           given={K.},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9b237999cea4eb51604f2ff2faa1ce27}{%
           family={Nagaoka},
           familyi={N\bibinitperiod},
           given={H.},
           giveni={H\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{c018a2a076df755de98e563083704b8e}
      \strng{fullhash}{c018a2a076df755de98e563083704b8e}
      \strng{bibnamehash}{c018a2a076df755de98e563083704b8e}
      \strng{authorbibnamehash}{c018a2a076df755de98e563083704b8e}
      \strng{authornamehash}{c018a2a076df755de98e563083704b8e}
      \strng{authorfullhash}{c018a2a076df755de98e563083704b8e}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A Boltzmann machine is a network of stochastic neurons. The set of all the Boltzmann machines with a fixed topology forms a geometric manifold of high dimension, where modifiable synaptic weights of connections play the role of a coordinate system to specify networks. A learning trajectory, for example, is a curve in this manifold. It is important to study the geometry of the neural manifold, rather than the behavior of a single network, in order to know the capabilities and limitations of neural networks of a fixed topology. Using the new theory of information geometry, a natural invariant Riemannian metric and a dual pair of affine connections on the Boltzmann neural network manifold are established. The meaning of geometrical structures is elucidated from the stochastic and the statistical point of view. This leads to a natural modification of the Boltzmann machine learning rule.{$<>$}}
      \field{eventtitle}{{{IEEE Transactions}} on {{Neural Networks}}}
      \field{issn}{1941-0093}
      \field{journaltitle}{IEEE Transactions on Neural Networks}
      \field{month}{3}
      \field{number}{2}
      \field{title}{Information Geometry of {{Boltzmann}} Machines}
      \field{urlday}{16}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{3}
      \field{year}{1992}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{260\bibrangedash 271}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1109/72.125867
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\5Q29TWP2\Information_geometry_of_Boltzmann_machines.pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/125867
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/125867
      \endverb
      \keyw{Computer architecture,Information geometry,Information processing,Machine learning,Manifolds,Network topology,Neural networks,Neurons,Probability distribution,Stochastic processes,ungelesen,zitiert}
    \endentry
    \entry{amirsoleimaniInMemoryVectorMatrixMultiplication2020}{article}{}
      \name{author}{9}{}{%
        {{un=0,uniquepart=base,hash=fe8e1be182a4d879fa02f73523ccff1a}{%
           family={Amirsoleimani},
           familyi={A\bibinitperiod},
           given={Amirali},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4a0a99d9aac0cdeb27a696e76be62551}{%
           family={Alibart},
           familyi={A\bibinitperiod},
           given={Fabien},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1f542c97f483fdc1d4681c0949e0a297}{%
           family={Yon},
           familyi={Y\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7134fc96f02d6e6c40c2e17689208b55}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Jianxiong},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e0e65bdea48c750f875447a680b8f0c9}{%
           family={Pazhouhandeh},
           familyi={P\bibinitperiod},
           given={M.\bibnamedelimi Reza},
           giveni={M\bibinitperiod\bibinitdelim R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fd52e3aa16844d480f9f665d84f898b8}{%
           family={Ecoffey},
           familyi={E\bibinitperiod},
           given={Serge},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c11d91afe582122a3bdbba97f2b937d5}{%
           family={Beilliard},
           familyi={B\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=51706b61af04d6f1a70e96335cc55761}{%
           family={Genov},
           familyi={G\bibinitperiod},
           given={Roman},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2274efaac27cae844c56190b84632a0d}{%
           family={Drouin},
           familyi={D\bibinitperiod},
           given={Dominique},
           giveni={D\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{a60cb42bc1eb852d760440ff4d8faa13}
      \strng{fullhash}{22538c39aaa0b89da22b9345d8259540}
      \strng{bibnamehash}{22538c39aaa0b89da22b9345d8259540}
      \strng{authorbibnamehash}{22538c39aaa0b89da22b9345d8259540}
      \strng{authornamehash}{a60cb42bc1eb852d760440ff4d8faa13}
      \strng{authorfullhash}{22538c39aaa0b89da22b9345d8259540}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The low communication bandwidth between memory and processing units in conventional von Neumann machines does not support the requirements of emerging applications that rely extensively on large sets of data. More recent computing paradigms, such as high parallelization and near-memory computing, help alleviate the data communication bottleneck to some extent, but paradigm-shifting concepts are required. In-memory computing has emerged as a prime candidate to eliminate this bottleneck by colocating memory and processing. In this context, resistive switching (RS) memory devices is a key promising choice, due to their unique intrinsic device-level properties, enabling both storing and computing with a small, massively-parallel footprint at low power. Theoretically, this directly translates to a major boost in energy efficiency and computational throughput, but various practical challenges remain. A qualitative and quantitative analysis of several key existing challenges in implementing high-capacity, high-volume RS memories for accelerating the most computationally demanding computation in machine learning (ML) inference, that of vector-matrix multiplication (VMM), is presented. The monolithic integration of RS memories with complementary metal–oxide–semiconductor (CMOS) integrated circuits is presented as the core underlying technology. The key existing design choices in terms of device-level physical implementation, circuit-level design, and system-level considerations is reviewed and an outlook for future directions is provided.}
      \field{issn}{2640-4567}
      \field{journaltitle}{Advanced Intelligent Systems}
      \field{langid}{english}
      \field{number}{11}
      \field{shorttitle}{In-{{Memory Vector-Matrix Multiplication}} in {{Monolithic Complementary Metal}}–{{Oxide}}–{{Semiconductor-Memristor Integrated Circuits}}}
      \field{title}{In-{{Memory Vector-Matrix Multiplication}} in {{Monolithic Complementary Metal}}–{{Oxide}}–{{Semiconductor-Memristor Integrated Circuits}}: {{Design Choices}}, {{Challenges}}, and {{Perspectives}}}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{2}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{2000115}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1002/aisy.202000115
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\SZ37ATSG\In‐Memory Vector‐Matrix Multiplication in Monolithic Complementary.pdf
      \endverb
      \verb{urlraw}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202000115
      \endverb
      \verb{url}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202000115
      \endverb
      \keyw{complementary metal–oxide–semiconductor,in-memory computing,inference,memristors,redox-based random access memories,resistive switching memories,ungelesen,vector-matrix multiplications}
    \endentry
    \entry{ASICDesignFlow}{online}{}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{url}
      \field{labeltitlesource}{title}
      \field{abstract}{The ASIC design flow is a time-tested mature methodology that combines multiple steps to form the backbone of every ASIC design project. All IC engineers have encountered the ASIC design flow in some sequence or another.}
      \field{langid}{american}
      \field{title}{{{ASIC Design Flow}} for {{VLSI Engineering Teams}} [{{GUIDE}}] - {{Xinyx Design}}}
      \field{urlday}{3}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{urldateera}{ce}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\QPQ9HMW7\asic-design-flow-for-vlsi-engineering-teams.html
      \endverb
      \verb{urlraw}
      \verb https://www.xinyxdesign.com/resources/asic-design-flow-for-vlsi-engineering-teams/
      \endverb
      \verb{url}
      \verb https://www.xinyxdesign.com/resources/asic-design-flow-for-vlsi-engineering-teams/
      \endverb
    \endentry
    \entry{babuReconfigurableFPGAArchitectures2021}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=cc380d1d99c98ef6f63272dca593fcb6}{%
           family={Babu},
           familyi={B\bibinitperiod},
           given={Praveenkumar},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6795a3530621b20bafa639d715437f1e}{%
           family={Parthasarathy},
           familyi={P\bibinitperiod},
           given={Eswaran},
           giveni={E\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{325bd983d0fe179833dd594081065a7c}
      \strng{fullhash}{325bd983d0fe179833dd594081065a7c}
      \strng{bibnamehash}{325bd983d0fe179833dd594081065a7c}
      \strng{authorbibnamehash}{325bd983d0fe179833dd594081065a7c}
      \strng{authornamehash}{325bd983d0fe179833dd594081065a7c}
      \strng{authorfullhash}{325bd983d0fe179833dd594081065a7c}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Reconfigurable computing is a potential paradigm which has been effectively performing mostly in the developments of devices likely Field Programmable Gate Arrays (FPGAs). This paper illustrates the reconfigurable architecture of FPGA and its types. Most widely used high-speed computation fabrics utilized in reconfigurable computing are FPGAs. This paper demonstrates the architectures used in reconfigurable computing and shows the various advantages of using reconfigurable computing design over conventional Application-Specific Integrated Circuits for achieving high level of performance for a desired application. The survey deals with the architecture of FPGAs and their types in detail. This paper also explains the highlights and challenges of fine-grained and coarse-grained architectures. FPGAs have supported partial reconfiguration over the few years. This survey also includes the partial reconfiguration techniques and the various applications of reconfigurability.}
      \field{day}{1}
      \field{issn}{2250-2114}
      \field{journaltitle}{Journal of The Institution of Engineers (India): Series B}
      \field{langid}{english}
      \field{month}{2}
      \field{number}{1}
      \field{shortjournal}{J. Inst. Eng. India Ser. B}
      \field{shorttitle}{Reconfigurable {{FPGA Architectures}}}
      \field{title}{Reconfigurable {{FPGA Architectures}}: {{A Survey}} and {{Applications}}}
      \field{urlday}{20}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{102}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{143\bibrangedash 156}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1007/s40031-020-00508-y
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\ED2JQJ53\Babu_Parthasarathy_2021_Reconfigurable FPGA Architectures.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s40031-020-00508-y
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s40031-020-00508-y
      \endverb
      \keyw{ASIC,Coarse-grained architecture,Fine-grained architecture,FPGA,Partial reconfiguration,Reconfigurable computing}
    \endentry
    \entry{baiEfficiencySystematicSurvey2024}{online}{}
      \name{author}{13}{}{%
        {{un=0,uniquepart=base,hash=23d9a6dc03689ab3301ebd908a248ef3}{%
           family={Bai},
           familyi={B\bibinitperiod},
           given={Guangji},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=97d8fed80608e46eecb5fcb8015eafc6}{%
           family={Chai},
           familyi={C\bibinitperiod},
           given={Zheng},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=df074f4606a794e1b0742c1341d69633}{%
           family={Ling},
           familyi={L\bibinitperiod},
           given={Chen},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8c23fdbe9467d8a9fb05caa4b245e273}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Shiyu},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=367fe0648744bb7b14c03f309076f9d3}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Jiaying},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=50541ce48b62a59478859829bdda8bad}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Nan},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1fe9d7eee79870b05fbfc687d3b14490}{%
           family={Shi},
           familyi={S\bibinitperiod},
           given={Tingwei},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7007785cf1d2fc3671f5044bab070c4d}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Ziyang},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b519b2e6bde9eadfe69c9e41a428b5c1}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Mengdan},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=417dbad4e47345638c314aae00957100}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Yifei},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1eae00cb583e941d0d1d59f23e11f581}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Carl},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0095a25a0f65884c2573dc71d0269cc7}{%
           family={Cheng},
           familyi={C\bibinitperiod},
           given={Yue},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1f1b5a611871b013dd18b84a54bbb23f}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Liang},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{181a9af44738afaae025245ec6870519}
      \strng{fullhash}{20e3ebabaa311cc86b5936e734500002}
      \strng{bibnamehash}{20e3ebabaa311cc86b5936e734500002}
      \strng{authorbibnamehash}{20e3ebabaa311cc86b5936e734500002}
      \strng{authornamehash}{181a9af44738afaae025245ec6870519}
      \strng{authorfullhash}{20e3ebabaa311cc86b5936e734500002}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between various resources and corresponding optimization techniques. A standardized set of evaluation metrics and datasets is also presented to facilitate consistent and fair comparisons across different models and techniques. By offering a comprehensive overview of the current sota and identifying open research avenues, this survey serves as a foundational reference for researchers and practitioners, aiding them in developing more sustainable and efficient LLMs in a rapidly evolving landscape.}
      \field{day}{3}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{1}
      \field{pubstate}{preprint}
      \field{shorttitle}{Beyond {{Efficiency}}}
      \field{title}{Beyond {{Efficiency}}: {{A Systematic Survey}} of {{Resource-Efficient Large Language Models}}}
      \field{urlday}{23}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2401.00625
      \endverb
      \verb{eprint}
      \verb 2401.00625
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\YPEGVJL8\\Bai et al_2024_Beyond Efficiency.pdf;C\:\\Users\\simon\\Zotero\\storage\\XTGM8EVM\\2401.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2401.00625
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2401.00625
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{baischerLearningHardwareTutorial2021}{online}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=f19cfbd2dfcdb98b0f0e4828ac34c599}{%
           family={Baischer},
           familyi={B\bibinitperiod},
           given={Lukas},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=255d9141d39b3469b0274feb006b8204}{%
           family={Wess},
           familyi={W\bibinitperiod},
           given={Matthias},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e5c4e4a75192c2ef494c16cdb8490ab9}{%
           family={TaheriNejad},
           familyi={T\bibinitperiod},
           given={Nima},
           giveni={N\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{7eb4a39fc2c7322a396449d3115dad77}
      \strng{fullhash}{7eb4a39fc2c7322a396449d3115dad77}
      \strng{bibnamehash}{7eb4a39fc2c7322a396449d3115dad77}
      \strng{authorbibnamehash}{7eb4a39fc2c7322a396449d3115dad77}
      \strng{authornamehash}{7eb4a39fc2c7322a396449d3115dad77}
      \strng{authorfullhash}{7eb4a39fc2c7322a396449d3115dad77}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep neural networks (DNNs) have the advantage that they can take into account a large number of parameters, which enables them to solve complex tasks. In computer vision and speech recognition, they have a better accuracy than common algorithms, and in some tasks, they boast an even higher accuracy than human experts. With the progress of DNNs in recent years, many other fields of application such as diagnosis of diseases and autonomous driving are taking advantage of them. The trend at DNNs is clear: The network size is growing exponentially, which leads to an exponential increase in computational effort and required memory size. For this reason, optimized hardware accelerators are used to increase the performance of the inference of neuronal networks. However, there are various neural network hardware accelerator platforms, such as graphics processing units (GPUs), application specific integrated circuits (ASICs) and field programmable gate arrays (FPGAs). Each of these platforms offer certain advantages and disadvantages. Also, there are various methods for reducing the computational effort of DNNs, which are differently suitable for each hardware accelerator. In this article an overview of existing neural network hardware accelerators and acceleration methods is given. Their strengths and weaknesses are shown and a recommendation of suitable applications is given. In particular, we focus on acceleration of the inference of convolutional neural networks (CNNs) used for image recognition tasks. Given that there exist many different hardware architectures. FPGA-based implementations are well-suited to show the effect of DNN optimization methods on accuracy and throughput. For this reason, the focus of this work is more on FPGA-based implementations.}
      \field{day}{19}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{4}
      \field{pubstate}{preprint}
      \field{shorttitle}{Learning on {{Hardware}}}
      \field{title}{Learning on {{Hardware}}: {{A Tutorial}} on {{Neural Network Accelerators}} and {{Co-Processors}}}
      \field{urlday}{18}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2104.09252
      \endverb
      \verb{eprint}
      \verb 2104.09252
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\PDJA9RTT\\Baischer et al_2021_Learning on Hardware.pdf;C\:\\Users\\simon\\Zotero\\storage\\KSCY6NQ4\\2104.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2104.09252
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2104.09252
      \endverb
      \keyw{Computer Science - Hardware Architecture,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
    \endentry
    \entry{barraEquivalenceHopfieldNetworks2012}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=47b34dc63f08dbd00ef6743d67e224cc}{%
           family={Barra},
           familyi={B\bibinitperiod},
           given={Adriano},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cd44f8b10dfba5c99f46658c246a520a}{%
           family={Bernacchia},
           familyi={B\bibinitperiod},
           given={Alberto},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dfefb16b409c10caae227b7068c16fbd}{%
           family={Santucci},
           familyi={S\bibinitperiod},
           given={Enrica},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ad5c54d53300337a4815b5f6d901679e}{%
           family={Contucci},
           familyi={C\bibinitperiod},
           given={Pierluigi},
           giveni={P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{684a66abc02a03c1401235d2f13aad6c}
      \strng{fullhash}{84453ff33f69ae5de5103612d4dd451e}
      \strng{bibnamehash}{84453ff33f69ae5de5103612d4dd451e}
      \strng{authorbibnamehash}{84453ff33f69ae5de5103612d4dd451e}
      \strng{authornamehash}{684a66abc02a03c1401235d2f13aad6c}
      \strng{authorfullhash}{84453ff33f69ae5de5103612d4dd451e}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A specific type of neural networks, the Restricted Boltzmann Machines (RBM), are implemented for classification and feature detection in machine learning. They are characterized by separate layers of visible and hidden units, which are able to learn efficiently a generative model of the observed data. We study a “hybrid” version of RBMs, in which hidden units are analog and visible units are binary, and we show that thermodynamics of visible units are equivalent to those of a Hopfield network, in which the N visible units are the neurons and the P hidden units are the learned patterns. We apply the method of stochastic stability to derive the thermodynamics of the model, by considering a formal extension of this technique to the case of multiple sets of stored patterns, which may act as a benchmark for the study of correlated sets. Our results imply that simulating the dynamics of a Hopfield network, requiring the update of N neurons and the storage of N(N−1)/2 synapses, can be accomplished by a hybrid Boltzmann Machine, requiring the update of N+P neurons but the storage of only NP synapses. In addition, the well known glass transition of the Hopfield network has a counterpart in the Boltzmann Machine: it corresponds to an optimum criterion for selecting the relative sizes of the hidden and visible layers, resolving the trade-off between flexibility and generality of the model. The low storage phase of the Hopfield model corresponds to few hidden units and hence a overly constrained RBM, while the spin-glass phase (too many hidden units) corresponds to unconstrained RBM prone to overfitting of the observed data.}
      \field{day}{1}
      \field{issn}{0893-6080}
      \field{journaltitle}{Neural Networks}
      \field{month}{10}
      \field{shortjournal}{Neural Networks}
      \field{title}{On the Equivalence of {{Hopfield}} Networks and {{Boltzmann Machines}}}
      \field{urlday}{16}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{34}
      \field{year}{2012}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 9}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1016/j.neunet.2012.06.003
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\FBU8CDS7\\On the equivalence of Hopfield networks and Boltzmann Machines.pdf;C\:\\Users\\simon\\Zotero\\storage\\NFHUABGB\\S0893608012001608.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0893608012001608
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0893608012001608
      \endverb
      \keyw{Boltzmann Machines,Hopfield networks,Statistical mechanics,ungelesen}
    \endentry
    \entry{baskervilleDesignScienceResearch2018}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=6364a5db3195777af09140358fdf8c14}{%
           family={Baskerville},
           familyi={B\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b3a00a1597cbdf206cb87120f3593486}{%
           family={Baiyere},
           familyi={B\bibinitperiod},
           given={Abayomi},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6cb3773d13196927d56a5877ea24b6a2}{%
           family={Gregor},
           familyi={G\bibinitperiod},
           given={Shirley},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d9ddca3b1b7af0f96d62a62424bd939a}{%
           family={Hevner},
           familyi={H\bibinitperiod},
           given={Alan},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=17ba57d6ffcdf7c38bb96e0b62e0425d}{%
           family={Rossi},
           familyi={R\bibinitperiod},
           given={Matti},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{aac7a28f40c6e025b248499f81829ebc}
      \strng{fullhash}{99572c8678e85e614fc975f584ef411f}
      \strng{bibnamehash}{99572c8678e85e614fc975f584ef411f}
      \strng{authorbibnamehash}{99572c8678e85e614fc975f584ef411f}
      \strng{authornamehash}{aac7a28f40c6e025b248499f81829ebc}
      \strng{authorfullhash}{99572c8678e85e614fc975f584ef411f}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{day}{31}
      \field{issn}{1536-9323}
      \field{journaltitle}{Journal of the Association for Information Systems}
      \field{month}{5}
      \field{number}{5}
      \field{shorttitle}{Design {{Science Research Contributions}}}
      \field{title}{Design {{Science Research Contributions}}: {{Finding}} a {{Balance}} between {{Artifact}} and {{Theory}}}
      \field{volume}{19}
      \field{year}{2018}
      \field{dateera}{ce}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\VSSGDVAL\3.html
      \endverb
      \verb{urlraw}
      \verb https://aisel.aisnet.org/jais/vol19/iss5/3
      \endverb
      \verb{url}
      \verb https://aisel.aisnet.org/jais/vol19/iss5/3
      \endverb
    \endentry
    \entry{beichlMetropolisAlgorithm2000}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=060caa4c8d44a9b84620ae0f4990684e}{%
           family={Beichl},
           familyi={B\bibinitperiod},
           given={I.},
           giveni={I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=25526416d828bcd136e73a765f2677c8}{%
           family={Sullivan},
           familyi={S\bibinitperiod},
           given={F.},
           giveni={F\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{b52e75707df3d853da7dffce76f2cf8d}
      \strng{fullhash}{b52e75707df3d853da7dffce76f2cf8d}
      \strng{bibnamehash}{b52e75707df3d853da7dffce76f2cf8d}
      \strng{authorbibnamehash}{b52e75707df3d853da7dffce76f2cf8d}
      \strng{authornamehash}{b52e75707df3d853da7dffce76f2cf8d}
      \strng{authorfullhash}{b52e75707df3d853da7dffce76f2cf8d}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The Metropolis Algorithm has been the most successful and influential of all the members of the computational species that used to be called the "Monte Carlo method". Today, topics related to this algorithm constitute an entire field of computational science supported by a deep theory and having applications ranging from physical simulations to the foundations of computational complexity. Since the rejection method invention (J. von Neumann), it has been developed extensively and applied in a wide variety of settings. The Metropolis Algorithm can be formulated as an instance of the rejection method used for generating steps in a Markov chain.}
      \field{eventtitle}{Computing in {{Science}} \& {{Engineering}}}
      \field{issn}{1558-366X}
      \field{journaltitle}{Computing in Science \& Engineering}
      \field{month}{1}
      \field{number}{1}
      \field{title}{The {{Metropolis Algorithm}}}
      \field{urlday}{27}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{2}
      \field{year}{2000}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{65\bibrangedash 69}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1109/5992.814660
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\HWDNSHCW\\Beichl_Sullivan_2000_The Metropolis Algorithm.pdf;C\:\\Users\\simon\\Zotero\\storage\\ZTYBSUIQ\\814660.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/814660
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/814660
      \endverb
      \keyw{Computational complexity,Computational modeling,Distributed computing,Distribution functions,Hospitals,Monte Carlo methods,Physics computing,Probability distribution,Sampling methods}
    \endentry
    \entry{bellettiJanusFPGABasedSystem2009}{article}{}
      \name{author}{23}{}{%
        {{un=0,uniquepart=base,hash=9cd335cb01e09255ddd3ff4b754fa169}{%
           family={Belletti},
           familyi={B\bibinitperiod},
           given={Francesco},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=80199e15f80b79d1897b8ebe2a965c27}{%
           family={Cotallo},
           familyi={C\bibinitperiod},
           given={Maria},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c6a2c6b63e3cd3ddf29de8bfbe9b2b7c}{%
           family={Cruz},
           familyi={C\bibinitperiod},
           given={A},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5be93422797493b1a4ee9660a39063c6}{%
           family={Fernandez},
           familyi={F\bibinitperiod},
           given={Luis\bibnamedelima Antonio},
           giveni={L\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f0c4d4fc18af6a47f50ec7c7c00e9481}{%
           family={Gordillo-Guerrero},
           familyi={G\bibinithyphendelim G\bibinitperiod},
           given={Antonio},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=aad568f61a2fcd92e2dfff3d807319bd}{%
           family={Guidetti},
           familyi={G\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ab11961d38a2c704aa577a107c3dd2b9}{%
           family={Maiorano},
           familyi={M\bibinitperiod},
           given={Andrea},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=760e0ca8b306c783b3ee6757496ce756}{%
           family={Mantovani},
           familyi={M\bibinitperiod},
           given={Filippo},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c9b2035d21222b7902347a7cdedb3e64}{%
           family={Marinari},
           familyi={M\bibinitperiod},
           given={Enzo},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e4bc695a482ec7436e89631c5f128e9a}{%
           family={Martin-Mayor},
           familyi={M\bibinithyphendelim M\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7ae7b475fa0ded2cae183ceb821261a4}{%
           family={Munoz-Sudupe},
           familyi={M\bibinithyphendelim S\bibinitperiod},
           given={Antonio},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=133d1681451004526b84ed93dd751b80}{%
           family={Navarro},
           familyi={N\bibinitperiod},
           given={Denis},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=633aedb846f931194e4d892bef86fdc0}{%
           family={Parisi},
           familyi={P\bibinitperiod},
           given={Giorgio},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=98dd080db385f147d10fd0b56308654d}{%
           family={Perez-Gaviro},
           familyi={P\bibinithyphendelim G\bibinitperiod},
           given={Sergio},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fc33afc625fb00ee149334581e84a09f}{%
           family={Rossi},
           familyi={R\bibinitperiod},
           given={Mauro},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c647fb78d49b95f5e52cfdb7a10c5c85}{%
           family={Ruiz-Lorenzo},
           familyi={R\bibinithyphendelim L\bibinitperiod},
           given={Juan\bibnamedelima J},
           giveni={J\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=eb6f250c21be472fa2766217361d10b6}{%
           family={Schifano},
           familyi={S\bibinitperiod},
           given={Sebastiano\bibnamedelima Fabio},
           giveni={S\bibinitperiod\bibinitdelim F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b7dc47e443a1ef5740622ab982e2711b}{%
           family={Sciretti},
           familyi={S\bibinitperiod},
           given={Daniele},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a3783362f03ba0b471c284597bf48615}{%
           family={Tarancon},
           familyi={T\bibinitperiod},
           given={Alfonso},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0518555b789a4716031aca30291e2c56}{%
           family={Tripiccione},
           familyi={T\bibinitperiod},
           given={Raffaele},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8d5249d17e3f781c6e904e70ca81f36b}{%
           family={Velasco},
           familyi={V\bibinitperiod},
           given={J\bibnamedelima Luis},
           giveni={J\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8b2217c7de476fb598bf2d56d5ed93dd}{%
           family={Yllanes},
           familyi={Y\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2007d448e98f96f8d8e3d09476cb4a39}{%
           family={Zanier},
           familyi={Z\bibinitperiod},
           given={Gianpaolo},
           giveni={G\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{ae5e7f8727c4ee9f0e1bc4323d15b4c3}
      \strng{fullhash}{83333503ee6b29f55a1254a7326d6485}
      \strng{bibnamehash}{83333503ee6b29f55a1254a7326d6485}
      \strng{authorbibnamehash}{83333503ee6b29f55a1254a7326d6485}
      \strng{authornamehash}{ae5e7f8727c4ee9f0e1bc4323d15b4c3}
      \strng{authorfullhash}{83333503ee6b29f55a1254a7326d6485}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Janus is a modular, massively parallel, and reconfigurable FPGA-based computing system. Each Janus module has one computational core and one host. Janus is tailored to, but not limited to, the needs of a class of hard scientific applications characterized by regular code structure, unconventional data-manipulation requirements, and a few Megabits database. The authors discuss this configurable system's architecture and focus on its use for Monte Carlo simulations of statistical mechanics, as Janus performs impressively on this class of application.}
      \field{eventtitle}{Computing in {{Science}} \& {{Engineering}}}
      \field{issn}{1558-366X}
      \field{journaltitle}{Computing in Science \& Engineering}
      \field{month}{1}
      \field{number}{1}
      \field{shorttitle}{Janus}
      \field{title}{Janus: {{An FPGA-Based System}} for {{High-Performance Scientific Computing}}}
      \field{urlday}{10}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{volume}{11}
      \field{year}{2009}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{48\bibrangedash 58}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1109/MCSE.2009.11
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\5LLBIGEX\\Belletti et al_2009_Janus.pdf;C\:\\Users\\simon\\Zotero\\storage\\3HBRQPYB\\4720223.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/4720223
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/4720223
      \endverb
      \keyw{Application software,Biology computing,Computational modeling,Computer simulation,field-programmable gate array,FPGA,Janus,Lattices,Monte Carlo methods,Monte Carlo simulations,Nobel Prize,Parallel processing,Pervasive computing,Physics computing,scientific computing,Scientific computing}
    \endentry
    \entry{bjarnasonModelSoftwarePrototyping2021a}{inproceedings}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=7122de7a9d5de51fcaf6c0ef57cf4d29}{%
           family={Bjarnason},
           familyi={B\bibinitperiod},
           given={Elizabeth},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0a9577800d5f33cebcf63d80b410353d}{%
           family={Lang},
           familyi={L\bibinitperiod},
           given={Franz},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0585fbc1571d1e4169fa67e6972851b4}{%
           family={Mjöberg},
           familyi={M\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{4a389ad1dcbcd9032a24863f80e72153}
      \strng{fullhash}{4a389ad1dcbcd9032a24863f80e72153}
      \strng{bibnamehash}{4a389ad1dcbcd9032a24863f80e72153}
      \strng{authorbibnamehash}{4a389ad1dcbcd9032a24863f80e72153}
      \strng{authornamehash}{4a389ad1dcbcd9032a24863f80e72153}
      \strng{authorfullhash}{4a389ad1dcbcd9032a24863f80e72153}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Background: Prototyping is an established practice for user interface design and for requirements engineering within agile software development, even so there is a lack of theory on prototyping. Aims: The main research objective is to provide a means to categorise prototyping instances, in order to enable comparison and reflection of prototyping practices. Method: We have performed a systematic mapping study of methodological aspects of prototyping consisting of thirty-three primary studies upon which we designed a model of prototyping that was validated through a focus group at a case company. Results: Our model consists of four aspects of prototyping, namely purpose, prototype scope, prototype use, and exploration strategy. This model supported the focus group participants in discussing prototyping practices by considering concrete prototyping instances in terms of the concepts provided by our model. Conclusions: The model can be used to categorise prototyping instances and can support practitioners in reflecting on their prototyping practices. Our study provides a starting point for further research on prototyping and into how the practice can be applied more cost-effectively to elicit, validate, and communicate requirements.}
      \field{booktitle}{Proceedings of the 15th {{ACM}} / {{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}} ({{ESEM}})}
      \field{day}{11}
      \field{isbn}{978-1-4503-8665-4}
      \field{month}{10}
      \field{series}{{{ESEM}} '21}
      \field{title}{A {{Model}} of {{Software Prototyping}} Based on a {{Systematic Map}}}
      \field{urlday}{1}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 11}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1145/3475716.3475772
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\ZNE8Q4K2\Bjarnason et al_2021_A Model of Software Prototyping based on a Systematic Map.pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.1145/3475716.3475772
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.1145/3475716.3475772
      \endverb
      \keyw{Agile,requirements engineering,systematic mapping study}
    \endentry
    \entry{bohmNoiseinjectedAnalogIsing2022}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=6906cda244a0c9602ef5f894137e5999}{%
           family={Böhm},
           familyi={B\bibinitperiod},
           given={Fabian},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=abd4d3092d8d2cc29cd693f4959e241b}{%
           family={Alonso-Urquijo},
           familyi={A\bibinithyphendelim U\bibinitperiod},
           given={Diego},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=15848f48daa2d67c0000c401da4c6860}{%
           family={Verschaffelt},
           familyi={V\bibinitperiod},
           given={Guy},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=078083f12986a21d6a6dcdd91da7d0b1}{%
           family={Van\bibnamedelimb der\bibnamedelima Sande},
           familyi={V\bibinitperiod\bibinitdelim d\bibinitperiod\bibinitdelim S\bibinitperiod},
           given={Guy},
           giveni={G\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{8f884a70bd219d09d1aa125d32c0e57a}
      \strng{fullhash}{4edfe5f076fe87ae6a2e0357ae67a859}
      \strng{bibnamehash}{4edfe5f076fe87ae6a2e0357ae67a859}
      \strng{authorbibnamehash}{4edfe5f076fe87ae6a2e0357ae67a859}
      \strng{authornamehash}{8f884a70bd219d09d1aa125d32c0e57a}
      \strng{authorfullhash}{4edfe5f076fe87ae6a2e0357ae67a859}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Ising machines are a promising non-von-Neumann computational concept for neural network training and combinatorial optimization. However, while various neural networks can be implemented with Ising machines, their inability to perform fast statistical sampling makes them inefficient for training neural networks compared to digital computers. Here, we introduce a universal concept to achieve ultrafast statistical sampling with analog Ising machines by injecting noise. With an opto-electronic Ising machine, we experimentally demonstrate that this can be used for accurate sampling of Boltzmann distributions and for unsupervised training of neural networks, with equal accuracy as software-based training. Through simulations, we find that Ising machines can perform statistical sampling orders-of-magnitudes faster than software-based methods. This enables the use of Ising machines beyond combinatorial optimization and makes them into efficient tools for machine learning and other applications.}
      \field{day}{4}
      \field{issn}{2041-1723}
      \field{issue}{1}
      \field{journaltitle}{Nature Communications}
      \field{langid}{english}
      \field{month}{10}
      \field{number}{1}
      \field{shortjournal}{Nat Commun}
      \field{title}{Noise-Injected Analog {{Ising}} Machines Enable Ultrafast Statistical Sampling and Machine Learning}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{13}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{5847}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/s41467-022-33441-3
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\CUQQWVHL\Noise-injected analog Ising machines enable ultrafast statistical sampling and machine learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s41467-022-33441-3
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s41467-022-33441-3
      \endverb
      \keyw{Complex networks,Computer science,Information theory and computation,Optoelectronic devices and components,ungelesen}
    \endentry
    \entry{boutrosFPGAArchitecturePrinciples2021}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=8b5079d5ebf378d0f9262924d2be36bc}{%
           family={Boutros},
           familyi={B\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=530b3538b2a3f0dece69e656e93cb352}{%
           family={Betz},
           familyi={B\bibinitperiod},
           given={Vaughn},
           giveni={V\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{dda1beb64009034b9b32fe0cc48a9d71}
      \strng{fullhash}{dda1beb64009034b9b32fe0cc48a9d71}
      \strng{bibnamehash}{dda1beb64009034b9b32fe0cc48a9d71}
      \strng{authorbibnamehash}{dda1beb64009034b9b32fe0cc48a9d71}
      \strng{authornamehash}{dda1beb64009034b9b32fe0cc48a9d71}
      \strng{authorfullhash}{dda1beb64009034b9b32fe0cc48a9d71}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Since their inception more than thirty years ago, field-programmable gate arrays (FPGAs) have been widely used to implement a myriad of applications from different domains. As a result of their low-level hardware reconfigurability, FPGAs have much faster design cycles and lower development costs compared to custom-designed chips. The design of an FPGA architecture involves many different design choices starting from the high-level architectural parameters down to the transistor-level implementation details, with the goal of making a highly programmable device while minimizing the area and performance cost of reconfigurability. As the needs of applications and the capabilities of process technology are constantly evolving, FPGA architecture must also adapt. In this article, we review the evolution of the different key components of modern commercial FPGA architectures and shed the light on their main design principles and implementation challenges.}
      \field{eventtitle}{{{IEEE Circuits}} and {{Systems Magazine}}}
      \field{issn}{1558-0830}
      \field{journaltitle}{IEEE Circuits and Systems Magazine}
      \field{number}{2}
      \field{shorttitle}{{{FPGA Architecture}}}
      \field{title}{{{FPGA Architecture}}: {{Principles}} and {{Progression}}}
      \field{urlday}{20}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{21}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{4\bibrangedash 29}
      \range{pages}{26}
      \verb{doi}
      \verb 10.1109/MCAS.2021.3071607
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\R4KDQUQP\\Boutros_Betz_2021_FPGA Architecture.pdf;C\:\\Users\\simon\\Zotero\\storage\\ILKB34AE\\9439568.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/9439568
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/9439568
      \endverb
      \keyw{Circuits and systems,Field programmable gate arrays,Hardware,Performance evaluation}
    \endentry
    \entry{caiHarnessingIntrinsicNoise2019}{online}{}
      \name{author}{11}{ul=4}{%
        {{un=0,uniquepart=base,hash=cb901327bef9dd5a7677bff79488868d}{%
           family={Cai},
           familyi={C\bibinitperiod},
           given={Fuxi},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=2,uniquepart=given,hash=ac3cbe4ed1d9c09600630c219b2c7b69}{%
           family={Kumar},
           familyi={K\bibinitperiod},
           given={Suhas},
           giveni={S\bibinitperiod},
           givenun=2}}%
        {{un=0,uniquepart=base,hash=42900b0c4f16b4d6869e64dc8f16701e}{%
           family={Van\bibnamedelima Vaerenbergh},
           familyi={V\bibinitperiod\bibinitdelim V\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=1,uniquepart=given,hash=7dbcd223db163eb1fc0b2e4f844602cd}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Rui},
           giveni={R\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=8e6ea9d2aeafe6a7426446dd92acf0ff}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Can},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b4d48c0ed5d150e3b3b7aeda8a4778e0}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Shimeng},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5df495f41be653cb3555f2e44bcf94d9}{%
           family={Xia},
           familyi={X\bibinitperiod},
           given={Qiangfei},
           giveni={Q\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=097061c4172ddd74306566ccbc43f2fc}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={J.\bibnamedelimi Joshua},
           giveni={J\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=669a689b20c1effe38f4703152cb7e41}{%
           family={Beausoleil},
           familyi={B\bibinitperiod},
           given={Raymond},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cb550a145b27c55a56001120c672da49}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=331a1832282d5aa7a2ec79e22f030a8d}{%
           family={Strachan},
           familyi={S\bibinitperiod},
           given={John\bibnamedelima Paul},
           giveni={J\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{e65a793c2dca64e13134fc19b07b8bbd}
      \strng{fullhash}{9badd0c77ca35a2d929018beaaf29ae6}
      \strng{bibnamehash}{9badd0c77ca35a2d929018beaaf29ae6}
      \strng{authorbibnamehash}{9badd0c77ca35a2d929018beaaf29ae6}
      \strng{authornamehash}{e65a793c2dca64e13134fc19b07b8bbd}
      \strng{authorfullhash}{9badd0c77ca35a2d929018beaaf29ae6}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe a hybrid analog-digital computing approach to solve important combinatorial optimization problems that leverages memristors (two-terminal nonvolatile memories). While previous memristor accelerators have had to minimize analog noise effects, we show that our optimization solver harnesses such noise as a computing resource. Here we describe a memristor-Hopfield Neural Network (mem-HNN) with massively parallel operations performed in a dense crossbar array. We provide experimental demonstrations solving NP-hard max-cut problems directly in analog crossbar arrays, and supplement this with experimentally-grounded simulations to explore scalability with problem size, providing the success probabilities, time and energy to solution, and interactions with intrinsic analog noise. Compared to fully digital approaches, and present-day quantum and optical accelerators, we forecast the mem-HNN to have over four orders of magnitude higher solution throughput per power consumption. This suggests substantially improved performance and scalability compared to current quantum annealing approaches, while operating at room temperature and taking advantage of existing CMOS technology augmented with emerging analog non-volatile memristors.}
      \field{day}{3}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{4}
      \field{pubstate}{preprint}
      \field{title}{Harnessing {{Intrinsic Noise}} in {{Memristor Hopfield Neural Networks}} for {{Combinatorial Optimization}}}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1903.11194
      \endverb
      \verb{eprint}
      \verb 1903.11194
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\FLZE6J2J\Cai et al. - 2019 - Harnessing Intrinsic Noise in Memristor Hopfield N.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1903.11194
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1903.11194
      \endverb
      \keyw{Computer Science - Emerging Technologies,ungelesen,zitiert}
    \endentry
    \entry{caiPowerefficientCombinatorialOptimization2020}{article}{}
      \name{author}{14}{ul=4}{%
        {{un=0,uniquepart=base,hash=cb901327bef9dd5a7677bff79488868d}{%
           family={Cai},
           familyi={C\bibinitperiod},
           given={Fuxi},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=2,uniquepart=given,hash=ac3cbe4ed1d9c09600630c219b2c7b69}{%
           family={Kumar},
           familyi={K\bibinitperiod},
           given={Suhas},
           giveni={S\bibinitperiod},
           givenun=2}}%
        {{un=0,uniquepart=base,hash=42900b0c4f16b4d6869e64dc8f16701e}{%
           family={Van\bibnamedelima Vaerenbergh},
           familyi={V\bibinitperiod\bibinitdelim V\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6215eb4464621bc3e2ca5e27d19c54a8}{%
           family={Sheng},
           familyi={S\bibinitperiod},
           given={Xia},
           giveni={X\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7dbcd223db163eb1fc0b2e4f844602cd}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Rui},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8e6ea9d2aeafe6a7426446dd92acf0ff}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Can},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=06805d867f08208c417a892b6b52fc2c}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Zhan},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a9431faf92145ee3c8726394bb4d169c}{%
           family={Foltin},
           familyi={F\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b4d48c0ed5d150e3b3b7aeda8a4778e0}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Shimeng},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5df495f41be653cb3555f2e44bcf94d9}{%
           family={Xia},
           familyi={X\bibinitperiod},
           given={Qiangfei},
           giveni={Q\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=097061c4172ddd74306566ccbc43f2fc}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={J.\bibnamedelimi Joshua},
           giveni={J\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=669a689b20c1effe38f4703152cb7e41}{%
           family={Beausoleil},
           familyi={B\bibinitperiod},
           given={Raymond},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6811e986c30783c64efd7bc58eef1ca2}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Wei\bibnamedelima D.},
           giveni={W\bibinitperiod\bibinitdelim D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=331a1832282d5aa7a2ec79e22f030a8d}{%
           family={Strachan},
           familyi={S\bibinitperiod},
           given={John\bibnamedelima Paul},
           giveni={J\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{faf35f8d88a4c2c127d3b67b1124edf3}
      \strng{fullhash}{abee1734f5789a45ab3fdba7e80c3149}
      \strng{bibnamehash}{abee1734f5789a45ab3fdba7e80c3149}
      \strng{authorbibnamehash}{abee1734f5789a45ab3fdba7e80c3149}
      \strng{authornamehash}{faf35f8d88a4c2c127d3b67b1124edf3}
      \strng{authorfullhash}{abee1734f5789a45ab3fdba7e80c3149}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{To tackle important combinatorial optimization problems, a variety of annealing-inspired computing accelerators, based on several different technology platforms, have been proposed, including quantum-, optical- and electronics-based approaches. However, to be of use in industrial applications, further improvements in speed and energy efficiency are necessary. Here, we report a memristor-based annealing system that uses an energy-efficient neuromorphic architecture based on a Hopfield neural network. Our analogue–digital computing approach creates an optimization solver in which massively parallel operations are performed in a dense crossbar array that can inject the needed computational noise through the analogue array and device errors, amplified or dampened by using a novel feedback algorithm. We experimentally show that the approach can solve non-deterministic polynomial-time (NP)-hard max-cut problems by harnessing the intrinsic hardware noise. We also use experimentally grounded simulations to explore scalability with problem size, which suggest that our memristor-based approach can offer a solution throughput over four orders of magnitude higher per power consumption relative to current quantum, optical and fully digital approaches.}
      \field{issn}{2520-1131}
      \field{journaltitle}{Nature Electronics}
      \field{langid}{english}
      \field{month}{7}
      \field{number}{7}
      \field{shortjournal}{Nat Electron}
      \field{title}{Power-Efficient Combinatorial Optimization Using Intrinsic Noise in Memristor {{Hopfield}} Neural Networks}
      \field{urlday}{21}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{3}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{409\bibrangedash 418}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1038/s41928-020-0436-6
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\F4SBPVXU\Cai et al_2020_Power-efficient combinatorial optimization using intrinsic noise in memristor.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s41928-020-0436-6
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s41928-020-0436-6
      \endverb
      \keyw{Computational science,Electrical and electronic engineering,Electronic devices,Electronic properties and materials}
    \endentry
    \entry{changDirectObservationDualFilament2017}{article}{}
      \name{author}{7}{}{%
        {{un=0,uniquepart=base,hash=8e3a762efc3eb1613f9191540822660b}{%
           family={Chang},
           familyi={C\bibinitperiod},
           given={Chia-Fu},
           giveni={C\bibinithyphendelim F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=319c80fee854268a0060a27ef7ebc9d9}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Jui-Yuan},
           giveni={J\bibinithyphendelim Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8363bd4e7c76b20bea81df1c83f68371}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Chun-Wei},
           giveni={C\bibinithyphendelim W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7a865ef3dc5e2dce0ece0e69c3e1b502}{%
           family={Chiu},
           familyi={C\bibinitperiod},
           given={Chung-Hua},
           giveni={C\bibinithyphendelim H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=05b959e1f8a96aa7aec58fd7def8dbd0}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Ting-Yi},
           giveni={T\bibinithyphendelim Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=051d61ccc2a6d14ffd23ec0ff8cce35f}{%
           family={Yeh},
           familyi={Y\bibinitperiod},
           given={Ping-Hung},
           giveni={P\bibinithyphendelim H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8efe36a3893b9eb173200c51397f8e47}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Wen-Wei},
           giveni={W\bibinithyphendelim W\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{67eb9711330025777b842c3e1474789d}
      \strng{fullhash}{ad7027127b00106f9278ad6704353ae6}
      \strng{bibnamehash}{ad7027127b00106f9278ad6704353ae6}
      \strng{authorbibnamehash}{ad7027127b00106f9278ad6704353ae6}
      \strng{authornamehash}{67eb9711330025777b842c3e1474789d}
      \strng{authorfullhash}{ad7027127b00106f9278ad6704353ae6}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The Forming phenomenon is observed via in situ transmission electron microscopy in the Ag/Ta2O5/Pt system. The device is switched to a low-resistance state as the dual filament is connected to the electrodes. The results of energy dispersive spectrometer and electron energy loss spectroscopy analyses demonstrate that the filament is composed by a stack of oxygen vacancies and Ag metal.}
      \field{issn}{1613-6829}
      \field{journaltitle}{Small}
      \field{number}{15}
      \field{title}{Direct {{Observation}} of {{Dual-Filament Switching Behaviors}} in {{Ta2O5-Based Memristors}}}
      \field{urlday}{22}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{13}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1603116}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1002/smll.201603116
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\UJ4BKUM2\smll.html
      \endverb
      \verb{urlraw}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1002/smll.201603116
      \endverb
      \verb{url}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1002/smll.201603116
      \endverb
      \keyw{dual filaments,hydroxide,in situ TEM,memristors,RRAM,Ta2O5}
    \endentry
    \entry{chenApplicationVoltageComparator2021}{inproceedings}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=d001bce88e27fe293cfaf3708133e2ec}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Ying},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=1,uniquepart=given,hash=65a6d2d755deef911f45c4634c09bdde}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Miao},
           giveni={M\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=054572adbd5890ae20b2cae2be41b69c}{%
           family={Shen},
           familyi={S\bibinitperiod},
           given={Xuejing},
           giveni={X\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{d683a32c28394ad935fc78c9130e689f}
      \strng{fullhash}{d683a32c28394ad935fc78c9130e689f}
      \strng{bibnamehash}{d683a32c28394ad935fc78c9130e689f}
      \strng{authorbibnamehash}{d683a32c28394ad935fc78c9130e689f}
      \strng{authornamehash}{d683a32c28394ad935fc78c9130e689f}
      \strng{authorfullhash}{d683a32c28394ad935fc78c9130e689f}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The voltage comparator which is composed by the integrated operational amplifier is a kind of common analog signal processor, which is widely used. Voltage comparator circuit of automobile charging system, which is composed of voltage comparator, is a kind of common electrical applications. Through the Multisim simulation software, we can deepen our understanding and grasp of the theoretical knowledge of the voltage comparator.}
      \field{booktitle}{2021 {{IEEE International Conference}} on {{Power}}, {{Intelligent Computing}} and {{Systems}} ({{ICPICS}})}
      \field{eventtitle}{2021 {{IEEE International Conference}} on {{Power}}, {{Intelligent Computing}} and {{Systems}} ({{ICPICS}})}
      \field{month}{7}
      \field{title}{Application of {{Voltage Comparator}} and Its {{Multisim Simulation}}}
      \field{urlday}{25}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{28\bibrangedash 30}
      \range{pages}{3}
      \verb{doi}
      \verb 10.1109/ICPICS52425.2021.9524134
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\TE6TC24A\\Chen et al_2021_Application of Voltage Comparator and its Multisim Simulation.pdf;C\:\\Users\\simon\\Zotero\\storage\\65GKLUYC\\9524134.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/9524134
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/9524134
      \endverb
      \keyw{Automobiles,Computational modeling,Conferences,Monitoring,Multisim,Operational amplifiers,Software,voltage comparator,voltage comparator circuit}
    \endentry
    \entry{cichyDeepNeuralNetworks2019}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=b12210ea5623bafbf6557da34301f76a}{%
           family={Cichy},
           familyi={C\bibinitperiod},
           given={Radoslaw\bibnamedelima M.},
           giveni={R\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3d5877ff71a20cebbe3644a4043fadf4}{%
           family={Kaiser},
           familyi={K\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Elsevier}%
      }
      \strng{namehash}{5932d4a81b15a9095407a3fc919d4d91}
      \strng{fullhash}{5932d4a81b15a9095407a3fc919d4d91}
      \strng{bibnamehash}{5932d4a81b15a9095407a3fc919d4d91}
      \strng{authorbibnamehash}{5932d4a81b15a9095407a3fc919d4d91}
      \strng{authornamehash}{5932d4a81b15a9095407a3fc919d4d91}
      \strng{authorfullhash}{5932d4a81b15a9095407a3fc919d4d91}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{day}{1}
      \field{eprinttype}{pmid}
      \field{issn}{1364-6613, 1879-307X}
      \field{journaltitle}{Trends in Cognitive Sciences}
      \field{langid}{english}
      \field{month}{4}
      \field{number}{4}
      \field{shortjournal}{Trends in Cognitive Sciences}
      \field{title}{Deep {{Neural Networks}} as {{Scientific Models}}}
      \field{urlday}{23}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{23}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{305\bibrangedash 317}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/j.tics.2019.01.009
      \endverb
      \verb{eprint}
      \verb 30795896
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\4CWZNGI3\Cichy_Kaiser_2019_Deep Neural Networks as Scientific Models.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(19)30034-8
      \endverb
      \verb{url}
      \verb https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(19)30034-8
      \endverb
      \keyw{deep learning,explanation,exploration,gelesen,neural network,prediction,scientific model,ungelesen,zitiert}
    \endentry
    \entry{dallyEvolutionGraphicsProcessing2021}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=c4a8df92e1f8e22aa2062da8bcfd402d}{%
           family={Dally},
           familyi={D\bibinitperiod},
           given={William\bibnamedelima J.},
           giveni={W\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cacfbd2fac3b3c6c59a4f36eb0743326}{%
           family={Keckler},
           familyi={K\bibinitperiod},
           given={Stephen\bibnamedelima W.},
           giveni={S\bibinitperiod\bibinitdelim W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3aa2dfd7bd68dcafe8f2c42acff8258}{%
           family={Kirk},
           familyi={K\bibinitperiod},
           given={David\bibnamedelima B.},
           giveni={D\bibinitperiod\bibinitdelim B\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{241e0d8be1336c7f9d7b0a13cc8bc8d1}
      \strng{fullhash}{241e0d8be1336c7f9d7b0a13cc8bc8d1}
      \strng{bibnamehash}{241e0d8be1336c7f9d7b0a13cc8bc8d1}
      \strng{authorbibnamehash}{241e0d8be1336c7f9d7b0a13cc8bc8d1}
      \strng{authornamehash}{241e0d8be1336c7f9d7b0a13cc8bc8d1}
      \strng{authorfullhash}{241e0d8be1336c7f9d7b0a13cc8bc8d1}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Graphics processing units (GPUs) power today’s fastest supercomputers, are the dominant platform for deep learning, and provide the intelligence for devices ranging from self-driving cars to robots and smart cameras. They also generate compelling photorealistic images at real-time frame rates. GPUs have evolved by adding features to support new use cases. NVIDIA’s GeForce 256, the first GPU, was a dedicated processor for real-time graphics, an application that demands large amounts of floating-point arithmetic for vertex and fragment shading computations and high memory bandwidth. As real-time graphics advanced, GPUs became programmable. The combination of programmability and floating-point performance made GPUs attractive for running scientific applications. Scientists found ways to use early programmable GPUs by casting their calculations as vertex and fragment shaders. GPUs evolved to meet the needs of scientific users by adding hardware for simpler programming, double-precision floating-point arithmetic, and resilience.}
      \field{eventtitle}{{{IEEE Micro}}}
      \field{issn}{1937-4143}
      \field{journaltitle}{IEEE Micro}
      \field{month}{11}
      \field{number}{6}
      \field{title}{Evolution of the {{Graphics Processing Unit}} ({{GPU}})}
      \field{urlday}{20}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{41}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{42\bibrangedash 51}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/MM.2021.3113475
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\W3XBQE66\9623445.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/9623445
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/9623445
      \endverb
      \keyw{Graphics processing units}
    \endentry
    \entry{darioamodeiAICompute}{online}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=1eeb0598da229e760e68f8ca10de591e}{%
           family={{Dario Amodei}},
           familyi={D\bibinitperiod}}}%
        {{un=0,uniquepart=base,hash=f81c1e578990f8f8327eadf636450c0a}{%
           family={{Danny Hernandez}},
           familyi={D\bibinitperiod}}}%
      }
      \strng{namehash}{6e8ab90f78fac6e4e6d6b6518e6c89a5}
      \strng{fullhash}{6e8ab90f78fac6e4e6d6b6518e6c89a5}
      \strng{bibnamehash}{6e8ab90f78fac6e4e6d6b6518e6c89a5}
      \strng{authorbibnamehash}{6e8ab90f78fac6e4e6d6b6518e6c89a5}
      \strng{authornamehash}{6e8ab90f78fac6e4e6d6b6518e6c89a5}
      \strng{authorfullhash}{6e8ab90f78fac6e4e6d6b6518e6c89a5}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{url}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We’re releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore’s Law had a 2-year doubling period)[\^{}footnote-correction]. Since 2012, this metric has grown by more than 300,000x (a 2-year doubling period would yield only a 7x increase). Improvements in compute have been a key component of AI progress, so as long as this trend continues, it’s worth preparing for the implications of systems far outside today’s capabilities.}
      \field{langid}{american}
      \field{title}{{{AI}} and Compute}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{urldateera}{ce}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\LW5E6CXF\ai-and-compute.html
      \endverb
      \verb{urlraw}
      \verb https://openai.com/research/ai-and-compute
      \endverb
      \verb{url}
      \verb https://openai.com/research/ai-and-compute
      \endverb
      \keyw{ungelesen,zitiert}
    \endentry
    \entry{DiscreteContinuousModels}{online}{}
      \list{organization}{3}{%
        {Discrete}%
        {Continuous Models}%
        {Applied Computational Science}%
      }
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{url}
      \field{labeltitlesource}{title}
      \field{abstract}{Discrete and Continuous Models and Applied Computational Science}
      \field{langid}{english}
      \field{title}{Discrete and {{Continuous Models}} and {{Applied Computational Science}}}
      \field{urlday}{8}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{urldateera}{ce}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\GQEE2Q99\miph.html
      \endverb
      \verb{urlraw}
      \verb http://journals.rudn.ru/miph
      \endverb
      \verb{url}
      \verb http://journals.rudn.ru/miph
      \endverb
    \endentry
    \entry{dramschChapterOne702020}{incollection}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=37e4f276c87873863ef2a9602753a12d}{%
           family={Dramsch},
           familyi={D\bibinitperiod},
           given={Jesper\bibnamedelima Sören},
           giveni={J\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{2}{}{%
        {{hash=7447b1719aec4d046851d2c9708210fe}{%
           family={Moseley},
           familyi={M\bibinitperiod},
           given={Ben},
           giveni={B\bibinitperiod}}}%
        {{hash=2ad0525fe321d79e5cd9c32144ca8bd7}{%
           family={Krischer},
           familyi={K\bibinitperiod},
           given={Lion},
           giveni={L\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Elsevier}%
      }
      \strng{namehash}{37e4f276c87873863ef2a9602753a12d}
      \strng{fullhash}{37e4f276c87873863ef2a9602753a12d}
      \strng{bibnamehash}{37e4f276c87873863ef2a9602753a12d}
      \strng{authorbibnamehash}{37e4f276c87873863ef2a9602753a12d}
      \strng{authornamehash}{37e4f276c87873863ef2a9602753a12d}
      \strng{authorfullhash}{37e4f276c87873863ef2a9602753a12d}
      \strng{editorbibnamehash}{dd894d0187ca86c096aa5a95d9d0d86f}
      \strng{editornamehash}{dd894d0187ca86c096aa5a95d9d0d86f}
      \strng{editorfullhash}{dd894d0187ca86c096aa5a95d9d0d86f}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This review gives an overview of the development of machine learning in geoscience. A thorough analysis of the codevelopments of machine learning applications throughout the last 70 years relates the recent enthusiasm for machine learning to developments in geoscience. I explore the shift of kriging toward a mainstream machine learning method and the historic application of neural networks in geoscience, following the general trend of machine learning enthusiasm through the decades. Furthermore, this chapter explores the shift from mathematical fundamentals and knowledge in software development toward skills in model validation, applied statistics, and integrated subject matter expertise. The review is interspersed with code examples to complement the theoretical foundations and illustrate model validation and machine learning explainability for science. The scope of this review includes various shallow machine learning methods, e.g., decision trees, random forests, support-vector machines, and Gaussian processes, as well as, deep neural networks, including feed-forward neural networks, convolutional neural networks, recurrent neural networks, and generative adversarial networks. Regarding geoscience, the review has a bias toward geophysics but aims to strike a balance with geochemistry, geostatistics, and geology, however, excludes remote sensing, as this would exceed the scope. In general, I aim to provide context for the recent enthusiasm surrounding deep learning with respect to research, hardware, and software developments that enable successful application of shallow and deep machine learning in all disciplines of Earth science.}
      \field{booktitle}{Advances in {{Geophysics}}}
      \field{day}{1}
      \field{month}{1}
      \field{series}{Machine {{Learning}} in {{Geosciences}}}
      \field{title}{Chapter {{One}} - 70 Years of Machine Learning in Geoscience in Review}
      \field{urlday}{28}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{61}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 55}
      \range{pages}{55}
      \verb{doi}
      \verb 10.1016/bs.agph.2020.08.002
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\L5UC2QWA\\Dramsch_2020_Chapter One - 70 years of machine learning in geoscience in review.pdf;C\:\\Users\\simon\\Zotero\\storage\\DSKCIMKK\\S0065268720300054.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0065268720300054
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0065268720300054
      \endverb
      \keyw{Deep learning,Earth science,Geology,Geophysics,Geoscience,Kriging,Machine learning,Neural networks,Review}
    \endentry
    \entry{duModelBasedPlanning2021}{online}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=0292e43cebfdbcde9e20aa4fdbe17700}{%
           family={Du},
           familyi={D\bibinitperiod},
           given={Yilun},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c0160314ad6c07a2e2f8b764e263fa21}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Toru},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7570e7c3fc2c13d59af4d7cdb9962a4d}{%
           family={Mordatch},
           familyi={M\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{b1594ee29851153a4975b088d6724427}
      \strng{fullhash}{b1594ee29851153a4975b088d6724427}
      \strng{bibnamehash}{b1594ee29851153a4975b088d6724427}
      \strng{authorbibnamehash}{b1594ee29851153a4975b088d6724427}
      \strng{authornamehash}{b1594ee29851153a4975b088d6724427}
      \strng{authorfullhash}{b1594ee29851153a4975b088d6724427}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Model-based planning holds great promise for improving both sample efficiency and generalization in reinforcement learning (RL). We show that energy-based models (EBMs) are a promising class of models to use for model-based planning. EBMs naturally support inference of intermediate states given start and goal state distributions. We provide an online algorithm to train EBMs while interacting with the environment, and show that EBMs allow for significantly better online learning than corresponding feed-forward networks. We further show that EBMs support maximum entropy state inference and are able to generate diverse state space plans. We show that inference purely in state space - without planning actions - allows for better generalization to previously unseen obstacles in the environment and prevents the planner from exploiting the dynamics model by applying uncharacteristic action sequences. Finally, we show that online EBM training naturally leads to intentionally planned state exploration which performs significantly better than random exploration.}
      \field{day}{8}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{month}{3}
      \field{pubstate}{preprint}
      \field{title}{Model {{Based Planning}} with {{Energy Based Models}}}
      \field{urlday}{19}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1909.06878
      \endverb
      \verb{eprint}
      \verb 1909.06878
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\VJYEAH2U\\Du et al_2021_Model Based Planning with Energy Based Models.pdf;C\:\\Users\\simon\\Zotero\\storage\\TVYW9SDL\\1909.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1909.06878
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1909.06878
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning,ungelesen,zitiert}
    \endentry
    \entry{duImplicitGenerationGeneralization2020}{online}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=0292e43cebfdbcde9e20aa4fdbe17700}{%
           family={Du},
           familyi={D\bibinitperiod},
           given={Yilun},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7570e7c3fc2c13d59af4d7cdb9962a4d}{%
           family={Mordatch},
           familyi={M\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{107fe3b9990ca6662db1b5c57d875371}
      \strng{fullhash}{107fe3b9990ca6662db1b5c57d875371}
      \strng{bibnamehash}{107fe3b9990ca6662db1b5c57d875371}
      \strng{authorbibnamehash}{107fe3b9990ca6662db1b5c57d875371}
      \strng{authornamehash}{107fe3b9990ca6662db1b5c57d875371}
      \strng{authorfullhash}{107fe3b9990ca6662db1b5c57d875371}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.}
      \field{day}{29}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{month}{6}
      \field{pubstate}{preprint}
      \field{title}{Implicit {{Generation}} and {{Generalization}} in {{Energy-Based Models}}}
      \field{urlday}{23}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1903.08689
      \endverb
      \verb{eprint}
      \verb 1903.08689
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\RDJVNPK3\\Du_Mordatch_2020_Implicit Generation and Generalization in Energy-Based Models.pdf;C\:\\Users\\simon\\Zotero\\storage\\IRNZEZX7\\1903.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1903.08689
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1903.08689
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning,ungelesen,zitiert}
    \endentry
    \entry{durstewitzDeepNeuralNetworks2019}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=2b69d2b721f59091dda2b7f803401426}{%
           family={Durstewitz},
           familyi={D\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=09d35b70790c68251b77456e2ca47b33}{%
           family={Koppe},
           familyi={K\bibinitperiod},
           given={Georgia},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8ebbfae49f1821edf489d06caa5cc8fb}{%
           family={Meyer-Lindenberg},
           familyi={M\bibinithyphendelim L\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{5ee925b08c241b029b12e43a4de2bb11}
      \strng{fullhash}{5ee925b08c241b029b12e43a4de2bb11}
      \strng{bibnamehash}{5ee925b08c241b029b12e43a4de2bb11}
      \strng{authorbibnamehash}{5ee925b08c241b029b12e43a4de2bb11}
      \strng{authornamehash}{5ee925b08c241b029b12e43a4de2bb11}
      \strng{authorfullhash}{5ee925b08c241b029b12e43a4de2bb11}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Machine and deep learning methods, today’s core of artificial intelligence, have been applied with increasing success and impact in many commercial and research settings. They are powerful tools for large scale data analysis, prediction and classification, especially in very data-rich environments (“big data”), and have started to find their way into medical applications. Here we will first give an overview of machine learning methods, with a focus on deep and recurrent neural networks, their relation to statistics, and the core principles behind them. We will then discuss and review directions along which (deep) neural networks can be, or already have been, applied in the context of psychiatry, and will try to delineate their future potential in this area. We will also comment on an emerging area that so far has been much less well explored: by embedding semantically interpretable computational models of brain dynamics or behavior into a statistical machine learning context, insights into dysfunction beyond mere prediction and classification may be gained. Especially this marriage of computational models with statistical inference may offer insights into neural and behavioral mechanisms that could open completely novel avenues for psychiatric treatment.}
      \field{issn}{1476-5578}
      \field{issue}{11}
      \field{journaltitle}{Molecular Psychiatry}
      \field{langid}{english}
      \field{month}{11}
      \field{number}{11}
      \field{shortjournal}{Mol Psychiatry}
      \field{title}{Deep Neural Networks in Psychiatry}
      \field{urlday}{23}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{24}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1583\bibrangedash 1598}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1038/s41380-019-0365-9
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\RHCV9KWS\Durstewitz et al_2019_Deep neural networks in psychiatry.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s41380-019-0365-9
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s41380-019-0365-9
      \endverb
      \keyw{gelesen,Neuroscience,Psychiatric disorders,ungelesen,zitiert}
    \endentry
    \entry{ebertSystematischesRequirementsEngineering2008}{book}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=0a42c5417e74c35afa6f539302ff2574}{%
           family={Ebert},
           familyi={E\bibinitperiod},
           given={Christof},
           giveni={C\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{0a42c5417e74c35afa6f539302ff2574}
      \strng{fullhash}{0a42c5417e74c35afa6f539302ff2574}
      \strng{bibnamehash}{0a42c5417e74c35afa6f539302ff2574}
      \strng{authorbibnamehash}{0a42c5417e74c35afa6f539302ff2574}
      \strng{authornamehash}{0a42c5417e74c35afa6f539302ff2574}
      \strng{authorfullhash}{0a42c5417e74c35afa6f539302ff2574}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{day}{1}
      \field{isbn}{978-3-89864-546-1}
      \field{month}{1}
      \field{title}{Systematisches {{Requirements Engineering}} Und {{Management}} - {{Anforderungen}} Ermitteln, Spezifizieren, Analysieren Und Verwalten (2. {{Aufl}}.).}
      \field{year}{2008}
      \field{dateera}{ce}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\6CUJXR9S\Ebert_2008_Systematisches Requirements Engineering und Management - Anforderungen.pdf
      \endverb
    \endentry
    \entry{fahlmanMassivelyParallelArchitectures1983}{book}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=34aa534e8ce7b11651d85a45a74ff406}{%
           family={Fahlman},
           familyi={F\bibinitperiod},
           given={Scott},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=1,uniquepart=given,hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod},
           givenun=1}}%
        {{un=1,uniquepart=given,hash=c1a5f9ca77bf5edd13d3665aa702769f}{%
           family={Sejnowski},
           familyi={S\bibinitperiod},
           given={Terrence},
           giveni={T\bibinitperiod},
           givenun=1}}%
      }
      \strng{namehash}{dea93d0f133237f0c4bee98866424c3f}
      \strng{fullhash}{dea93d0f133237f0c4bee98866424c3f}
      \strng{bibnamehash}{dea93d0f133237f0c4bee98866424c3f}
      \strng{authorbibnamehash}{dea93d0f133237f0c4bee98866424c3f}
      \strng{authornamehash}{dea93d0f133237f0c4bee98866424c3f}
      \strng{authorfullhash}{dea93d0f133237f0c4bee98866424c3f}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{ABSTRACT It is becoming,increasingly apparent that some,aspects of intelligent behavior require enormous ,computational power,and,that some,sort of massively parallel computing architecture is the most plausible way to deliver sueh power. Parallelism, rather than raw speed of the computing,elements. seems,to be the way that the brain gets such jobs done. But even if the need for massive parallelism is admitted, there is still the question of what kind of parallel architecture best fits the needs of various A1 tasks. In this paper we will attempt to isolate a number(\$\#\$\#\$\#CommaToBean intelligent system must perform. We will describe several families of massively parallel computing architectures, and we will see which of these computational tasks can be handled by each of these families. In particular, we will describe a new architecture, which we call the Boltzmann machine, whose abilities appear to include a number,of tasks that are inefficient}
      \field{day}{1}
      \field{journaltitle}{[No source information available]}
      \field{month}{1}
      \field{pagetotal}{109}
      \field{shorttitle}{Massively {{Parallel Architectures}} for {{AI}}}
      \field{title}{Massively {{Parallel Architectures}} for {{AI}}: {{NETL}}, {{Thistle}}, and {{Boltzmann Machines}}.}
      \field{year}{1983}
      \field{dateera}{ce}
      \field{pages}{113}
      \range{pages}{1}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\5QXANLI6\1983 - Massively Parallel Architectures for Al NETL, Thistle, and Boltzmann Machines.pdf
      \endverb
      \keyw{ungelesen,zitiert}
    \endentry
    \entry{fischerIntroductionRestrictedBoltzmann2012}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=824f2dd58979463cc1c555dcfd563d9f}{%
           family={Fischer},
           familyi={F\bibinitperiod},
           given={Asja},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7848f2716625f371de810c3dbdf1d904}{%
           family={Igel},
           familyi={I\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{4}{}{%
        {{hash=b3ca704c62db81f60a7936028f56200c}{%
           family={Alvarez},
           familyi={A\bibinitperiod},
           given={Luis},
           giveni={L\bibinitperiod}}}%
        {{hash=ec4f94e6391b10d047e072a868c586bc}{%
           family={Mejail},
           familyi={M\bibinitperiod},
           given={Marta},
           giveni={M\bibinitperiod}}}%
        {{hash=b1232946fa277be55b11e3900f121b62}{%
           family={Gomez},
           familyi={G\bibinitperiod},
           given={Luis},
           giveni={L\bibinitperiod}}}%
        {{hash=f8f735ac7dde1a7702929da2415a5645}{%
           family={Jacobo},
           familyi={J\bibinitperiod},
           given={Julio},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{a4352af4e9353191f0015ee4c0ee032a}
      \strng{fullhash}{a4352af4e9353191f0015ee4c0ee032a}
      \strng{bibnamehash}{a4352af4e9353191f0015ee4c0ee032a}
      \strng{authorbibnamehash}{a4352af4e9353191f0015ee4c0ee032a}
      \strng{authornamehash}{a4352af4e9353191f0015ee4c0ee032a}
      \strng{authorfullhash}{a4352af4e9353191f0015ee4c0ee032a}
      \strng{editorbibnamehash}{a538ce6a30f741f51e19e36fa5b9d09f}
      \strng{editornamehash}{2b61adecf0a53635a094216d7cf1b07a}
      \strng{editorfullhash}{a538ce6a30f741f51e19e36fa5b9d09f}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided.}
      \field{booktitle}{Progress in {{Pattern Recognition}}, {{Image Analysis}}, {{Computer Vision}}, and {{Applications}}}
      \field{isbn}{978-3-642-33275-3}
      \field{langid}{english}
      \field{series}{Lecture {{Notes}} in {{Computer Science}}}
      \field{title}{An {{Introduction}} to {{Restricted Boltzmann Machines}}}
      \field{year}{2012}
      \field{dateera}{ce}
      \field{pages}{14\bibrangedash 36}
      \range{pages}{23}
      \verb{doi}
      \verb 10.1007/978-3-642-33275-3_2
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\6E9KS4GX\Fischer_Igel_2012_An Introduction to Restricted Boltzmann Machines.pdf
      \endverb
      \keyw{gelesen,Gibbs Sampling,Markov Chain,Markov Chain Monte Carlo,Markov Chain Monte Carlo Method,Restrict Boltzmann Machine,ungelesen,zitiert}
    \endentry
    \entry{gawlikowskiSurveyUncertaintyDeep2023}{article}{}
      \name{author}{14}{}{%
        {{un=0,uniquepart=base,hash=c7b9939dd7cf821607c0cf453a170baf}{%
           family={Gawlikowski},
           familyi={G\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cfec487698cec0755cf48f6fdbf2c88c}{%
           family={Tassi},
           familyi={T\bibinitperiod},
           given={Cedrique\bibnamedelimb Rovile\bibnamedelima Njieutcheu},
           giveni={C\bibinitperiod\bibinitdelim R\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c1de696d3a8d95f8f7a0c49ddff64c0f}{%
           family={Ali},
           familyi={A\bibinitperiod},
           given={Mohsin},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8d074ee955c33190bb25727a6e310b51}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Jongseok},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dc7eb309d27edb6edbeeb3c0149ca1c8}{%
           family={Humt},
           familyi={H\bibinitperiod},
           given={Matthias},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cf7e48609498fdd16eadafcb88590659}{%
           family={Feng},
           familyi={F\bibinitperiod},
           given={Jianxiang},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6baa0277df8aa8696f9880cc66242f6d}{%
           family={Kruspe},
           familyi={K\bibinitperiod},
           given={Anna},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e8ad581b48403dea2b06971054fe9ff1}{%
           family={Triebel},
           familyi={T\bibinitperiod},
           given={Rudolph},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c8c73bfb7060212cd42508f1f1495516}{%
           family={Jung},
           familyi={J\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dfa9d61dde3c7fc246f107f4f9786186}{%
           family={Roscher},
           familyi={R\bibinitperiod},
           given={Ribana},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6f2884ff580e1af3b02da17360b3c4ac}{%
           family={Shahzad},
           familyi={S\bibinitperiod},
           given={Muhammad},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9c245130418606dd988ed12a97044d62}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Wen},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7b86c5a1293f66bbd036f3e3fdf5909b}{%
           family={Bamler},
           familyi={B\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e5e6aebfefcbe0d7c5db556334617b20}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Xiao\bibnamedelima Xiang},
           giveni={X\bibinitperiod\bibinitdelim X\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{b244bc1d8342cbf68d363dd761d0f9b2}
      \strng{fullhash}{6a1983024099d69ba26f86229a810b60}
      \strng{bibnamehash}{6a1983024099d69ba26f86229a810b60}
      \strng{authorbibnamehash}{6a1983024099d69ba26f86229a810b60}
      \strng{authornamehash}{b244bc1d8342cbf68d363dd761d0f9b2}
      \strng{authorfullhash}{6a1983024099d69ba26f86229a810b60}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Over the last decade, neural networks have reached almost every field of science and become a crucial part of various real world applications. Due to the increasing spread, confidence in neural network predictions has become more and more important. However, basic neural networks do not deliver certainty estimates or suffer from over- or under-confidence, i.e. are badly calibrated. To overcome this, many researchers have been working on understanding and quantifying uncertainty in a neural network’s prediction. As a result, different types and sources of uncertainty have been identified and various approaches to measure and quantify uncertainty in neural networks have been proposed. This work gives a comprehensive overview of uncertainty estimation in neural networks, reviews recent advances in the field, highlights current challenges, and identifies potential research opportunities. It is intended to give anyone interested in uncertainty estimation in neural networks a broad overview and introduction, without presupposing prior knowledge in this field. For that, a comprehensive introduction to the most crucial sources of uncertainty is given and their separation into reducible model uncertainty and irreducible data uncertainty is presented. The modeling of these uncertainties based on deterministic neural networks, Bayesian neural networks (BNNs), ensemble of neural networks, and test-time data augmentation approaches is introduced and different branches of these fields as well as the latest developments are discussed. For a practical application, we discuss different measures of uncertainty, approaches for calibrating neural networks, and give an overview of existing baselines and available implementations. Different examples from the wide spectrum of challenges in the fields of medical image analysis, robotics, and earth observation give an idea of the needs and challenges regarding uncertainties in the practical applications of neural networks. Additionally, the practical limitations of uncertainty quantification methods in neural networks for mission- and safety-critical real world applications are discussed and an outlook on the next steps towards a broader usage of such methods is given.}
      \field{day}{1}
      \field{issn}{1573-7462}
      \field{journaltitle}{Artificial Intelligence Review}
      \field{langid}{english}
      \field{month}{10}
      \field{number}{1}
      \field{shortjournal}{Artif Intell Rev}
      \field{title}{A Survey of Uncertainty in Deep Neural Networks}
      \field{urlday}{23}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{56}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1513\bibrangedash 1589}
      \range{pages}{77}
      \verb{doi}
      \verb 10.1007/s10462-023-10562-9
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\39QQTBNQ\Gawlikowski et al_2023_A survey of uncertainty in deep neural networks.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10462-023-10562-9
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10462-023-10562-9
      \endverb
      \keyw{Bayesian deep neural networks,Calibration,Ensembles,gelesen,Test-time augmentation,Uncertainty,ungelesen,zitiert}
    \endentry
    \entry{gmComprehensiveSurveyAnalysis2020}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=d3d0adaab03b7955420f1ab891aa3807}{%
           family={Gm},
           familyi={G\bibinitperiod},
           given={Harshvardhan},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f50133ff053db4c0bb7d91a106ad7a06}{%
           family={Gourisaria},
           familyi={G\bibinitperiod},
           given={Mahendra\bibnamedelima Kumar},
           giveni={M\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c22c77f34b3829214c95bbd45e51cc3e}{%
           family={Pandey},
           familyi={P\bibinitperiod},
           given={Manjusha},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=02ade2c1f257048ee172c42315dbf72e}{%
           family={Rautaray},
           familyi={R\bibinitperiod},
           given={Siddharth\bibnamedelima Swarup},
           giveni={S\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{044298a65eaa00f1a819b35323faf4a1}
      \strng{fullhash}{2f569691646b68d5faf918ddbf9f9350}
      \strng{bibnamehash}{2f569691646b68d5faf918ddbf9f9350}
      \strng{authorbibnamehash}{2f569691646b68d5faf918ddbf9f9350}
      \strng{authornamehash}{044298a65eaa00f1a819b35323faf4a1}
      \strng{authorfullhash}{2f569691646b68d5faf918ddbf9f9350}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Generative models have been in existence for many decades. In the field of machine learning, we come across many scenarios when directly learning a target is intractable through discriminative models, and in such cases the joint distribution of the target and the training data is approximated and generated. These generative models help us better represent or model a set of data by generating data in the form of Markov chains or simply employing a generative iterative process to do the same. With the recent innovation of Generative Adversarial Networks (GANs), it is now possible to make use of AI to generate pieces of art, music, etc. with a high extent of realism. In this paper, we review and analyse critically all the generative models, namely Gaussian Mixture Models (GMM), Hidden Markov Models (HMM), Latent Dirichlet Allocation (LDA), Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBN), Deep Boltzmann Machines (DBM), and GANs. We study their algorithms and implement each of the models to provide the reader some insights on which generative model to pick from while dealing with a problem. We also provide some noteworthy contributions done in the past to these models from the literature.}
      \field{day}{1}
      \field{issn}{1574-0137}
      \field{journaltitle}{Computer Science Review}
      \field{month}{11}
      \field{shortjournal}{Computer Science Review}
      \field{title}{A Comprehensive Survey and Analysis of Generative Models in Machine Learning}
      \field{urlday}{8}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{38}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{100285}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1016/j.cosrev.2020.100285
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\RASD8WLC\S1574013720303853.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S1574013720303853
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S1574013720303853
      \endverb
      \keyw{Bayesian inference,Deep learning,Generative models,Machine learning,Neural networks}
    \endentry
    \entry{gregorPositioningPresentingDesign2013}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=6cb3773d13196927d56a5877ea24b6a2}{%
           family={Gregor},
           familyi={G\bibinitperiod},
           given={Shirley},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e054261c7f1739e1ba1db0a45e385c1b}{%
           family={Hevner},
           familyi={H\bibinitperiod},
           given={Alan\bibnamedelima R.},
           giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Management Information Systems Research Center, University of Minnesota}%
      }
      \strng{namehash}{62477b683a6398d89f0689141480a261}
      \strng{fullhash}{62477b683a6398d89f0689141480a261}
      \strng{bibnamehash}{62477b683a6398d89f0689141480a261}
      \strng{authorbibnamehash}{62477b683a6398d89f0689141480a261}
      \strng{authornamehash}{62477b683a6398d89f0689141480a261}
      \strng{authorfullhash}{62477b683a6398d89f0689141480a261}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Design science research (DSR) has staked its rightful ground as an important and legitimate Information Systems (IS) research paradigm. We contend that DSR has yet to attain its full potential impact on the development and use of information systems due to gaps in the understanding and application of DSR concepts and methods. This essay aims to help researchers (1) appreciate the levels of artifact abstractions that may be DSR contributions, (2) identify appropriate ways of consuming and producing knowledge when they are preparing journal articles or other scholarly works, (3) understand and position the knowledge contributions of their research projects, and (4) structure a DSR article so that it emphasizes significant contributions to the knowledge base. Our focal contribution is the DSR knowledge contribution framework with two dimensions based on the existing state of knowledge in both the problem and solution domains for the research opportunity under study. In addition, we propose a DSR communication schema with similarities to more conventional publication patterns, but which substitutes the description of the DSR artifact in place of a traditional results section. We evaluate the DSR contribution framework and the DSR communication schema via examinations of DSR exemplar publications.}
      \field{eprinttype}{jstor}
      \field{issn}{0276-7783}
      \field{journaltitle}{MIS Quarterly}
      \field{number}{2}
      \field{title}{Positioning and {{Presenting Design Science Research}} for {{Maximum Impact}}}
      \field{urlday}{28}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{37}
      \field{year}{2013}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{337\bibrangedash 355}
      \range{pages}{19}
      \verb{eprint}
      \verb 43825912
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\477R6UKA\Gregor_Hevner_2013_Positioning and Presenting Design Science Research for Maximum Impact.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.jstor.org/stable/43825912
      \endverb
      \verb{url}
      \verb https://www.jstor.org/stable/43825912
      \endverb
    \endentry
    \entry{gustafssonEnergyBasedModelsDeep2020}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=2a88f827488823d2f90106fe78bd1bd2}{%
           family={Gustafsson},
           familyi={G\bibinitperiod},
           given={Fredrik\bibnamedelima K.},
           giveni={F\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d677155b50f1f1836c11fe2ffcf82285}{%
           family={Danelljan},
           familyi={D\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=47da2823c04e6193acbc7c35e4978b13}{%
           family={Bhat},
           familyi={B\bibinitperiod},
           given={Goutam},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a7d921004899fc1272afda1ee990486b}{%
           family={Schön},
           familyi={S\bibinitperiod},
           given={Thomas\bibnamedelima B.},
           giveni={T\bibinitperiod\bibinitdelim B\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{4}{}{%
        {{hash=85ab53268da1006b51b453d57d3566f2}{%
           family={Vedaldi},
           familyi={V\bibinitperiod},
           given={Andrea},
           giveni={A\bibinitperiod}}}%
        {{hash=a3e36d7b360de7388d78599c2446ac34}{%
           family={Bischof},
           familyi={B\bibinitperiod},
           given={Horst},
           giveni={H\bibinitperiod}}}%
        {{hash=b452a32296958371572717940f900884}{%
           family={Brox},
           familyi={B\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=4af0c62bceb93c143845555052179bc8}{%
           family={Frahm},
           familyi={F\bibinitperiod},
           given={Jan-Michael},
           giveni={J\bibinithyphendelim M\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{b610072a77f150fe3a212b2a1febaedf}
      \strng{fullhash}{20d96bfea0ca9c08009a5e2d5a7059fa}
      \strng{bibnamehash}{20d96bfea0ca9c08009a5e2d5a7059fa}
      \strng{authorbibnamehash}{20d96bfea0ca9c08009a5e2d5a7059fa}
      \strng{authornamehash}{b610072a77f150fe3a212b2a1febaedf}
      \strng{authorfullhash}{20d96bfea0ca9c08009a5e2d5a7059fa}
      \strng{editorbibnamehash}{7914c282424cc181ca1aa3c2103905d2}
      \strng{editornamehash}{282d479612a43d3afb0ccff664d33d17}
      \strng{editorfullhash}{7914c282424cc181ca1aa3c2103905d2}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{While deep learning-based classification is generally tackled using standardized approaches, a wide variety of techniques are employed for regression. In computer vision, one particularly popular such technique is that of confidence-based regression, which entails predicting a confidence value for each input-target pair (x,~y). While this approach has demonstrated impressive results, it requires important task-dependent design choices, and the predicted confidences lack a natural probabilistic meaning. We address these issues by proposing a general and conceptually simple regression method with a clear probabilistic interpretation. In our proposed approach, we create an energy-based model of the conditional target density p(y|x), using a deep neural network to predict the un-normalized density from (x,~y). This model of p(y|x) is trained by directly minimizing the associated negative log-likelihood, approximated using Monte Carlo sampling. We perform comprehensive experiments on four computer vision regression tasks. Our approach outperforms direct regression, as well as other probabilistic and confidence-based methods. Notably, our model achieves a \$\$2.2\textbackslash\%\$\$2.2\%AP improvement over Faster-RCNN for object detection on the COCO dataset, and sets a new state-of-the-art on visual tracking when applied for bounding box estimation. In contrast to confidence-based methods, our approach is also shown to be directly applicable to more general tasks such as age and head-pose estimation. Code is available at https://github.com/fregu856/ebms\_regression.}
      \field{booktitle}{Computer {{Vision}} – {{ECCV}} 2020}
      \field{isbn}{978-3-030-58565-5}
      \field{langid}{english}
      \field{series}{Lecture {{Notes}} in {{Computer Science}}}
      \field{title}{Energy-{{Based Models}} for {{Deep Probabilistic Regression}}}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{pages}{325\bibrangedash 343}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1007/978-3-030-58565-5_20
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\NAN4GFF3\Gustafsson et al_2020_Energy-Based Models for Deep Probabilistic Regression.pdf
      \endverb
      \keyw{ungelesen}
    \endentry
    \entry{helmenstineHowManyAtoms2022}{online}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=c20d77d69a6b79fc4ab830812f8af0e2}{%
           family={Helmenstine},
           familyi={H\bibinitperiod},
           given={Anne},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{organization}{2}{%
        {Science Notes}%
        {Projects}%
      }
      \strng{namehash}{c20d77d69a6b79fc4ab830812f8af0e2}
      \strng{fullhash}{c20d77d69a6b79fc4ab830812f8af0e2}
      \strng{bibnamehash}{c20d77d69a6b79fc4ab830812f8af0e2}
      \strng{authorbibnamehash}{c20d77d69a6b79fc4ab830812f8af0e2}
      \strng{authornamehash}{c20d77d69a6b79fc4ab830812f8af0e2}
      \strng{authorfullhash}{c20d77d69a6b79fc4ab830812f8af0e2}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Learn how many atoms are in the world and how to perform the calculation. Compare this number to the number of way to order playing cards.}
      \field{day}{10}
      \field{hour}{19}
      \field{langid}{american}
      \field{minute}{0}
      \field{month}{5}
      \field{second}{47}
      \field{timezone}{Z}
      \field{title}{How {{Many Atoms Are}} in the {{World}}?}
      \field{urlday}{21}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\I5MHB32W\how-many-atoms-are-in-the-world.html
      \endverb
      \verb{urlraw}
      \verb https://sciencenotes.org/how-many-atoms-are-in-the-world/
      \endverb
      \verb{url}
      \verb https://sciencenotes.org/how-many-atoms-are-in-the-world/
      \endverb
      \keyw{gelesen,ungelesen,zitiert}
    \endentry
    \entry{hevnerDesignScienceInformation2004a}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=e054261c7f1739e1ba1db0a45e385c1b}{%
           family={Hevner},
           familyi={H\bibinitperiod},
           given={Alan\bibnamedelima R.},
           giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5aec260b5b1381edc4b8ff6755f2d543}{%
           family={March},
           familyi={M\bibinitperiod},
           given={Salvatore\bibnamedelima T.},
           giveni={S\bibinitperiod\bibinitdelim T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c518c214dc9e77148493e9ea47f505ab}{%
           family={Park},
           familyi={P\bibinitperiod},
           given={Jinsoo},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=65565df90d4793da7a2f20e5e1b07345}{%
           family={Ram},
           familyi={R\bibinitperiod},
           given={Sudha},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Management Information Systems Research Center, University of Minnesota}%
      }
      \strng{namehash}{77b3350005135e016e7a83431f326d8f}
      \strng{fullhash}{b163bef761d9a2c79a3d232595321775}
      \strng{bibnamehash}{b163bef761d9a2c79a3d232595321775}
      \strng{authorbibnamehash}{b163bef761d9a2c79a3d232595321775}
      \strng{authornamehash}{77b3350005135e016e7a83431f326d8f}
      \strng{authorfullhash}{b163bef761d9a2c79a3d232595321775}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.}
      \field{eprinttype}{jstor}
      \field{issn}{0276-7783}
      \field{journaltitle}{MIS Quarterly}
      \field{number}{1}
      \field{title}{Design {{Science}} in {{Information Systems Research}}}
      \field{urlday}{28}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{28}
      \field{year}{2004}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{75\bibrangedash 105}
      \range{pages}{31}
      \verb{doi}
      \verb 10.2307/25148625
      \endverb
      \verb{eprint}
      \verb 25148625
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\6PLSKXEF\Hevner et al_2004_Design Science in Information Systems Research.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.jstor.org/stable/25148625
      \endverb
      \verb{url}
      \verb https://www.jstor.org/stable/25148625
      \endverb
    \endentry
    \entry{hintemannDataCenters20212022}{book}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=a9e782d1d039365b8dc5e24636eedf4c}{%
           family={Hintemann},
           familyi={H\bibinitperiod},
           given={Ralph},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=85defdfb3a3c4a9b420d361bfef7e5ae}{%
           family={Hinterholzer},
           familyi={H\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{f6cf1ca829526a1acf0cb8b554caf6ac}
      \strng{fullhash}{f6cf1ca829526a1acf0cb8b554caf6ac}
      \strng{bibnamehash}{f6cf1ca829526a1acf0cb8b554caf6ac}
      \strng{authorbibnamehash}{f6cf1ca829526a1acf0cb8b554caf6ac}
      \strng{authornamehash}{f6cf1ca829526a1acf0cb8b554caf6ac}
      \strng{authorfullhash}{f6cf1ca829526a1acf0cb8b554caf6ac}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The energy consumption of data centers continues to increase. At 17 billion kWh, data centers consumed 6.5 \% more electricity in 2021 than in 2020. The main reason for the growth in energy consumption is the expansion of cloud data centers in Germany and the associated increase in the number of large data centers. However, traditional data centers operated by companies themselves also continue to have a high share of data center capacities in Germany.}
      \field{day}{5}
      \field{month}{8}
      \field{shorttitle}{Data Centers 2021}
      \field{title}{Data Centers 2021: {{Data}} Center Boom in {{Germany}} Continues - {{Cloud}} Computing Drives the Growth of the Data Center Industry and Its Energy Consumption}
      \field{year}{2022}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.13140/RG.2.2.31826.43207
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\8S6LNT75\Data centers 2021 Data center boom in Germany continues.pdf
      \endverb
      \keyw{ungelesen,zitiert}
    \endentry
    \entry{hintonBoltzmannMachines2014}{incollection}{}
      \name{author}{1}{}{%
        {{un=1,uniquepart=given,hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod},
           givenun=1}}%
      }
      \name{editor}{2}{}{%
        {{hash=e94495f7b9c258a3a6722a976198d15b}{%
           family={Sammut},
           familyi={S\bibinitperiod},
           given={Claude},
           giveni={C\bibinitperiod}}}%
        {{hash=021b501817a8d003c9b230aaed364b70}{%
           family={Webb},
           familyi={W\bibinitperiod},
           given={Geoffrey\bibnamedelima I.},
           giveni={G\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Boston, MA}%
      }
      \list{publisher}{1}{%
        {Springer US}%
      }
      \strng{namehash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{fullhash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{bibnamehash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{authorbibnamehash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{authornamehash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{authorfullhash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{editorbibnamehash}{e7ced06e01b6a1273dcf5a7a3f56d1e4}
      \strng{editornamehash}{e7ced06e01b6a1273dcf5a7a3f56d1e4}
      \strng{editorfullhash}{e7ced06e01b6a1273dcf5a7a3f56d1e4}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Encyclopedia of {{Machine Learning}} and {{Data Mining}}}
      \field{isbn}{978-1-4899-7502-7}
      \field{langid}{english}
      \field{title}{Boltzmann {{Machines}}}
      \field{urlday}{16}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{year}{2014}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 7}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1007/978-1-4899-7502-7_31-1
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\4JUU99AL\Hinton - 2014 - Boltzmann Machines.pdf
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/10.1007/978-1-4899-7502-7_31-1
      \endverb
      \verb{url}
      \verb https://link.springer.com/10.1007/978-1-4899-7502-7_31-1
      \endverb
      \keyw{gelesen,ungelesen,zitiert}
    \endentry
    \entry{hintonPracticalGuideTraining2012a}{incollection}{}
      \name{author}{1}{}{%
        {{un=1,uniquepart=given,hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E.},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
           givenun=1}}%
      }
      \name{editor}{3}{}{%
        {{hash=e0889b25b37f68a2d73747dc5dfcb8a0}{%
           family={Montavon},
           familyi={M\bibinitperiod},
           given={Grégoire},
           giveni={G\bibinitperiod}}}%
        {{hash=a37988d2f8817dd50a03b4bb6e16ad66}{%
           family={Orr},
           familyi={O\bibinitperiod},
           given={Geneviève\bibnamedelima B.},
           giveni={G\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=9f1b6144a45b1967e989e74552e37ada}{%
           family={Müller},
           familyi={M\bibinitperiod},
           given={Klaus-Robert},
           giveni={K\bibinithyphendelim R\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{813bd95fe553e6079cd53a567b238287}
      \strng{fullhash}{813bd95fe553e6079cd53a567b238287}
      \strng{bibnamehash}{813bd95fe553e6079cd53a567b238287}
      \strng{authorbibnamehash}{813bd95fe553e6079cd53a567b238287}
      \strng{authornamehash}{813bd95fe553e6079cd53a567b238287}
      \strng{authorfullhash}{813bd95fe553e6079cd53a567b238287}
      \strng{editorbibnamehash}{e3cedb7c379887c636fcfdc76a12a82d}
      \strng{editornamehash}{e3cedb7c379887c636fcfdc76a12a82d}
      \strng{editorfullhash}{e3cedb7c379887c636fcfdc76a12a82d}
      \field{extraname}{1}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradate}{1}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.}
      \field{booktitle}{Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}}
      \field{isbn}{978-3-642-35289-8}
      \field{langid}{english}
      \field{series}{Lecture {{Notes}} in {{Computer Science}}}
      \field{title}{A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2012}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{599\bibrangedash 619}
      \range{pages}{21}
      \verb{doi}
      \verb 10.1007/978-3-642-35289-8_32
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\VZ5ZDZTH\Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-642-35289-8_32
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-642-35289-8_32
      \endverb
      \keyw{gelesen,Hide Unit,Learning Rate,Reconstruction Error,Restrict Boltzmann Machine,Training Case,ungelesen,zitiert}
    \endentry
    \entry{hintonPracticalGuideTraining2012}{incollection}{}
      \name{author}{1}{}{%
        {{un=1,uniquepart=given,hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E.},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
           givenun=1}}%
      }
      \name{editor}{3}{}{%
        {{hash=e0889b25b37f68a2d73747dc5dfcb8a0}{%
           family={Montavon},
           familyi={M\bibinitperiod},
           given={Grégoire},
           giveni={G\bibinitperiod}}}%
        {{hash=a37988d2f8817dd50a03b4bb6e16ad66}{%
           family={Orr},
           familyi={O\bibinitperiod},
           given={Geneviève\bibnamedelima B.},
           giveni={G\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=9f1b6144a45b1967e989e74552e37ada}{%
           family={Müller},
           familyi={M\bibinitperiod},
           given={Klaus-Robert},
           giveni={K\bibinithyphendelim R\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer Berlin Heidelberg}%
      }
      \strng{namehash}{813bd95fe553e6079cd53a567b238287}
      \strng{fullhash}{813bd95fe553e6079cd53a567b238287}
      \strng{bibnamehash}{813bd95fe553e6079cd53a567b238287}
      \strng{authorbibnamehash}{813bd95fe553e6079cd53a567b238287}
      \strng{authornamehash}{813bd95fe553e6079cd53a567b238287}
      \strng{authorfullhash}{813bd95fe553e6079cd53a567b238287}
      \strng{editorbibnamehash}{e3cedb7c379887c636fcfdc76a12a82d}
      \strng{editornamehash}{e3cedb7c379887c636fcfdc76a12a82d}
      \strng{editorfullhash}{e3cedb7c379887c636fcfdc76a12a82d}
      \field{extraname}{2}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradate}{2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Neural {{Networks}}: {{Tricks}} of the {{Trade}}}
      \field{isbn}{978-3-642-35288-1 978-3-642-35289-8}
      \field{langid}{english}
      \field{title}{A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{7700}
      \field{year}{2012}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{599\bibrangedash 619}
      \range{pages}{21}
      \verb{doi}
      \verb 10.1007/978-3-642-35289-8_32
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\HGALEWG5\Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1007/978-3-642-35289-8_32
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/978-3-642-35289-8_32
      \endverb
    \endentry
    \entry{hizzaniMemristorbasedHardwareAlgorithms2023}{online}{}
      \name{author}{9}{}{%
        {{un=0,uniquepart=base,hash=147438de5e05a025f98f1f50d862b2bc}{%
           family={Hizzani},
           familyi={H\bibinitperiod},
           given={Mohammad},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=43890d8bb55670c13cbd1ff6acfcdc4d}{%
           family={Heittmann},
           familyi={H\bibinitperiod},
           given={Arne},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0b3b5b04c8644dcf8f84dfeb476d2af0}{%
           family={Hutchinson},
           familyi={H\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0b63f82dc427700e6e83212044fc2561}{%
           family={Dobrynin},
           familyi={D\bibinitperiod},
           given={Dmitrii},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=42900b0c4f16b4d6869e64dc8f16701e}{%
           family={Van\bibnamedelima Vaerenbergh},
           familyi={V\bibinitperiod\bibinitdelim V\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f17434480655bd7861ada1e978a75776}{%
           family={Bhattacharya},
           familyi={B\bibinitperiod},
           given={Tinish},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0441b9a36a9847baa4a974256ab936bc}{%
           family={Renaudineau},
           familyi={R\bibinitperiod},
           given={Adrien},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=165140a2ceaaf89bc0aa71af802e6a2d}{%
           family={Strukov},
           familyi={S\bibinitperiod},
           given={Dmitri},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=331a1832282d5aa7a2ec79e22f030a8d}{%
           family={Strachan},
           familyi={S\bibinitperiod},
           given={John\bibnamedelima Paul},
           giveni={J\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{a8665c0fb8fab66c0d2843c6edd7ceaa}
      \strng{fullhash}{90145d6d54f143a4843726d7909fa903}
      \strng{bibnamehash}{90145d6d54f143a4843726d7909fa903}
      \strng{authorbibnamehash}{90145d6d54f143a4843726d7909fa903}
      \strng{authornamehash}{a8665c0fb8fab66c0d2843c6edd7ceaa}
      \strng{authorfullhash}{90145d6d54f143a4843726d7909fa903}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Ising solvers offer a promising physics-based approach to tackle the challenging class of combinatorial optimization problems. However, typical solvers operate in a quadratic energy space, having only pair-wise coupling elements which already dominate area and energy. We show that such quadratization can cause severe problems: increased dimensionality, a rugged search landscape, and misalignment with the original objective function. Here, we design and quantify a higher-order Hopfield optimization solver, with 28nm CMOS technology and memristive couplings for lower area and energy computations. We combine algorithmic and circuit analysis to show quantitative advantages over quadratic Ising Machines (IM)s, yielding 48x and 72x reduction in time-to-solution (TTS) and energy-to-solution (ETS) respectively for Boolean satisfiability problems of 150 variables, with favorable scaling.}
      \field{day}{2}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{11}
      \field{pubstate}{preprint}
      \field{title}{Memristor-Based Hardware and Algorithms for Higher-Order {{Hopfield}} Optimization Solver Outperforming Quadratic {{Ising}} Machines}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2311.01171
      \endverb
      \verb{eprint}
      \verb 2311.01171
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\UWNBRWXG\Hizzani et al. - 2023 - Memristor-based hardware and algorithms for higher.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2311.01171
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2311.01171
      \endverb
      \keyw{Computer Science - Emerging Technologies,Computer Science - Hardware Architecture,ungelesen}
    \endentry
    \entry{hopfieldNeuralNetworksPhysical1982}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=9358686fc958d08cb3cffdebe832f513}{%
           family={Hopfield},
           familyi={H\bibinitperiod},
           given={J\bibnamedelima J},
           giveni={J\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Proceedings of the National Academy of Sciences}%
      }
      \strng{namehash}{9358686fc958d08cb3cffdebe832f513}
      \strng{fullhash}{9358686fc958d08cb3cffdebe832f513}
      \strng{bibnamehash}{9358686fc958d08cb3cffdebe832f513}
      \strng{authorbibnamehash}{9358686fc958d08cb3cffdebe832f513}
      \strng{authornamehash}{9358686fc958d08cb3cffdebe832f513}
      \strng{authorfullhash}{9358686fc958d08cb3cffdebe832f513}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.}
      \field{journaltitle}{Proceedings of the National Academy of Sciences}
      \field{month}{4}
      \field{number}{8}
      \field{title}{Neural Networks and Physical Systems with Emergent Collective Computational Abilities.}
      \field{urlday}{19}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{79}
      \field{year}{1982}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{2554\bibrangedash 2558}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1073/pnas.79.8.2554
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\BF4BJIPP\Hopfield_1982_Neural networks and physical systems with emergent collective computational.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.pnas.org/doi/10.1073/pnas.79.8.2554
      \endverb
      \verb{url}
      \verb https://www.pnas.org/doi/10.1073/pnas.79.8.2554
      \endverb
      \keyw{ungelesen}
    \endentry
    \entry{hsuEffectsGenerativeDiscriminative2010}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=40b75d90ab5a655900eb81a284353aa5}{%
           family={Hsu},
           familyi={H\bibinitperiod},
           given={Anne},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fd3b66b2b5eef2d1be0b8b540fecd15d}{%
           family={Griffiths},
           familyi={G\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{4010429a81c4a712f94a7b939fc43b6c}
      \strng{fullhash}{4010429a81c4a712f94a7b939fc43b6c}
      \strng{bibnamehash}{4010429a81c4a712f94a7b939fc43b6c}
      \strng{authorbibnamehash}{4010429a81c4a712f94a7b939fc43b6c}
      \strng{authornamehash}{4010429a81c4a712f94a7b939fc43b6c}
      \strng{authorfullhash}{4010429a81c4a712f94a7b939fc43b6c}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Rational models of category learning can take two different approaches to representing the relationship between objects and categories. The generative approach solves the categorization problem by building a probabilistic model of each category and using Bayes’ rule to infer category labels. In contrast, the discriminative approach directly learns a mapping between inputs and category labels. With this distinction in mind, we revisit a previously studied categorization experiment that showed people are biased towards categorizing objects into a category with higher variability. Modelling results predict that generative learners should be more greatly affected by category variability than discriminative learners. We show that humans can be prompted to adopt either a generative or discriminative approach to learning the same input, resulting in the predicted effect on use of category variability.}
      \field{day}{1}
      \field{journaltitle}{Proceedings of 32nd Annual Conference of the Cognitive Science Society}
      \field{month}{2}
      \field{shortjournal}{Proceedings of 32nd Annual Conference of the Cognitive Science Society}
      \field{title}{Effects of {{Generative}} and {{Discriminative Learning}} on {{Use}} of {{Category Variability}}}
      \field{year}{2010}
      \field{dateera}{ce}
    \endentry
    \entry{huSurveyConvolutionalNeural2022}{inproceedings}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=664faf5c4926697a847b5938e1b92da3}{%
           family={Hu},
           familyi={H\bibinitperiod},
           given={Yunxiang},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=1,uniquepart=given,hash=d7964b7344608750fbd75e3a3d8a9025}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Yuhao},
           giveni={Y\bibinitperiod},
           givenun=1}}%
        {{un=1,uniquepart=given,hash=227a238900964dd461e702a6d3e6b9eb}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Zhuovuan},
           giveni={Z\bibinitperiod},
           givenun=1}}%
      }
      \strng{namehash}{d61c00c2d6793b68c7b3224bad27b28b}
      \strng{fullhash}{d61c00c2d6793b68c7b3224bad27b28b}
      \strng{bibnamehash}{d61c00c2d6793b68c7b3224bad27b28b}
      \strng{authorbibnamehash}{d61c00c2d6793b68c7b3224bad27b28b}
      \strng{authornamehash}{d61c00c2d6793b68c7b3224bad27b28b}
      \strng{authorfullhash}{d61c00c2d6793b68c7b3224bad27b28b}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In recent years, artificial intelligence (AI) has been under rapid development, applied in various areas. Among a vast number of neural network (NN) models, the convolutional neural network (CNN) has a mainstream status in application such as image and sound recognition and machine decision. The convolution operation is the most complex and requires acceleration. A practical method is to optimize the architecture of the deep learning processor (DLP). The traditional CPU architecture lacks parallelism and memory bandwidth and is not suitable for CNN operations. Current researches are focused on graphic processing unit (GPU), field programmable gate array (FPGA) and application specific integrated circuit (ASIC). GPU is the maturest and the most widely applied, however it is not flexible and has high cost and energy consumption. Even though FPGA possesses high flexibility and low energy consumption, it is inferior in performance. ASIC, due to targeted design, is advanced in performance and energy consumption. However, it is highly inflexible. This article reviews the research outcomes of the three classic types of processors applied to CNN, and put forward the future research trend. In particular, this paper analyzes and compares the experimental performance of several processors of different types, and then summarizes the respective advantageous application fields. Hence, the novelty of this article is in the summary of practical DLPs, which is expected to provide helps for the AI researchers, and guide the selection of CNN-supporting hardware in industrial application.}
      \field{booktitle}{2022 14th {{International Conference}} on {{Computer Research}} and {{Development}} ({{ICCRD}})}
      \field{eventtitle}{2022 14th {{International Conference}} on {{Computer Research}} and {{Development}} ({{ICCRD}})}
      \field{month}{1}
      \field{shorttitle}{A {{Survey}} on {{Convolutional Neural Network Accelerators}}}
      \field{title}{A {{Survey}} on {{Convolutional Neural Network Accelerators}}: {{GPU}}, {{FPGA}} and {{ASIC}}}
      \field{urlday}{19}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{100\bibrangedash 107}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/ICCRD54409.2022.9730377
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\AZ2KN6IP\\Hu et al_2022_A Survey on Convolutional Neural Network Accelerators.pdf;C\:\\Users\\simon\\Zotero\\storage\\V7IPZZFL\\9730377.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/9730377
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/9730377
      \endverb
      \keyw{Application specific integrated circuits,ASIC,Computer architecture,convolutional neural network,Deep learning,deep learning accelerator,Energy consumption,FPGA,GPU,Graphics processing units,Market research,Training}
    \endentry
    \entry{huembeliPhysicsEnergybasedModels2022}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=0dc141a1accc8e42cabfabb2f4c82de1}{%
           family={Huembeli},
           familyi={H\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=13964e1d0dff462d866640377d7f11d8}{%
           family={Arrazola},
           familyi={A\bibinitperiod},
           given={Juan\bibnamedelima Miguel},
           giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7ceb1ede08b81c0ede1cc933ce63d461}{%
           family={Killoran},
           familyi={K\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3660f6bf9d4821aa4c362d0abd316e4f}{%
           family={Mohseni},
           familyi={M\bibinitperiod},
           given={Masoud},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f5a146c5713dd2c5d0bae5d5b944d722}{%
           family={Wittek},
           familyi={W\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{7d87bf4fa2ed694ab6da611b8fa9d5f8}
      \strng{fullhash}{94c9be7f536a1d5ebbb7fab5f33d31c0}
      \strng{bibnamehash}{94c9be7f536a1d5ebbb7fab5f33d31c0}
      \strng{authorbibnamehash}{94c9be7f536a1d5ebbb7fab5f33d31c0}
      \strng{authornamehash}{7d87bf4fa2ed694ab6da611b8fa9d5f8}
      \strng{authorfullhash}{94c9be7f536a1d5ebbb7fab5f33d31c0}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Energy-based models (EBMs) are experiencing a resurgence of interest in both the physics community and the machine learning community. This article provides an intuitive introduction to EBMs, without requiring any background in machine learning, connecting elementary concepts from physics with basic concepts and tools in generative models, and finally giving a perspective where current research in the field is heading. This article, in its original form, was written as an online lecture note in HTML and Javascript and contains interactive graphics. We recommend the reader to also visit the interactive version.}
      \field{day}{6}
      \field{issn}{2524-4914}
      \field{journaltitle}{Quantum Machine Intelligence}
      \field{langid}{english}
      \field{month}{1}
      \field{number}{1}
      \field{shortjournal}{Quantum Mach. Intell.}
      \field{title}{The Physics of Energy-Based Models}
      \field{urlday}{19}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{4}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1007/s42484-021-00057-7
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\5SE9BGQ4\The physics of energy-based models.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s42484-021-00057-7
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s42484-021-00057-7
      \endverb
      \keyw{Energy-based models,gelesen,Gibbs sampling,Machine learning,Spin glasses,ungelesen,zitiert}
    \endentry
    \entry{isingBeitragZurTheorie1925}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=ac607f8a2f05ebe851db6cea5d807d8f}{%
           family={Ising},
           familyi={I\bibinitperiod},
           given={Ernst},
           giveni={E\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{ac607f8a2f05ebe851db6cea5d807d8f}
      \strng{fullhash}{ac607f8a2f05ebe851db6cea5d807d8f}
      \strng{bibnamehash}{ac607f8a2f05ebe851db6cea5d807d8f}
      \strng{authorbibnamehash}{ac607f8a2f05ebe851db6cea5d807d8f}
      \strng{authornamehash}{ac607f8a2f05ebe851db6cea5d807d8f}
      \strng{authorfullhash}{ac607f8a2f05ebe851db6cea5d807d8f}
      \field{sortinit}{I}
      \field{sortinithash}{8d291c51ee89b6cd86bf5379f0b151d8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{day}{1}
      \field{issn}{0044-3328}
      \field{journaltitle}{Zeitschrift für Physik}
      \field{langid}{ngerman}
      \field{month}{2}
      \field{number}{1}
      \field{shortjournal}{Z. Physik}
      \field{title}{Beitrag zur Theorie des Ferromagnetismus}
      \field{urlday}{21}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{31}
      \field{year}{1925}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{253\bibrangedash 258}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1007/BF02980577
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\6PRA279U\Ising_1925_Beitrag zur Theorie des Ferromagnetismus.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/BF02980577
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/BF02980577
      \endverb
    \endentry
    \entry{izadkhahNPNPCompleteNPHard2022}{incollection}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=a8f76981ca037918ff0c9d12f83a8222}{%
           family={Izadkhah},
           familyi={I\bibinitperiod},
           given={Habib},
           giveni={H\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{1}{}{%
        {{hash=a8f76981ca037918ff0c9d12f83a8222}{%
           family={Izadkhah},
           familyi={I\bibinitperiod},
           given={Habib},
           giveni={H\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{a8f76981ca037918ff0c9d12f83a8222}
      \strng{fullhash}{a8f76981ca037918ff0c9d12f83a8222}
      \strng{bibnamehash}{a8f76981ca037918ff0c9d12f83a8222}
      \strng{authorbibnamehash}{a8f76981ca037918ff0c9d12f83a8222}
      \strng{authornamehash}{a8f76981ca037918ff0c9d12f83a8222}
      \strng{authorfullhash}{a8f76981ca037918ff0c9d12f83a8222}
      \strng{editorbibnamehash}{a8f76981ca037918ff0c9d12f83a8222}
      \strng{editornamehash}{a8f76981ca037918ff0c9d12f83a8222}
      \strng{editorfullhash}{a8f76981ca037918ff0c9d12f83a8222}
      \field{sortinit}{I}
      \field{sortinithash}{8d291c51ee89b6cd86bf5379f0b151d8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The theory of computational complexity examines the difficulty of solving problems by computer (more precisely, algorithmically). In this theory, the complexity of problem definitions is classified into two sets; P which denotes “Polynomial” time and NP which indicates “Non-deterministic Polynomial” time. There are also NP-Hard and NP-Complete sets, which we use to express more complex problems. This chapter provides 87 exercises for addressing the theory of computational complexity.}
      \field{booktitle}{Problems on {{Algorithms}}: {{A Comprehensive Exercise Book}} for {{Students}} in {{Software Engineering}}}
      \field{isbn}{978-3-031-17043-0}
      \field{langid}{english}
      \field{title}{P, {{NP}}, {{NP-Complete}}, and~{{NP-Hard Problems}}}
      \field{urlday}{21}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{497\bibrangedash 511}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1007/978-3-031-17043-0_15
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\Q7PFEYG3\Izadkhah_2022_P, NP, NP-Complete, and NP-Hard Problems.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-031-17043-0_15
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-031-17043-0_15
      \endverb
    \endentry
    \entry{kellnerSoftwareProcessSimulation1999}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=1b861f8b6f8fc9f54ff180a18e7b656c}{%
           family={Kellner},
           familyi={K\bibinitperiod},
           given={Marc\bibnamedelima I},
           giveni={M\bibinitperiod\bibinitdelim I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5f204d205ba0737e9f58ecff4468f2ad}{%
           family={Madachy},
           familyi={M\bibinitperiod},
           given={Raymond\bibnamedelima J},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f4b9e654ddd38375220c67b8db19906a}{%
           family={Raffo},
           familyi={R\bibinitperiod},
           given={David\bibnamedelima M},
           giveni={D\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{040c47a793e0e296b1b487535e30e477}
      \strng{fullhash}{040c47a793e0e296b1b487535e30e477}
      \strng{bibnamehash}{040c47a793e0e296b1b487535e30e477}
      \strng{authorbibnamehash}{040c47a793e0e296b1b487535e30e477}
      \strng{authornamehash}{040c47a793e0e296b1b487535e30e477}
      \strng{authorfullhash}{040c47a793e0e296b1b487535e30e477}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Software process simulation modeling is increasingly being used to address a variety of issues from the strategic management of software development, to supporting process improvements, to software project management training. The scope of software process simulation applications ranges from narrow focused portions of the life cycle to longer term product evolutionary models with broad organizational impacts. This article provides an overview of work being conducted in this field. It identifies the questions and issues that simulation can be used to address (`why'), the scope and variables that can be usefully simulated (`what'), and the modeling approaches and techniques that can be most productively employed (`how'). It includes a summary of the papers in this special issue of the Journal of Systems and Software, which were presented at the First International Silver Falls Workshop on Software Process Simulation Modeling (ProSim'98). It also provides a framework that helps characterize work in this field, and applies this new characterization scheme to many of the articles in this special issue. This paper concludes by offering some guidance in selecting a simulation modeling approach for practical application, and recommending some issues warranting additional research.}
      \field{day}{15}
      \field{issn}{0164-1212}
      \field{journaltitle}{Journal of Systems and Software}
      \field{month}{4}
      \field{number}{2}
      \field{shortjournal}{Journal of Systems and Software}
      \field{shorttitle}{Software Process Simulation Modeling}
      \field{title}{Software Process Simulation Modeling: {{Why}}? {{What}}? {{How}}?}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{volume}{46}
      \field{year}{1999}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{91\bibrangedash 105}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1016/S0164-1212(99)00003-5
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\CEGDAX75\S0164121299000035.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0164121299000035
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0164121299000035
      \endverb
    \endentry
    \entry{larochelleClassificationUsingDiscriminative2008}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=42a970b0a0f1ed24b23064370cc9392f}{%
           family={Larochelle},
           familyi={L\bibinitperiod},
           given={Hugo},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{3939458261d536f2d95b5cca16be8906}
      \strng{fullhash}{3939458261d536f2d95b5cca16be8906}
      \strng{bibnamehash}{3939458261d536f2d95b5cca16be8906}
      \strng{authorbibnamehash}{3939458261d536f2d95b5cca16be8906}
      \strng{authornamehash}{3939458261d536f2d95b5cca16be8906}
      \strng{authorfullhash}{3939458261d536f2d95b5cca16be8906}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.}
      \field{booktitle}{Proceedings of the 25th International Conference on {{Machine}} Learning}
      \field{day}{5}
      \field{isbn}{978-1-60558-205-4}
      \field{month}{7}
      \field{series}{{{ICML}} '08}
      \field{title}{Classification Using Discriminative Restricted {{Boltzmann}} Machines}
      \field{urlday}{22}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2008}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{536\bibrangedash 543}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1145/1390156.1390224
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\EPN7UF5H\Larochelle_Bengio_2008_Classification using discriminative restricted Boltzmann machines.pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.1145/1390156.1390224
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.1145/1390156.1390224
      \endverb
      \keyw{gelesen,ungelesen,zitiert}
    \endentry
    \entry{larochelleLearningAlgorithmsClassification2012}{article}{}
      \name{author}{4}{ul=2}{%
        {{un=0,uniquepart=base,hash=42a970b0a0f1ed24b23064370cc9392f}{%
           family={Larochelle},
           familyi={L\bibinitperiod},
           given={Hugo},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ffda50f9ce7d55739ef488df4c0a7d61}{%
           family={Mandel},
           familyi={M\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7045b009b04d57bd2e19b5dfa0864d4f}{%
           family={Pascanu},
           familyi={P\bibinitperiod},
           given={Razvan},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=419350ebbeb4eba5351469f378dee007}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Y.},
           giveni={Y\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{a2c29ccca2ac9de9dae6ab3c7ce7aa20}
      \strng{fullhash}{e0e1d9f5b214b8f851941f4b6661c1e3}
      \strng{bibnamehash}{e0e1d9f5b214b8f851941f4b6661c1e3}
      \strng{authorbibnamehash}{e0e1d9f5b214b8f851941f4b6661c1e3}
      \strng{authornamehash}{a2c29ccca2ac9de9dae6ab3c7ce7aa20}
      \strng{authorfullhash}{e0e1d9f5b214b8f851941f4b6661c1e3}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely semi-supervised and multitask learning.}
      \field{day}{1}
      \field{journaltitle}{The Journal of Machine Learning Research}
      \field{month}{3}
      \field{shortjournal}{The Journal of Machine Learning Research}
      \field{title}{Learning Algorithms for the Classification Restricted {{Boltzmann}} Machine}
      \field{volume}{13}
      \field{year}{2012}
      \field{dateera}{ce}
      \field{pages}{643\bibrangedash 669}
      \range{pages}{27}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\BE788SPH\Larochelle et al_2012_Learning algorithms for the classification restricted Boltzmann machine.pdf
      \endverb
      \keyw{gelesen,ungelesen}
    \endentry
    \entry{lehnertMostResourceEfficient2023}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=0ce382e9c4b7e41db4b053280c52a8e0}{%
           family={Lehnert},
           familyi={L\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=830d58c70e032bcf37b49812e05a78e7}{%
           family={Holzinger},
           familyi={H\bibinitperiod},
           given={Philipp},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=de3ea8b41b0e0b3bb5c2c77ff4bf9c37}{%
           family={Pfenning},
           familyi={P\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d7439028e3fc7cd4e5603906f2751c53}{%
           family={Müller},
           familyi={M\bibinitperiod},
           given={Ralf},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4089a422505d5da7da8c974eef0a9a6d}{%
           family={Reichenbach},
           familyi={R\bibinitperiod},
           given={Marc},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{4d4cff942e5b8066acf7266d01e79a27}
      \strng{fullhash}{3cba400a20ddb0d63542904f0de7b15e}
      \strng{bibnamehash}{3cba400a20ddb0d63542904f0de7b15e}
      \strng{authorbibnamehash}{3cba400a20ddb0d63542904f0de7b15e}
      \strng{authornamehash}{4d4cff942e5b8066acf7266d01e79a27}
      \strng{authorfullhash}{3cba400a20ddb0d63542904f0de7b15e}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Fast and resource-efficient inference in artificial neural networks (ANNs) is of utmost importance and drives many new developments in the area of new hardware architectures, e.g., by means of systolic arrays or algorithmic optimization such as pruning. In this paper, we present a novel method for lowering the computation effort for ANN inference utilizing ideas from information theory. Weight matrices are sliced into submatrices of logarithmic aspect ratios. These slices are then factorized. This reduces the number of required computations without compromising on fully parallel processing. We create a new hardware architecture for this dedicated purpose. We also provide a tool to map these sliced and factorized matrices efficiently to reconfigurable hardware. By comparing to the state of the art FPGA implementations, we can prove our claim by lowering hardware resources measured in look-up-tables (LUTs) by a factor of three to six. Our method does not rely on any particular property of the weight matrices of the ANN. It works for the general task of multiplying an input vector with a constant matrix and is also suitable for digital signal processing beyond ANNs.}
      \field{eventtitle}{{{IEEE Access}}}
      \field{issn}{2169-3536}
      \field{journaltitle}{IEEE Access}
      \field{title}{Most {{Resource Efficient Matrix Vector Multiplication}} on {{FPGAs}}}
      \field{urlday}{16}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{11}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{3881\bibrangedash 3898}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1109/ACCESS.2023.3234622
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\YJ8M8N3A\\Lehnert et al_2023_Most Resource Efficient Matrix Vector Multiplication on FPGAs.pdf;C\:\\Users\\simon\\Zotero\\storage\\YXM7AXQA\\10007836.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/10007836?denied=
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/10007836?denied=
      \endverb
      \keyw{computational efficiency,Computational efficiency,computer architecture,Computer architecture,Constant matrix multiplication,Encoding,Field programmable gate arrays,Matrix decomposition,neural networks,Neural networks,reconfigurable architectures,Reconfigurable architectures,Signal processing algorithms,Sparse matrices}
    \endentry
    \entry{liTemperatureBasedRestricted2016}{article}{}
      \name{author}{7}{}{%
        {{un=0,uniquepart=base,hash=5af8bacca383a9c0ab2f1a4387841e25}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Guoqi},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ec2c9d0720e869194aa6407638458740}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Lei},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c7609922d2d0d41bda4e39b9bd864203}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2b0664cd81b3e08dc190cff141ff81ca}{%
           family={Wen},
           familyi={W\bibinitperiod},
           given={Changyun},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f27ec46ccd272a021fd78bdac92dd490}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=450c46e7672959544a8d1e93543b1fc6}{%
           family={Pei},
           familyi={P\bibinitperiod},
           given={Jing},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=512d467783f6f39ed2b0030f1dd0a31b}{%
           family={Shi},
           familyi={S\bibinitperiod},
           given={Luping},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{b157e56daaa7e7bfa40e697eaf02dee9}
      \strng{fullhash}{8c5c73f7ee5e1a795888f20926a4085d}
      \strng{bibnamehash}{8c5c73f7ee5e1a795888f20926a4085d}
      \strng{authorbibnamehash}{8c5c73f7ee5e1a795888f20926a4085d}
      \strng{authornamehash}{b157e56daaa7e7bfa40e697eaf02dee9}
      \strng{authorfullhash}{8c5c73f7ee5e1a795888f20926a4085d}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Restricted Boltzmann machines (RBMs), which apply graphical models to learning probability distribution over a set of inputs, have attracted much attention recently since being proposed as building blocks of multi-layer learning systems called deep belief networks (DBNs). Note that temperature is a key factor of the Boltzmann distribution that RBMs originate from. However, none of existing schemes have considered the impact of temperature in the graphical model of DBNs. In this work, we propose temperature based restricted Boltzmann machines (TRBMs) which reveals that temperature is an essential parameter controlling the selectivity of the firing neurons in the hidden layers. We theoretically prove that the effect of temperature can be adjusted by setting the parameter of the sharpness of the logistic function in the proposed TRBMs. The performance of RBMs can be improved by adjusting the temperature parameter of TRBMs. This work provides a comprehensive insights into the deep belief networks and deep learning architectures from a physical point of view.}
      \field{day}{13}
      \field{issn}{2045-2322}
      \field{issue}{1}
      \field{journaltitle}{Scientific Reports}
      \field{langid}{english}
      \field{month}{1}
      \field{number}{1}
      \field{shortjournal}{Sci Rep}
      \field{title}{Temperature Based {{Restricted Boltzmann Machines}}}
      \field{urlday}{27}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{6}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{19133}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/srep19133
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\2EB9SJC3\Li et al_2016_Temperature based Restricted Boltzmann Machines.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/srep19133
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/srep19133
      \endverb
      \keyw{Applied physics,Computational science}
    \endentry
    \entry{lucasIsingFormulationsMany2014}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=c24283f2760f25196eeb40788d2733b1}{%
           family={Lucas},
           familyi={L\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Frontiers}%
      }
      \strng{namehash}{c24283f2760f25196eeb40788d2733b1}
      \strng{fullhash}{c24283f2760f25196eeb40788d2733b1}
      \strng{bibnamehash}{c24283f2760f25196eeb40788d2733b1}
      \strng{authorbibnamehash}{c24283f2760f25196eeb40788d2733b1}
      \strng{authornamehash}{c24283f2760f25196eeb40788d2733b1}
      \strng{authorfullhash}{c24283f2760f25196eeb40788d2733b1}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We provide Ising formulations for many NP-complete and NP-hard problems, including all of Karp's 21 NP-complete problems. This collects and extends mappings to the Ising model from partitioning, covering and satisfiability. In each case, the required number of spins is at most cubic in the size of the problem. This work may be useful in designing adiabatic quantum optimization algorithms.}
      \field{day}{12}
      \field{issn}{2296-424X}
      \field{journaltitle}{Frontiers in Physics}
      \field{langid}{english}
      \field{month}{2}
      \field{shortjournal}{Front. Phys.}
      \field{title}{Ising Formulations of Many {{NP}} Problems}
      \field{urlday}{22}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{2}
      \field{year}{2014}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.3389/fphy.2014.00005
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\DPHDRI6B\Lucas_2014_Ising formulations of many NP problems.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.frontiersin.org/articles/10.3389/fphy.2014.00005
      \endverb
      \verb{url}
      \verb https://www.frontiersin.org/articles/10.3389/fphy.2014.00005
      \endverb
      \keyw{adiabatic quantum computation,Algorithms,complexity theory,NP,spin glasses}
    \endentry
    \entry{luccioniPowerHungryProcessing2023}{online}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=99e76cfbcf273c6493ef4ebf46d4a452}{%
           family={Luccioni},
           familyi={L\bibinitperiod},
           given={Alexandra\bibnamedelima Sasha},
           giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7ac0678b9ae57db7ec09752056af6dd4}{%
           family={Jernite},
           familyi={J\bibinitperiod},
           given={Yacine},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3bf550429e9ba5e3e93b995e9b40d85}{%
           family={Strubell},
           familyi={S\bibinitperiod},
           given={Emma},
           giveni={E\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{1699cc98825f5cf8630c68817bb19e78}
      \strng{fullhash}{1699cc98825f5cf8630c68817bb19e78}
      \strng{bibnamehash}{1699cc98825f5cf8630c68817bb19e78}
      \strng{authorbibnamehash}{1699cc98825f5cf8630c68817bb19e78}
      \strng{authornamehash}{1699cc98825f5cf8630c68817bb19e78}
      \strng{authorfullhash}{1699cc98825f5cf8630c68817bb19e78}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of "generality" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.}
      \field{day}{28}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{11}
      \field{pubstate}{preprint}
      \field{shorttitle}{Power {{Hungry Processing}}}
      \field{title}{Power {{Hungry Processing}}: {{Watts Driving}} the {{Cost}} of {{AI Deployment}}?}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2311.16863
      \endverb
      \verb{eprint}
      \verb 2311.16863
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\66VJIJMH\Power Hungry Processing Watts Driving the Cost of AI Deployment.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2311.16863
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2311.16863
      \endverb
      \keyw{Computer Science - Machine Learning,ungelesen,zitiert}
    \endentry
    \entry{luqiRapidSoftwarePrototyping1992}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=3e4ab7d46286f723b511652fe87b5bc4}{%
           family={Luqi},
           familyi={L\bibinitperiod},
           given={L.},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ac84cda69669794d8c128a05e6881f9e}{%
           family={Steigerwald},
           familyi={S\bibinitperiod},
           given={R.},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{2ed4ded8a1103741cd3660190c3df0a9}
      \strng{fullhash}{2ed4ded8a1103741cd3660190c3df0a9}
      \strng{bibnamehash}{2ed4ded8a1103741cd3660190c3df0a9}
      \strng{authorbibnamehash}{2ed4ded8a1103741cd3660190c3df0a9}
      \strng{authornamehash}{2ed4ded8a1103741cd3660190c3df0a9}
      \strng{authorfullhash}{2ed4ded8a1103741cd3660190c3df0a9}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Rapid software prototyping is an iterative software development methodology aimed at improving the analysis, design, and development of proposed systems. The paper describes rapid prototyping at the system and software levels and reviews the characteristics of computer-aided prototyping. The authors then describe the state-of-the-art in rapid prototyping and discuss technologies that improve the future outlook for prototyping, such as prototyping languages, software reuse, and designer interfaces. To add some cohesion to the concepts, they describe the characteristics of a computer-aided rapid prototyping system. Finally, they provide summaries of the outstanding papers that comprise the rapid prototyping mini-track.{$<>$}}
      \field{booktitle}{Proceedings of the {{Twenty-Fifth Hawaii International Conference}} on {{System Sciences}}}
      \field{eventtitle}{Proceedings of the {{Twenty-Fifth Hawaii International Conference}} on {{System Sciences}}}
      \field{month}{1}
      \field{title}{Rapid Software Prototyping}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{volume}{ii}
      \field{year}{1992}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{470\bibrangedash 479 vol.2}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1109/HICSS.1992.183261
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\FZIBWQSI\\Luqi_Steigerwald_1992_Rapid software prototyping.pdf;C\:\\Users\\simon\\Zotero\\storage\\M7ANERCS\\183261.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/183261
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/183261
      \endverb
      \keyw{Aircraft,Delay,Hardware,Humans,Prototypes,Software design,Software prototyping,Software systems,Timing,Virtual prototyping}
    \endentry
    \entry{mackayInformationTheoryInference2003}{book}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=99e8c42965ec01aec3921af3e8ecd658}{%
           family={MacKay},
           familyi={M\bibinitperiod},
           given={David\bibnamedelimb J.\bibnamedelimi C.},
           giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim C\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{fullhash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{bibnamehash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{authorbibnamehash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{authornamehash}{99e8c42965ec01aec3921af3e8ecd658}
      \strng{authorfullhash}{99e8c42965ec01aec3921af3e8ecd658}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Information theory and inference, often taught separately, are here united in one entertaining textbook. These topics lie at the heart of many exciting areas of contemporary science and engineering - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics, and cryptography. This textbook introduces theory in tandem with applications. Information theory is taught alongside practical communication systems, such as arithmetic coding for data compression and sparse-graph codes for error-correction. A toolbox of inference techniques, including message-passing algorithms, Monte Carlo methods, and variational approximations, are developed alongside applications of these tools to clustering, convolutional codes, independent component analysis, and neural networks. The final part of the book describes the state of the art in error-correcting codes, including low-density parity-check codes, turbo codes, and digital fountain codes -- the twenty-first century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, David MacKay's groundbreaking book is ideal for self-learning and for undergraduate or graduate courses. Interludes on crosswords, evolution, and sex provide entertainment along the way. In sum, this is a textbook on information, communication, and coding for a new generation of students, and an unparalleled entry point into these subjects for professionals in areas as diverse as computational biology, financial engineering, and machine learning.}
      \field{day}{25}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-0-521-64298-9}
      \field{langid}{english}
      \field{month}{9}
      \field{pagetotal}{694}
      \field{title}{Information {{Theory}}, {{Inference}} and {{Learning Algorithms}}}
      \field{year}{2003}
      \field{dateera}{ce}
      \verb{eprint}
      \verb AKuMj4PN_EMC
      \endverb
      \keyw{Computers / Artificial Intelligence / Computer Vision & Pattern Recognition,Computers / Computer Science,Computers / Data Science / Data Modeling & Design,Computers / Data Science / Neural Networks,Computers / Information Theory,Mathematics / Algebra / General,Philosophy / Logic,Science / Physics / General,Technology & Engineering / Electronics / General}
    \endentry
    \entry{mahmoodiVersatileStochasticDot2019}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=407b5da92b69c48a9d84c64338e81e88}{%
           family={Mahmoodi},
           familyi={M\bibinitperiod},
           given={M.\bibnamedelimi R.},
           giveni={M\bibinitperiod\bibinitdelim R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=98500b97f7bb7ef06198203fb9c2a638}{%
           family={Prezioso},
           familyi={P\bibinitperiod},
           given={M.},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dd29f5901ef1f2605e12d654aeeb8a91}{%
           family={Strukov},
           familyi={S\bibinitperiod},
           given={D.\bibnamedelimi B.},
           giveni={D\bibinitperiod\bibinitdelim B\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{2326714c689cf992e8977487820a89c6}
      \strng{fullhash}{2326714c689cf992e8977487820a89c6}
      \strng{bibnamehash}{2326714c689cf992e8977487820a89c6}
      \strng{authorbibnamehash}{2326714c689cf992e8977487820a89c6}
      \strng{authornamehash}{2326714c689cf992e8977487820a89c6}
      \strng{authorfullhash}{2326714c689cf992e8977487820a89c6}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The key operation in stochastic neural networks, which have become the state-of-the-art approach for solving problems in machine learning, information theory, and statistics, is a stochastic dot-product. While there have been many demonstrations of dot-product circuits and, separately, of stochastic neurons, the efficient hardware implementation combining both functionalities is still missing. Here we report compact, fast, energy-efficient, and scalable stochastic dot-product circuits based on either passively integrated metal-oxide memristors or embedded floating-gate memories. The circuit’s high performance is due to mixed-signal implementation, while the efficient stochastic operation is achieved by utilizing circuit’s noise, intrinsic and/or extrinsic to the memory cell array. The dynamic scaling of weights, enabled by analog memory devices, allows for efficient realization of different annealing approaches to improve functionality. The proposed approach is experimentally verified for two representative applications, namely by implementing neural network for solving a four-node graph-partitioning problem, and a Boltzmann machine with 10-input and~8-hidden~neurons.}
      \field{day}{8}
      \field{issn}{2041-1723}
      \field{journaltitle}{Nature Communications}
      \field{langid}{english}
      \field{month}{11}
      \field{number}{1}
      \field{shortjournal}{Nat Commun}
      \field{title}{Versatile Stochastic Dot Product Circuits Based on Nonvolatile Memories for High Performance Neurocomputing and Neurooptimization}
      \field{urlday}{26}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{10}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{5113}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/s41467-019-13103-7
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\VA8WRCVN\Mahmoodi et al_2019_Versatile stochastic dot product circuits based on nonvolatile memories for.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s41467-019-13103-7
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s41467-019-13103-7
      \endverb
      \keyw{Electrical and electronic engineering,Electronic devices,Network models}
    \endentry
    \entry{mallComprehensiveReviewDeep2023}{article}{}
      \name{author}{7}{}{%
        {{un=0,uniquepart=base,hash=d10e727ff07e38866502dd3551626934}{%
           family={Mall},
           familyi={M\bibinitperiod},
           given={Pawan\bibnamedelima Kumar},
           giveni={P\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c11236314c6e7f14e2a2ce3388b46687}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Pradeep\bibnamedelima Kumar},
           giveni={P\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=80870078c4a3d682df2cae5ebb4ece7e}{%
           family={Srivastav},
           familyi={S\bibinitperiod},
           given={Swapnita},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=146edb4f12ad2d7709d926063c39a8ac}{%
           family={Narayan},
           familyi={N\bibinitperiod},
           given={Vipul},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7a9bf22b07dc621f233ca30964427c63}{%
           family={Paprzycki},
           familyi={P\bibinitperiod},
           given={Marcin},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b9d62f528cf6adac0b405139898355f1}{%
           family={Jaworska},
           familyi={J\bibinitperiod},
           given={Tatiana},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=46799cc4888f9ff1a8c1f9ae33386e4a}{%
           family={Ganzha},
           familyi={G\bibinitperiod},
           given={Maria},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{8a617cf9ad9e0979226978c0a3bdfd64}
      \strng{fullhash}{b0bd4e94d41c0b8096d561dad4c6acd2}
      \strng{bibnamehash}{b0bd4e94d41c0b8096d561dad4c6acd2}
      \strng{authorbibnamehash}{b0bd4e94d41c0b8096d561dad4c6acd2}
      \strng{authornamehash}{8a617cf9ad9e0979226978c0a3bdfd64}
      \strng{authorfullhash}{b0bd4e94d41c0b8096d561dad4c6acd2}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Artificial Intelligence (AI) solutions have been widely used in healthcare, and recent developments in deep neural networks have contributed to significant advances in medical image processing. Much ongoing research is aimed at helping medical practitioners by providing automated systems to analyze images and diagnose acute diseases, such as brain tumors, bone cancer, breast cancer, bone fracture, and many others. This comprehensive review delivers an overview of recent advances in medical imaging using deep neural networks. In addition to the comprehensive literature review, a summary of openly available data sources and future research directions are outlined.}
      \field{day}{1}
      \field{issn}{2772-4425}
      \field{journaltitle}{Healthcare Analytics}
      \field{month}{12}
      \field{shortjournal}{Healthcare Analytics}
      \field{shorttitle}{A Comprehensive Review of Deep Neural Networks for Medical Image Processing}
      \field{title}{A Comprehensive Review of Deep Neural Networks for Medical Image Processing: {{Recent}} Developments and Future Opportunities}
      \field{urlday}{23}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{4}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{100216}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1016/j.health.2023.100216
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\ZH4THS93\S2772442523000837.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S2772442523000837
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S2772442523000837
      \endverb
      \keyw{Artificial intelligence,Deep neural networks,gelesen,Machine learning,Medical imaging diagnostic analytics,Predictive analytics,ungelesen,zitiert}
    \endentry
    \entry{marinoDeepNeuralNetworks2023}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=c00dc6847d2d64ae7106a0aac8be9a40}{%
           family={Marinó},
           familyi={M\bibinitperiod},
           given={Giosué\bibnamedelima Cataldo},
           giveni={G\bibinitperiod\bibinitdelim C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5607636676ef8d52845d3c25224087d5}{%
           family={Petrini},
           familyi={P\bibinitperiod},
           given={Alessandro},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=568edc5b0f12b125ccc259ea010166eb}{%
           family={Malchiodi},
           familyi={M\bibinitperiod},
           given={Dario},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1098a094294cb173aaad80f688f21f3c}{%
           family={Frasca},
           familyi={F\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{92aa9c8c82efa38fbb4a2057e8747d81}
      \strng{fullhash}{1656622b5108f8a2d2c963f7edf6ffa7}
      \strng{bibnamehash}{1656622b5108f8a2d2c963f7edf6ffa7}
      \strng{authorbibnamehash}{1656622b5108f8a2d2c963f7edf6ffa7}
      \strng{authornamehash}{92aa9c8c82efa38fbb4a2057e8747d81}
      \strng{authorfullhash}{1656622b5108f8a2d2c963f7edf6ffa7}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The state-of-the-art performance for several real-world problems is currently reached by deep and, in particular, convolutional neural networks (CNN). Such learning models exploit recent results in the field of deep learning, leading to highly performing, yet very large neural networks with typically millions to billions of parameters. As a result, such models are often redundant and excessively oversized, with a detrimental effect on the environment in terms of unnecessary energy consumption and a limitation to their deployment on low-resource devices. The necessity for compression techniques able to reduce the number of model parameters and their resource demand is thereby increasingly felt by the research community. In this paper we propose the first extensive comparison, to the best of our knowledge, of the main lossy and structure-preserving approaches to compress pre-trained CNNs, applicable in principle to any existing model. Our study is intended to provide a first and preliminary guidance to choose the most suitable compression technique when there is the need to reduce the occupancy of pre-trained models. Both convolutional and fully-connected layers are included in the analysis. Our experiments involved two pre-trained state-of-the-art CNNs (proposed to solve classification or regression problems) and five benchmarks, and gave rise to important insights about the applicability and performance of such techniques w.r.t.the type of layer to be compressed and the category of problem tackled.}
      \field{day}{1}
      \field{issn}{0925-2312}
      \field{journaltitle}{Neurocomputing}
      \field{month}{2}
      \field{shortjournal}{Neurocomputing}
      \field{shorttitle}{Deep Neural Networks Compression}
      \field{title}{Deep Neural Networks Compression: {{A}} Comparative Survey and Choice Recommendations}
      \field{urlday}{23}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{520}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{152\bibrangedash 170}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1016/j.neucom.2022.11.072
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\TBJBQ945\S0925231222014643.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0925231222014643
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0925231222014643
      \endverb
      \keyw{CNN compression,Connection pruning,Huffman coding,Succinct Deep Neural Networks,Weight quantization,Weight sharing}
    \endentry
    \entry{metropolisEquationStateCalculations1953}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=2b73f47a1a84dd52f45fe57c381c3786}{%
           family={Metropolis},
           familyi={M\bibinitperiod},
           given={Nicholas},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8f2d64584d53ec74892cb43a030d890b}{%
           family={Rosenbluth},
           familyi={R\bibinitperiod},
           given={Arianna\bibnamedelima W.},
           giveni={A\bibinitperiod\bibinitdelim W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=19853e2ef86f6c052dc24bbd3c764f76}{%
           family={Rosenbluth},
           familyi={R\bibinitperiod},
           given={Marshall\bibnamedelima N.},
           giveni={M\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6fc89324948d311f731ac42029f909c2}{%
           family={Teller},
           familyi={T\bibinitperiod},
           given={Augusta\bibnamedelima H.},
           giveni={A\bibinitperiod\bibinitdelim H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e22e18e792a0fca9678ed7c3a29c1fed}{%
           family={Teller},
           familyi={T\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{5e4fd1c4740883d53e259a40b68b0611}
      \strng{fullhash}{a8859960f76f5f3055cd70f2655bf77a}
      \strng{bibnamehash}{a8859960f76f5f3055cd70f2655bf77a}
      \strng{authorbibnamehash}{a8859960f76f5f3055cd70f2655bf77a}
      \strng{authornamehash}{5e4fd1c4740883d53e259a40b68b0611}
      \strng{authorfullhash}{a8859960f76f5f3055cd70f2655bf77a}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.}
      \field{day}{1}
      \field{issn}{0021-9606}
      \field{journaltitle}{The Journal of Chemical Physics}
      \field{month}{6}
      \field{number}{6}
      \field{shortjournal}{The Journal of Chemical Physics}
      \field{title}{Equation of {{State Calculations}} by {{Fast Computing Machines}}}
      \field{urlday}{27}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{21}
      \field{year}{1953}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1087\bibrangedash 1092}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1063/1.1699114
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1063/1.1699114
      \endverb
      \verb{url}
      \verb https://doi.org/10.1063/1.1699114
      \endverb
    \endentry
    \entry{mihramSimulationMethodology1976}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=ba3e451bd1e5ce30476c2a949d1e8100}{%
           family={Mihram},
           familyi={M\bibinitperiod},
           given={G.\bibnamedelimi Arthur},
           giveni={G\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{ba3e451bd1e5ce30476c2a949d1e8100}
      \strng{fullhash}{ba3e451bd1e5ce30476c2a949d1e8100}
      \strng{bibnamehash}{ba3e451bd1e5ce30476c2a949d1e8100}
      \strng{authorbibnamehash}{ba3e451bd1e5ce30476c2a949d1e8100}
      \strng{authornamehash}{ba3e451bd1e5ce30476c2a949d1e8100}
      \strng{authorfullhash}{ba3e451bd1e5ce30476c2a949d1e8100}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Operational researchers, management scientists, and industrial engineers have been asked by Russell Ackoff to become ‘systems scientists’, yet he stated that ‘Systems Science is not a science’. (TIMS Interfaces, 2 (4), 41). A. C. Fabergé (Science184, 1330) notes that the original intent of operational researchers was that they be scientists, ‘trained to observe’. Hugh J. Miser (Operations Research22, 903), views ‘operations research as a science’, noting that its progress indeed is of a cyclic nature.}
      \field{day}{1}
      \field{issn}{1573-7187}
      \field{journaltitle}{Theory and Decision}
      \field{langid}{english}
      \field{month}{2}
      \field{number}{1}
      \field{shortjournal}{Theor Decis}
      \field{title}{Simulation Methodology}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{volume}{7}
      \field{year}{1976}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{67\bibrangedash 94}
      \range{pages}{28}
      \verb{doi}
      \verb 10.1007/BF00141103
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\JLWP34KN\Mihram_1976_Simulation methodology.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/BF00141103
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/BF00141103
      \endverb
      \keyw{Industrial Engineer,Management Scientist,Operation Research,Social Scientist,System Scientist}
    \endentry
    \entry{mocanuTopologicalInsightRestricted2016}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=74fdd3b656fcca3f8900696a23aee55c}{%
           family={Mocanu},
           familyi={M\bibinitperiod},
           given={Decebal\bibnamedelima Constantin},
           giveni={D\bibinitperiod\bibinitdelim C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=05f173e4a381c7001f40e858b8fd8143}{%
           family={Mocanu},
           familyi={M\bibinitperiod},
           given={Elena},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9017d7b33f416e377bb53909ecc5639d}{%
           family={Nguyen},
           familyi={N\bibinitperiod},
           given={Phuong\bibnamedelima H.},
           giveni={P\bibinitperiod\bibinitdelim H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9918b657837dac7f3d816f7951dc3568}{%
           family={Gibescu},
           familyi={G\bibinitperiod},
           given={Madeleine},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=85e45aafbde0191b2e141942bbc0ff43}{%
           family={Liotta},
           familyi={L\bibinitperiod},
           given={Antonio},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{470d49d25db1a7bba96482e834c3ce1b}
      \strng{fullhash}{33e5389957f43bc994cd8790fcc838e1}
      \strng{bibnamehash}{33e5389957f43bc994cd8790fcc838e1}
      \strng{authorbibnamehash}{33e5389957f43bc994cd8790fcc838e1}
      \strng{authornamehash}{470d49d25db1a7bba96482e834c3ce1b}
      \strng{authorfullhash}{33e5389957f43bc994cd8790fcc838e1}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Restricted Boltzmann Machines (RBMs) and models derived from them have been successfully used as basic building blocks in deep artificial neural networks for automatic features extraction, unsupervised weights initialization, but also as density estimators. Thus, their generative and discriminative capabilities, but also their computational time are instrumental to a wide range of applications. Our main contribution is to look at RBMs from a topological perspective, bringing insights from network science. Firstly, here we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which naturally have a small-world topology. Secondly, we demonstrate both on synthetic and real-world datasets that by constraining RBMs and GRBMs to a scale-free topology (while still considering local neighborhoods and data distribution), we reduce the number of weights that need to be computed by a few orders of magnitude, at virtually no loss in generative performance. Thirdly, we show that, for a fixed number of weights, our proposed sparse models (which by design have a higher number of hidden neurons) achieve better generative capabilities than standard fully connected RBMs and GRBMs (which by design have a smaller number of hidden neurons), at no additional computational costs.}
      \field{day}{1}
      \field{issn}{1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{langid}{english}
      \field{month}{9}
      \field{number}{2}
      \field{shortjournal}{Mach Learn}
      \field{title}{A Topological Insight into Restricted {{Boltzmann}} Machines}
      \field{urlday}{22}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{104}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{243\bibrangedash 270}
      \range{pages}{28}
      \verb{doi}
      \verb 10.1007/s10994-016-5570-z
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\524Q4ZX2\Mocanu et al_2016_A topological insight into restricted Boltzmann machines.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10994-016-5570-z
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10994-016-5570-z
      \endverb
      \keyw{Complex networks,Deep learning,Scale-free networks,Small-world networks,Sparse restricted Boltzmann machines}
    \endentry
    \entry{mohseniIsingMachinesHardware2022}{online}{}
      \name{author}{3}{}{%
        {{un=1,uniquepart=given,hash=041d56d53dfb9a46f190fe0be2bb9750}{%
           family={Mohseni},
           familyi={M\bibinitperiod},
           given={Naeimeh},
           giveni={N\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=039009cb81b7d29c91dfb85bc0c381d0}{%
           family={McMahon},
           familyi={M\bibinitperiod},
           given={Peter\bibnamedelima L.},
           giveni={P\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=319c4004a1d1800d25b8f3a4a6c32c9e}{%
           family={Byrnes},
           familyi={B\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{ec1f4ab6ff59e7ccd4db9c96532341f8}
      \strng{fullhash}{ec1f4ab6ff59e7ccd4db9c96532341f8}
      \strng{bibnamehash}{ec1f4ab6ff59e7ccd4db9c96532341f8}
      \strng{authorbibnamehash}{ec1f4ab6ff59e7ccd4db9c96532341f8}
      \strng{authornamehash}{ec1f4ab6ff59e7ccd4db9c96532341f8}
      \strng{authorfullhash}{ec1f4ab6ff59e7ccd4db9c96532341f8}
      \field{extraname}{1}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradate}{1}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Ising machines are hardware solvers which aim to find the absolute or approximate ground states of the Ising model. The Ising model is of fundamental computational interest because it is possible to formulate any problem in the complexity class NP as an Ising problem with only polynomial overhead. A scalable Ising machine that outperforms existing standard digital computers could have a huge impact for practical applications for a wide variety of optimization problems. In this review, we survey the current status of various approaches to constructing Ising machines and explain their underlying operational principles. The types of Ising machines considered here include classical thermal annealers based on technologies such as spintronics, optics, memristors, and digital hardware accelerators; dynamical-systems solvers implemented with optics and electronics; and superconducting-circuit quantum annealers. We compare and contrast their performance using standard metrics such as the ground-state success probability and time-to-solution, give their scaling relations with problem size, and discuss their strengths and weaknesses.}
      \field{day}{1}
      \field{eprintclass}{physics, physics:quant-ph}
      \field{eprinttype}{arxiv}
      \field{month}{4}
      \field{pubstate}{preprint}
      \field{title}{Ising Machines as Hardware Solvers of Combinatorial Optimization Problems}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2204.00276
      \endverb
      \verb{eprint}
      \verb 2204.00276
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\VWIC5SI6\Ising machines Hardware solvers for combinatorial.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2204.00276
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2204.00276
      \endverb
      \keyw{Physics - Applied Physics,Quantum Physics,ungelesen,zitiert}
    \endentry
    \entry{mohseniIsingMachinesHardware2022a}{article}{}
      \name{author}{3}{}{%
        {{un=1,uniquepart=given,hash=041d56d53dfb9a46f190fe0be2bb9750}{%
           family={Mohseni},
           familyi={M\bibinitperiod},
           given={Naeimeh},
           giveni={N\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=039009cb81b7d29c91dfb85bc0c381d0}{%
           family={McMahon},
           familyi={M\bibinitperiod},
           given={Peter\bibnamedelima L.},
           giveni={P\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=319c4004a1d1800d25b8f3a4a6c32c9e}{%
           family={Byrnes},
           familyi={B\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{ec1f4ab6ff59e7ccd4db9c96532341f8}
      \strng{fullhash}{ec1f4ab6ff59e7ccd4db9c96532341f8}
      \strng{bibnamehash}{ec1f4ab6ff59e7ccd4db9c96532341f8}
      \strng{authorbibnamehash}{ec1f4ab6ff59e7ccd4db9c96532341f8}
      \strng{authornamehash}{ec1f4ab6ff59e7ccd4db9c96532341f8}
      \strng{authorfullhash}{ec1f4ab6ff59e7ccd4db9c96532341f8}
      \field{extraname}{2}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradate}{2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Ising machines are hardware solvers that aim to find the absolute or approximate ground states of the Ising model. The Ising model is of fundamental computational interest because any problem in the complexity class NP can be formulated as an Ising problem with only polynomial overhead, and thus a scalable Ising machine that outperforms existing standard digital computers could have a huge impact for practical applications. We survey the status of various approaches to constructing Ising machines and explain their underlying operational principles. The types of Ising machines considered here include classical thermal annealers based on technologies such as spintronics, optics, memristors and digital hardware accelerators; dynamical systems solvers implemented with optics and electronics; and superconducting-circuit quantum annealers. We compare and contrast their performance using standard metrics such as the ground-state success probability and time-to-solution, give their scaling relations with problem size, and discuss their strengths and weaknesses.}
      \field{issn}{2522-5820}
      \field{journaltitle}{Nature Reviews Physics}
      \field{langid}{english}
      \field{month}{6}
      \field{number}{6}
      \field{shortjournal}{Nat Rev Phys}
      \field{title}{Ising Machines as Hardware Solvers of Combinatorial Optimization Problems}
      \field{urlday}{22}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{4}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{363\bibrangedash 379}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1038/s42254-022-00440-8
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\JZRT247M\Mohseni et al_2022_Ising machines as hardware solvers of combinatorial optimization problems.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s42254-022-00440-8
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s42254-022-00440-8
      \endverb
      \keyw{Computational science,Electronics,Information theory and computation,photonics and device physics}
    \endentry
    \entry{nazmbojnordiMemristiveBoltzmannMachine2016}{book}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=83e19a1b13d6dcbcef6ffbc75f9761db}{%
           family={Nazm\bibnamedelima Bojnordi},
           familyi={N\bibinitperiod\bibinitdelim B\bibinitperiod},
           given={Mahdi},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7547bef8ff84453e33abaf06969bda14}{%
           family={Ipek},
           familyi={I\bibinitperiod},
           given={Engin},
           giveni={E\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{88e97592c480643f75c1baa87c92c670}
      \strng{fullhash}{88e97592c480643f75c1baa87c92c670}
      \strng{bibnamehash}{88e97592c480643f75c1baa87c92c670}
      \strng{authorbibnamehash}{88e97592c480643f75c1baa87c92c670}
      \strng{authornamehash}{88e97592c480643f75c1baa87c92c670}
      \strng{authorfullhash}{88e97592c480643f75c1baa87c92c670}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{day}{1}
      \field{month}{3}
      \field{pagetotal}{1}
      \field{shorttitle}{Memristive {{Boltzmann}} Machine}
      \field{title}{Memristive {{Boltzmann}} Machine: {{A}} Hardware Accelerator for Combinatorial Optimization and Deep Learning}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{pages}{13}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1109/HPCA.2016.7446049
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\A9DK674U\Memristive Boltzmann machine A hardware accelerator for combinatorial optimization and deep learning.pdf
      \endverb
      \keyw{ungelesen,zitiert}
    \endentry
    \entry{nelsonSoftwarePrototyping2016}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=b2bfbebfd1f149e756fa6b215cd7a993}{%
           family={Nelson},
           familyi={N\bibinitperiod},
           given={Scott\bibnamedelima D.},
           giveni={S\bibinitperiod\bibinitdelim D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f094d63900d22024bb79708248927cc1}{%
           family={Fiol},
           familyi={F\bibinitperiod},
           given={Guilherme\bibnamedelima Del},
           giveni={G\bibinitperiod\bibinitdelim D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7154aedb082df1572edaf258e4e71b01}{%
           family={Hanseler},
           familyi={H\bibinitperiod},
           given={Haley},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b2e5cc1a4543e9ffcbd9c69b225f2423}{%
           family={Crouch},
           familyi={C\bibinitperiod},
           given={Barbara\bibnamedelima Insley},
           giveni={B\bibinitperiod\bibinitdelim I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c64ffb894d2c72d00d6a060929f7b050}{%
           family={Cummins},
           familyi={C\bibinitperiod},
           given={Mollie\bibnamedelima R.},
           giveni={M\bibinitperiod\bibinitdelim R\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Schattauer GmbH}%
      }
      \strng{namehash}{97967beba26e9327a3c0bcd6dbeec8c5}
      \strng{fullhash}{59f72deab937172d2b9ca2a09d3968b1}
      \strng{bibnamehash}{59f72deab937172d2b9ca2a09d3968b1}
      \strng{authorbibnamehash}{59f72deab937172d2b9ca2a09d3968b1}
      \strng{authornamehash}{97967beba26e9327a3c0bcd6dbeec8c5}
      \strng{authorfullhash}{59f72deab937172d2b9ca2a09d3968b1}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Health information exchange (HIE) between Poison Control Centers (PCCs) and Emergency Departments (EDs) could improve care of poisoned patients. However, PCC information systems are not designed to facilitate HIE with EDs; therefore, we are developing specialized software to support HIE within the normal workflow of the PCC using user-centered design and rapid prototyping. To describe the design of an HIE dashboard and the refinement of user requirements through rapid prototyping. Using previously elicited user requirements, we designed low-fidelity sketches of designs on paper with iterative refinement. Next, we designed an interactive high-fidelity prototype and conducted scenario-based usability tests with end users. Users were asked to think aloud while accomplishing tasks related to a case vignette. After testing, the users provided feedback and evaluated the prototype using the System Usability Scale (SUS). Survey results from three users provided useful feedback that was then incorporated into the design. After achieving a stable design, we used the prototype itself as the specification for development of the actual software. Benefits of prototyping included having 1) subject-matter experts heavily involved with the design; 2) flexibility to make rapid changes, 3) the ability to minimize software development efforts early in the design stage; 4) rapid finalization of requirements; 5) early visualization of designs; 6) and a powerful vehicle for communication of the design to the programmers. Challenges included 1) time and effort to develop the prototypes and case scenarios; 2) no simulation of system performance; 3) not having all proposed functionality available in the final product; and 4) missing needed data elements in the PCC information system.}
      \field{issn}{1869-0327}
      \field{journaltitle}{Applied Clinical Informatics}
      \field{langid}{english}
      \field{month}{1}
      \field{number}{1}
      \field{shortjournal}{Appl Clin Inform}
      \field{title}{Software Prototyping}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{volume}{07}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{22\bibrangedash 32}
      \range{pages}{11}
      \verb{doi}
      \verb 10.4338/ACI-2015-07-CR-0091
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\ITJYJMKG\Nelson et al_2016_Software prototyping.pdf
      \endverb
      \verb{urlraw}
      \verb http://www.thieme-connect.de/DOI/DOI?10.4338/ACI-2015-07-CR-0091
      \endverb
      \verb{url}
      \verb http://www.thieme-connect.de/DOI/DOI?10.4338/ACI-2015-07-CR-0091
      \endverb
      \keyw{computer interface,human engineering/methods,medical informatics/methods,Software design,user}
    \endentry
    \entry{oesterleMemorandumZurGestaltungsorientierten2010}{article}{}
      \name{author}{9}{}{%
        {{un=0,uniquepart=base,hash=d763870806a4e8c80870751bd0ddf90a}{%
           family={Oesterle},
           familyi={O\bibinitperiod},
           given={Hubert},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8cdc40448cfff3a37807d63b1f93f1b2}{%
           family={Becker},
           familyi={B\bibinitperiod},
           given={Jörg},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4b7024ad91d26df00a73444c3028e564}{%
           family={Hess},
           familyi={H\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9ca796a3dc1bca9e63ec8766fe0b695d}{%
           family={Karagiannis},
           familyi={K\bibinitperiod},
           given={Dimitris},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=97558c30684fb70538b0efcd39836dfd}{%
           family={Krcmar},
           familyi={K\bibinitperiod},
           given={Helmut},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=51ec38edaa291ff068b8b32d4374c57f}{%
           family={Loos},
           familyi={L\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=85cb0075023ecdcd5477885da4861038}{%
           family={Mertens},
           familyi={M\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0f2b8d969e6aa0ef4272d432d49e9ae5}{%
           family={Oberweis},
           familyi={O\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=87e56c9c39b1b742816367b02532f0a6}{%
           family={Sinz},
           familyi={S\bibinitperiod},
           given={Elmar},
           giveni={E\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{85a75641185bfebb7603e2457341d08d}
      \strng{fullhash}{da5948982eb634e4772b3feda04116a9}
      \strng{bibnamehash}{da5948982eb634e4772b3feda04116a9}
      \strng{authorbibnamehash}{da5948982eb634e4772b3feda04116a9}
      \strng{authornamehash}{85a75641185bfebb7603e2457341d08d}
      \strng{authorfullhash}{da5948982eb634e4772b3feda04116a9}
      \field{sortinit}{O}
      \field{sortinithash}{2cd7140a07aea5341f9e2771efe90aae}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Wie kann die Wirtschaftsinformatik am besten aus ihrer Forschung Nutzen für Wirtschaft und Gesellschaft erzeugen? Mit dieser Frage eröffnen die Autoren im vorliegenden Memorandum die Diskussion um die Forschungsparadigmen, die für die Disziplin adäquat sind. Die Autoren treten für die Gestaltungsorientierung der Wirtschaftsinformatik ein, machen sich aber auch explizit für einen Methodenpluralismus stark, begrüßen Diskussionen, Kritik oder Zustimmung und heißen Kommentare jeder Art willkommen, die dazu dienen, die Forschung in der Wirtschaftsinformatik voran zu bringen.}
      \field{day}{1}
      \field{journaltitle}{http://www.alexandria.unisg.ch/Publikationen/71074}
      \field{month}{9}
      \field{shortjournal}{http://www.alexandria.unisg.ch/Publikationen/71074}
      \field{title}{Memorandum Zur Gestaltungsorientierten {{Wirtschaftsinformatik}}}
      \field{volume}{62}
      \field{year}{2010}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.1007/BF03372838
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\2LQHKEVN\Oesterle et al_2010_Memorandum zur gestaltungsorientierten Wirtschaftsinformatik.pdf
      \endverb
    \endentry
    \entry{ortega-zamoranoFPGAHardwareAcceleration2016}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=5671bbae532acc1c3128337d2d5d1a51}{%
           family={Ortega-Zamorano},
           familyi={O\bibinithyphendelim Z\bibinitperiod},
           given={Francisco},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fdbd3ca1e466f17755e8b7a8fd65cf4b}{%
           family={Montemurro},
           familyi={M\bibinitperiod},
           given={Marcelo\bibnamedelima A.},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ca0264ff981600b61ac05b44d32f43b6}{%
           family={Cannas},
           familyi={C\bibinitperiod},
           given={Sergio\bibnamedelima A.},
           giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c8f2b3ba14aac3967322d9462f14de1c}{%
           family={Jerez},
           familyi={J\bibinitperiod},
           given={José\bibnamedelima M.},
           giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fa903837ccc34b239ea1e75b81537016}{%
           family={Franco},
           familyi={F\bibinitperiod},
           given={Leonardo},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{801d73da7665adb91f7253ec3e0da2b4}
      \strng{fullhash}{16a68b322a4694d26e46d8a09175a2d2}
      \strng{bibnamehash}{16a68b322a4694d26e46d8a09175a2d2}
      \strng{authorbibnamehash}{16a68b322a4694d26e46d8a09175a2d2}
      \strng{authornamehash}{801d73da7665adb91f7253ec3e0da2b4}
      \strng{authorfullhash}{16a68b322a4694d26e46d8a09175a2d2}
      \field{sortinit}{O}
      \field{sortinithash}{2cd7140a07aea5341f9e2771efe90aae}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A two-dimensional Ising model with nearest-neighbors ferromagnetic interactions is implemented in a Field Programmable Gate Array (FPGA) board.Extensive Monte Carlo simulations were carried out using an efficient hardware representation of individual spins and a combined global-local LFSR random number generator. Consistent results regarding the descriptive properties of magnetic systems, like energy, magnetization and susceptibility are obtained while a speed-up factor of approximately 6 times is achieved in comparison to previous FPGA-based published works and almost \$10\^{}4\$ times in comparison to a standard CPU simulation. A detailed description of the logic design used is given together with a careful analysis of the quality of the random number generator used. The obtained results confirm the potential of FPGAs for analyzing the statistical mechanics of magnetic systems.}
      \field{day}{1}
      \field{eprintclass}{physics}
      \field{eprinttype}{arxiv}
      \field{issn}{1045-9219}
      \field{journaltitle}{IEEE Transactions on Parallel and Distributed Systems}
      \field{month}{9}
      \field{number}{9}
      \field{shortjournal}{IEEE Trans. Parallel Distrib. Syst.}
      \field{title}{{{FPGA Hardware Acceleration}} of {{Monte Carlo Simulations}} for the {{Ising Model}}}
      \field{urlday}{10}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{volume}{27}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{2618\bibrangedash 2627}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/TPDS.2015.2505725
      \endverb
      \verb{eprint}
      \verb 1602.03016
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\VEEUL2RF\\Ortega-Zamorano et al_2016_FPGA Hardware Acceleration of Monte Carlo Simulations for the Ising Model.pdf;C\:\\Users\\simon\\Zotero\\storage\\TPMM7LBD\\1602.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1602.03016
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1602.03016
      \endverb
      \keyw{Computer Science - Hardware Architecture,Physics - Computational Physics}
    \endentry
    \entry{oesterleKonsortialforschung2010}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=5e8e4a3d1a5ed8d5d8d4cde224ce6589}{%
           family={Österle},
           familyi={Ö\bibinitperiod},
           given={Hubert},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=47b6ff6bcb1c3b107867b3a8f8f2d8db}{%
           family={Otto},
           familyi={O\bibinitperiod},
           given={Boris},
           giveni={B\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{8fa4dbbd3bd74abab1d64b663526e450}
      \strng{fullhash}{8fa4dbbd3bd74abab1d64b663526e450}
      \strng{bibnamehash}{8fa4dbbd3bd74abab1d64b663526e450}
      \strng{authorbibnamehash}{8fa4dbbd3bd74abab1d64b663526e450}
      \strng{authornamehash}{8fa4dbbd3bd74abab1d64b663526e450}
      \strng{authorfullhash}{8fa4dbbd3bd74abab1d64b663526e450}
      \field{sortinit}{Ö}
      \field{sortinithash}{2cd7140a07aea5341f9e2771efe90aae}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Gestaltungsorientierte Forschung in der Wirtschaftsinformatik strebt Ergebnisse an, welche den Anforderungen wissenschaftlicher Strenge und praktischer Relevanz gleichermaßen genügen. Jedoch stehen Forscher heutzutage vor der Herausforderung, überhaupt Zugang zur Wissensbasis in der Praxis zu erhalten und dieses Wissen zu erfassen. Vor diesem Hintergrund schlägt dieser Aufsatz eine Methode für Konsortialforschung vor, welche die multilaterale Zusammenarbeit zwischen Forschern und Praktikern im Forschungsprozess ermöglichen soll. Der Entwurf der Methode basiert auf einem selbstbewertenden Gestaltungsprozess, welcher sich über einen Zeitraum von über zwanzig Jahren erstreckte. Der Aufsatz trägt in zweifacher Weise zur wissenschaftlichen Diskussion bei. Zum einen adressiert er die wissenschaftliche Grundlage gestaltungsorientierter Forschung, denn er liefert Forschern eine Handlungsanleitung für die Zusammenarbeit mit Praktikern bei der Gestaltung von Artefakten. Zum anderen stellt die Methode selbst ein Artefakt dar, also das Ergebnis eines gestaltungsorientierten Forschungsprozesses.}
      \field{day}{1}
      \field{issn}{1861-8936}
      \field{journaltitle}{WIRTSCHAFTSINFORMATIK}
      \field{langid}{ngerman}
      \field{month}{10}
      \field{number}{5}
      \field{shortjournal}{WIRTSCHAFTSINFORMATIK}
      \field{title}{Konsortialforschung}
      \field{urlday}{29}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{52}
      \field{year}{2010}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{273\bibrangedash 285}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1007/s11576-010-0238-y
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\STSEPTIV\Österle_Otto_2010_Konsortialforschung.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s11576-010-0238-y
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s11576-010-0238-y
      \endverb
      \keyw{Consortium research,Design science research,Forschungsmethode,gestaltungsorientierte Forschung,Konsortialforschung,Research method}
    \endentry
    \entry{patronOptimalRelaxationRate2024}{online}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=4afd65509e78390332668806d48ee281}{%
           family={Patrón},
           familyi={P\bibinitperiod},
           given={A.},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6b24b27d5da47bf2abafa5eb986ea02e}{%
           family={Chepelianskii},
           familyi={C\bibinitperiod},
           given={A.\bibnamedelimi D.},
           giveni={A\bibinitperiod\bibinitdelim D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=413089015974b917c594daa48f8e3702}{%
           family={Prados},
           familyi={P\bibinitperiod},
           given={A.},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7f2999f99e8c9643d0ac3710d89f94ce}{%
           family={Trizac},
           familyi={T\bibinitperiod},
           given={E.},
           giveni={E\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{75917b8acabd22e5109a8371ea2e6b4b}
      \strng{fullhash}{17dd2a900c30fcd364eb55d04de4ed89}
      \strng{bibnamehash}{17dd2a900c30fcd364eb55d04de4ed89}
      \strng{authorbibnamehash}{17dd2a900c30fcd364eb55d04de4ed89}
      \strng{authornamehash}{75917b8acabd22e5109a8371ea2e6b4b}
      \strng{authorfullhash}{17dd2a900c30fcd364eb55d04de4ed89}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study the relaxation of the Metropolis Monte Carlo algorithm corresponding to a single particle trapped in a one-dimensional confining potential, with even jump distributions that ensure that the dynamics verifies detailed balance. Previous work suggested that, for smooth jump distributions, the fastest relaxation rate is obtained as a result of the competition between diffusive and rejection-dominated dynamics. In this work, we show that a new regime comes into play for two-peaked jump distributions, where the relaxation dynamics is neither dominated by diffusion nor rejection: the eigenmodes adopt an oscillatory form, reminiscent of charge density waves (CDW) -- thus we term this new regime the CDW regime. Using a combination of numerical and analytical techniques, the parameter regions corresponding to diffusion, rejection, and CDW are characterised, as well as the transition lines between them -- i.e. a phase diagram is built. The optimal relaxation rate is located at the triple point of phase coexistence, where the transition lines (diffusive-rejection, diffusive-CDW, and CDW-rejection) intersect. Our theoretical framework is checked versus the numerical diagonalisation of the master equation. We also briefly discuss more sophisticated attempts at optimising the relaxation rate to equilibrium.}
      \field{day}{17}
      \field{eprintclass}{cond-mat, physics:math-ph}
      \field{eprinttype}{arxiv}
      \field{month}{2}
      \field{pubstate}{preprint}
      \field{title}{On the Optimal Relaxation Rate for the {{Metropolis}} Algorithm in One Dimension}
      \field{urlday}{27}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2402.11267
      \endverb
      \verb{eprint}
      \verb 2402.11267
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\LVWVYVHT\\Patrón et al_2024_On the optimal relaxation rate for the Metropolis algorithm in one dimension.pdf;C\:\\Users\\simon\\Zotero\\storage\\AX9IPXGS\\2402.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2402.11267
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2402.11267
      \endverb
      \keyw{Condensed Matter - Statistical Mechanics,Mathematical Physics}
    \endentry
    \entry{peccerilloSurveyHardwareAccelerators2022}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=887f206aa8f3c4df2fb2b43b1e7e66c2}{%
           family={Peccerillo},
           familyi={P\bibinitperiod},
           given={Biagio},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5f3b920a85365d83812ca43af6c627e9}{%
           family={Mannino},
           familyi={M\bibinitperiod},
           given={Mirco},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ee39d58d30e43f4bb75d68ecb406af67}{%
           family={Mondelli},
           familyi={M\bibinitperiod},
           given={Andrea},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7748b527931a4e6bb3cfbc70fcfa3429}{%
           family={Bartolini},
           familyi={B\bibinitperiod},
           given={Sandro},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{782f5338de8e850da252a764fbbc5c06}
      \strng{fullhash}{4c5140b16ea6c8a80330fcce2cecd8d6}
      \strng{bibnamehash}{4c5140b16ea6c8a80330fcce2cecd8d6}
      \strng{authorbibnamehash}{4c5140b16ea6c8a80330fcce2cecd8d6}
      \strng{authornamehash}{782f5338de8e850da252a764fbbc5c06}
      \strng{authorfullhash}{4c5140b16ea6c8a80330fcce2cecd8d6}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In recent years, the limits of the multicore approach emerged in the so-called “dark silicon” issue and diminishing returns of an ever-increasing core count. Hardware manufacturers, out of necessity, switched their focus to accelerators, a new paradigm that pursues specialization and heterogeneity over generality and homogeneity. They are special-purpose hardware structures separated from the CPU with aspects that exhibit a high degree of variability. We define a taxonomy based on fourteen of these aspects, grouped in four macro-categories: general aspects, host coupling, architecture, and software aspects. According to it, we categorize around 100 accelerators of the last decade from both industry and academia, and critically analyze emerging trends. We complete our discussion with throughput and efficiency figures. Then, we discuss some prominent open challenges that accelerators are facing, analyzing state-of-the-art solutions, and suggesting prospective research directions for the future.}
      \field{day}{1}
      \field{issn}{1383-7621}
      \field{journaltitle}{Journal of Systems Architecture}
      \field{month}{8}
      \field{shortjournal}{Journal of Systems Architecture}
      \field{shorttitle}{A Survey on Hardware Accelerators}
      \field{title}{A Survey on Hardware Accelerators: {{Taxonomy}}, Trends, Challenges, and Perspectives}
      \field{urlday}{19}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{129}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{102561}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1016/j.sysarc.2022.102561
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\K7M9E2SZ\\Peccerillo et al. - 2022 - A survey on hardware accelerators Taxonomy, trend.pdf;C\:\\Users\\simon\\Zotero\\storage\\CDQXTCS2\\S1383762122001138.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S1383762122001138
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S1383762122001138
      \endverb
      \keyw{Accelerators,CGRA,Classification,Data-parallel,Domain-Specific Architectures,Future research directions,Machine Learning,Open challenges,PIM,Survey,Taxonomy}
    \endentry
    \entry{peffersDesignScienceResearch2007a}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=3127efe8cdd4557b7228a8724f568529}{%
           family={Peffers},
           familyi={P\bibinitperiod},
           given={Ken},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8b17b21a3e4c61d1302f46cba38c6e50}{%
           family={Tuunanen},
           familyi={T\bibinitperiod},
           given={Tuure},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=33ad207ea3031c6d744a31f5bc43c547}{%
           family={Rothenberger},
           familyi={R\bibinitperiod},
           given={Marcus\bibnamedelima A.},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=edf3e165ce4429b30d49e353a7d1417a}{%
           family={Chatterjee},
           familyi={C\bibinitperiod},
           given={Samir},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{4ccaed1d47a4875e5f027cdf2d20bbca}
      \strng{fullhash}{0d82c455b50c2b184d97697dac2e8660}
      \strng{bibnamehash}{0d82c455b50c2b184d97697dac2e8660}
      \strng{authorbibnamehash}{0d82c455b50c2b184d97697dac2e8660}
      \strng{authornamehash}{4ccaed1d47a4875e5f027cdf2d20bbca}
      \strng{authorfullhash}{0d82c455b50c2b184d97697dac2e8660}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.}
      \field{issn}{0742-1222, 1557-928X}
      \field{journaltitle}{Journal of Management Information Systems}
      \field{langid}{english}
      \field{month}{12}
      \field{number}{3}
      \field{shortjournal}{Journal of Management Information Systems}
      \field{title}{A {{Design Science Research Methodology}} for {{Information Systems Research}}}
      \field{urlday}{29}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{24}
      \field{year}{2007}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{45\bibrangedash 77}
      \range{pages}{33}
      \verb{doi}
      \verb 10.2753/MIS0742-1222240302
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\S4KX6IGZ\Peffers et al. - 2007 - A Design Science Research Methodology for Informat.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.tandfonline.com/doi/full/10.2753/MIS0742-1222240302
      \endverb
      \verb{url}
      \verb https://www.tandfonline.com/doi/full/10.2753/MIS0742-1222240302
      \endverb
    \endentry
    \entry{ramsauerHopfieldNetworksAll2021}{online}{}
      \name{author}{16}{}{%
        {{un=0,uniquepart=base,hash=f7ddff2d58ece37233af8650f706e499}{%
           family={Ramsauer},
           familyi={R\bibinitperiod},
           given={Hubert},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a51c94d4907b0807915103a6edf992d6}{%
           family={Schäfl},
           familyi={S\bibinitperiod},
           given={Bernhard},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5975ec99dbd3b842ab21f2ab0ac4ae3b}{%
           family={Lehner},
           familyi={L\bibinitperiod},
           given={Johannes},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=199f93e9036c0826afbdd1f1d3a0e4a7}{%
           family={Seidl},
           familyi={S\bibinitperiod},
           given={Philipp},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6bbe4b724e61ee3566ee6103bdd8b6bd}{%
           family={Widrich},
           familyi={W\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5fac86ca6e6b82166085795ec1b1cee8}{%
           family={Adler},
           familyi={A\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b9ded32a88a8a005a3d19eb472e63d7f}{%
           family={Gruber},
           familyi={G\bibinitperiod},
           given={Lukas},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2d115442cb1e7d6828c4eb445ae35e09}{%
           family={Holzleitner},
           familyi={H\bibinitperiod},
           given={Markus},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bf8ac6f796dfe901e6255449ef9d0794}{%
           family={Pavlović},
           familyi={P\bibinitperiod},
           given={Milena},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=563f412962e47abb351da8376e8c7d0b}{%
           family={Sandve},
           familyi={S\bibinitperiod},
           given={Geir\bibnamedelima Kjetil},
           giveni={G\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e91de498b2b27f3f9f58736395f5c9ac}{%
           family={Greiff},
           familyi={G\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3f039034477ef7ec25f2822c0706c426}{%
           family={Kreil},
           familyi={K\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d93f88df1acd17907db2b3033322d2dc}{%
           family={Kopp},
           familyi={K\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dd69f2d35665eb16375f92c741a26393}{%
           family={Klambauer},
           familyi={K\bibinitperiod},
           given={Günter},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4954f897f0e2228c69f8903b2e1e25d1}{%
           family={Brandstetter},
           familyi={B\bibinitperiod},
           given={Johannes},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=41b31e29fb2bdbf9f5c9c1b0d5b3e815}{%
           family={Hochreiter},
           familyi={H\bibinitperiod},
           given={Sepp},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{a62173d0ba59e357adbb69a0bfd8d4bc}
      \strng{fullhash}{7a726b7f6b8d01dc62668791e7436754}
      \strng{bibnamehash}{7a726b7f6b8d01dc62668791e7436754}
      \strng{authorbibnamehash}{7a726b7f6b8d01dc62668791e7436754}
      \strng{authornamehash}{a62173d0ba59e357adbb69a0bfd8d4bc}
      \strng{authorfullhash}{7a726b7f6b8d01dc62668791e7436754}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers}
      \field{day}{28}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{month}{4}
      \field{pubstate}{preprint}
      \field{title}{Hopfield {{Networks}} Is {{All You Need}}}
      \field{urlday}{28}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2008.02217
      \endverb
      \verb{eprint}
      \verb 2008.02217
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\TZQL3XBP\\Ramsauer et al_2021_Hopfield Networks is All You Need.pdf;C\:\\Users\\simon\\Zotero\\storage\\U2TRQQSW\\2008.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2008.02217
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2008.02217
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
    \endentry
    \entry{ranzatoEfficientLearningSparse2006}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,useprefix=false,hash=95ac6c23bb83367a9ed91ef0e828f26e}{%
           family={Ranzato},
           familyi={R\bibinitperiod},
           given={Marc'},
           giveni={M\bibinitperiod},
           givenun=0,
           prefix={aurelio},
           prefixi={a\bibinitperiod},
           prefixun=0}}%
        {{un=0,uniquepart=base,hash=8288873d2b417030806617ca6e4d4282}{%
           family={Poultney},
           familyi={P\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=83e9081b3be58a20d597b22b70648e30}{%
           family={Chopra},
           familyi={C\bibinitperiod},
           given={Sumit},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4bcf4781f71fb60e6af0729e79c3d4cb}{%
           family={Cun},
           familyi={C\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{26a163fbb7cd7fb2d3954f9b1ef189a7}
      \strng{fullhash}{1429fcba444b3a9514a2b9532e14ee36}
      \strng{bibnamehash}{1429fcba444b3a9514a2b9532e14ee36}
      \strng{authorbibnamehash}{1429fcba444b3a9514a2b9532e14ee36}
      \strng{authornamehash}{26a163fbb7cd7fb2d3954f9b1ef189a7}
      \strng{authorfullhash}{1429fcba444b3a9514a2b9532e14ee36}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces "stroke detectors" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Efficient {{Learning}} of {{Sparse Representations}} with an {{Energy-Based Model}}}
      \field{urlday}{7}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{19}
      \field{year}{2006}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\NBUMP2CI\Ranzato et al_2006_Efficient Learning of Sparse Representations with an Energy-Based Model.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2006/hash/87f4d79e36d68c3031ccf6c55e9bbd39-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2006/hash/87f4d79e36d68c3031ccf6c55e9bbd39-Abstract.html
      \endverb
    \endentry
    \entry{raoUltimateGuideASIC}{online}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=0016784ce06d1ce84f04c3a71c68ce9e}{%
           family={Rao},
           familyi={R\bibinitperiod},
           given={Ravi},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{0016784ce06d1ce84f04c3a71c68ce9e}
      \strng{fullhash}{0016784ce06d1ce84f04c3a71c68ce9e}
      \strng{bibnamehash}{0016784ce06d1ce84f04c3a71c68ce9e}
      \strng{authorbibnamehash}{0016784ce06d1ce84f04c3a71c68ce9e}
      \strng{authornamehash}{0016784ce06d1ce84f04c3a71c68ce9e}
      \strng{authorfullhash}{0016784ce06d1ce84f04c3a71c68ce9e}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{url}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{ASICs, or Application-Specific Integrated Circuits, are specialized chips designed to perform specific tasks with high efficiency and precision, offering a powerful solution for a wide range of industries and applications.}
      \field{shorttitle}{The {{Ultimate Guide}} to {{ASIC Design}}}
      \field{title}{The {{Ultimate Guide}} to {{ASIC Design}}: {{From Concept}} to {{Production}}}
      \field{urlday}{3}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{urldateera}{ce}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\TPEHUVA5\the-ultimate-guide-to-asic-design-from-concept-to-production.html
      \endverb
      \verb{urlraw}
      \verb https://www.wevolver.com/article/the-ultimate-guide-to-asic-design-from-concept-to-production, https://www.wevolver.com/article/the-ultimate-guide-to-asic-design-from-concept-to-production
      \endverb
      \verb{url}
      \verb https://www.wevolver.com/article/the-ultimate-guide-to-asic-design-from-concept-to-production,%20https://www.wevolver.com/article/the-ultimate-guide-to-asic-design-from-concept-to-production
      \endverb
    \endentry
    \entry{raschkaMachineLearningPython2020}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=bb679961e3f3f4bba79fc1cf3aa7df1c}{%
           family={Raschka},
           familyi={R\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7b6fb0cc7f06bfa0742ff02f5d1ddf92}{%
           family={Patterson},
           familyi={P\bibinitperiod},
           given={Joshua},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b2f90678fc4237a20d5caac00bb6ef37}{%
           family={Nolet},
           familyi={N\bibinitperiod},
           given={Corey},
           giveni={C\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Multidisciplinary Digital Publishing Institute}%
      }
      \strng{namehash}{98dca74dbbcc1bbd27b5f39fc5afbb71}
      \strng{fullhash}{98dca74dbbcc1bbd27b5f39fc5afbb71}
      \strng{bibnamehash}{98dca74dbbcc1bbd27b5f39fc5afbb71}
      \strng{authorbibnamehash}{98dca74dbbcc1bbd27b5f39fc5afbb71}
      \strng{authornamehash}{98dca74dbbcc1bbd27b5f39fc5afbb71}
      \strng{authorfullhash}{98dca74dbbcc1bbd27b5f39fc5afbb71}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.}
      \field{issn}{2078-2489}
      \field{issue}{4}
      \field{journaltitle}{Information}
      \field{langid}{english}
      \field{month}{4}
      \field{number}{4}
      \field{shorttitle}{Machine {{Learning}} in {{Python}}}
      \field{title}{Machine {{Learning}} in {{Python}}: {{Main Developments}} and {{Technology Trends}} in {{Data Science}}, {{Machine Learning}}, and {{Artificial Intelligence}}}
      \field{urlday}{8}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{volume}{11}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{193}
      \range{pages}{1}
      \verb{doi}
      \verb 10.3390/info11040193
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\A8TV6MPE\Raschka et al_2020_Machine Learning in Python.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.mdpi.com/2078-2489/11/4/193
      \endverb
      \verb{url}
      \verb https://www.mdpi.com/2078-2489/11/4/193
      \endverb
      \keyw{data science,deep learning,GPU computing,machine learning,neural networks,Python}
    \endentry
    \entry{RestrictedBoltzmannMachine}{online}{}
      \list{organization}{1}{%
        {scikit-learn}%
      }
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{url}
      \field{labeltitlesource}{title}
      \field{abstract}{For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model ( ...}
      \field{langid}{english}
      \field{title}{Restricted {{Boltzmann Machine}} Features for Digit Classification}
      \field{urlday}{10}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{urldateera}{ce}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\KZ4Q4B4G\plot_rbm_logistic_classification.html
      \endverb
      \verb{urlraw}
      \verb https://scikit-learn/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html
      \endverb
      \verb{url}
      \verb https://scikit-learn/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html
      \endverb
    \endentry
    \entry{robertMetropolisHastingsAlgorithm2016}{online}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=0a257fbe5a59f3a17cc4e1089b887257}{%
           family={Robert},
           familyi={R\bibinitperiod},
           given={Christian\bibnamedelima P.},
           giveni={C\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{0a257fbe5a59f3a17cc4e1089b887257}
      \strng{fullhash}{0a257fbe5a59f3a17cc4e1089b887257}
      \strng{bibnamehash}{0a257fbe5a59f3a17cc4e1089b887257}
      \strng{authorbibnamehash}{0a257fbe5a59f3a17cc4e1089b887257}
      \strng{authornamehash}{0a257fbe5a59f3a17cc4e1089b887257}
      \strng{authorfullhash}{0a257fbe5a59f3a17cc4e1089b887257}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This short note is a self-contained and basic introduction to the Metropolis-Hastings algorithm, this ubiquitous tool used for producing dependent simulations from an arbitrary distribution. The document illustrates the principles of the methodology on simple examples with R codes and provides references to the recent extensions of the method.}
      \field{day}{27}
      \field{eprintclass}{stat}
      \field{eprinttype}{arxiv}
      \field{month}{1}
      \field{pubstate}{preprint}
      \field{title}{The {{Metropolis-Hastings}} Algorithm}
      \field{urlday}{27}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1504.01896
      \endverb
      \verb{eprint}
      \verb 1504.01896
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\DPSLD9DJ\\Robert_2016_The Metropolis-Hastings algorithm.pdf;C\:\\Users\\simon\\Zotero\\storage\\KKAV6T6V\\1504.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1504.01896
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1504.01896
      \endverb
      \keyw{Statistics - Computation}
    \endentry
    \entry{rosenthalOptimalProposalDistributions2009}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=227f485776cca1977e6e4f80ec964e14}{%
           family={Rosenthal},
           familyi={R\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{227f485776cca1977e6e4f80ec964e14}
      \strng{fullhash}{227f485776cca1977e6e4f80ec964e14}
      \strng{bibnamehash}{227f485776cca1977e6e4f80ec964e14}
      \strng{authorbibnamehash}{227f485776cca1977e6e4f80ec964e14}
      \strng{authornamehash}{227f485776cca1977e6e4f80ec964e14}
      \strng{authorfullhash}{227f485776cca1977e6e4f80ec964e14}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We review recent work concerning optimal proposal scalings for Metropolis-Hastings MCMC algorithms, and adaptive MCMC algorithms for trying to improve the algorithm on the y.}
      \field{day}{29}
      \field{journaltitle}{Handbook of Markov Chain Monte Carlo}
      \field{month}{9}
      \field{shortjournal}{Handbook of Markov Chain Monte Carlo}
      \field{title}{Optimal {{Proposal Distributions}} and {{Adaptive MCMC}}}
      \field{year}{2009}
      \field{dateera}{ce}
    \endentry
    \entry{salakhutdinovDeepBoltzmannMachines2009}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=bd2be300d445e9f6db7808f9533e66cb}{%
           family={Salakhutdinov},
           familyi={S\bibinitperiod},
           given={Ruslan},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=1,uniquepart=given,hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod},
           givenun=1}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{b2a9f3fe9537777d7d4c84a9dad055f1}
      \strng{fullhash}{b2a9f3fe9537777d7d4c84a9dad055f1}
      \strng{bibnamehash}{b2a9f3fe9537777d7d4c84a9dad055f1}
      \strng{authorbibnamehash}{b2a9f3fe9537777d7d4c84a9dad055f1}
      \strng{authornamehash}{b2a9f3fe9537777d7d4c84a9dad055f1}
      \strng{authorfullhash}{b2a9f3fe9537777d7d4c84a9dad055f1}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.}
      \field{booktitle}{Proceedings of the {{Twelth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}
      \field{day}{15}
      \field{eventtitle}{Artificial {{Intelligence}} and {{Statistics}}}
      \field{issn}{1938-7228}
      \field{langid}{english}
      \field{month}{4}
      \field{title}{Deep {{Boltzmann Machines}}}
      \field{urlday}{16}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2009}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{448\bibrangedash 455}
      \range{pages}{8}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\V8LJQ6Z2\Deep Boltzmann Machines.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v5/salakhutdinov09a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v5/salakhutdinov09a.html
      \endverb
      \keyw{ungelesen}
    \endentry
    \entry{sarhadiStateArtHardware2015}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=c9d504acb238c16d402a2aa82ea88a6c}{%
           family={Sarhadi},
           familyi={S\bibinitperiod},
           given={Pouria},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=210b7326214ddb0388c9325709587232}{%
           family={Yousefpour},
           familyi={Y\bibinitperiod},
           given={Samereh},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{393183bb6dddfd819715addbf935ec62}
      \strng{fullhash}{393183bb6dddfd819715addbf935ec62}
      \strng{bibnamehash}{393183bb6dddfd819715addbf935ec62}
      \strng{authorbibnamehash}{393183bb6dddfd819715addbf935ec62}
      \strng{authornamehash}{393183bb6dddfd819715addbf935ec62}
      \strng{authorfullhash}{393183bb6dddfd819715addbf935ec62}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Nowadays due to the technology development and use of digital computers in various systems, need for development of high performance and robust software is attracting great attentions. Because of increasing complexity in algorithms and implementation hardware for embedded systems, proper simulation tools are required. In sophisticated systems design, hardware in the loop (HIL) simulation is known as a prominent simulation tool before realistic tests of the system and a step after software simulation. Simultaneously it can be used for verification and validation of automation and control software. HIL has had an historical background in aerospace industries. Recently, this tool has spread in different steps of system life cycle such as design, development, implementation and test of various applications including automobile industry, shipbuilding, power lines, robotic systems and etc. Utilizing a suitable hardware in the loop laboratory, in system design stages is a practical way to increase the system reliability and efficiency as well as value of product. Also, by proper investigation in this modelling and simulation method, many errors can be avoided in design procedure of software and hardware as well as their interconnections. In this study, structure and components of an hardware in the loop laboratory for different systems are explored, also it is tried to more evaluate the applications of HIL simulations in dynamics and control engineering. At last, general structure of an hardware in the loop lab for diverse industries is proposed and discussed.}
      \field{day}{1}
      \field{issn}{2195-2698}
      \field{journaltitle}{International Journal of Dynamics and Control}
      \field{langid}{english}
      \field{month}{12}
      \field{number}{4}
      \field{shortjournal}{Int. J. Dynam. Control}
      \field{shorttitle}{State of the Art}
      \field{title}{State of the Art: Hardware in the Loop Modeling and Simulation with Its Applications in Design, Development and Implementation of System and Control Software}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{volume}{3}
      \field{year}{2015}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{470\bibrangedash 479}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1007/s40435-014-0108-3
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\Z2472FGZ\Sarhadi_Yousefpour_2015_State of the art.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s40435-014-0108-3
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s40435-014-0108-3
      \endverb
      \keyw{Embedded systems,Hardware in the loop (HIL),Modelling & Simulation,System design}
    \endentry
    \entry{schlammingerCoolWayMeasure2014}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=9bcac85a605b2ecde014cd9ef122fec2}{%
           family={Schlamminger},
           familyi={S\bibinitperiod},
           given={Stephan},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{9bcac85a605b2ecde014cd9ef122fec2}
      \strng{fullhash}{9bcac85a605b2ecde014cd9ef122fec2}
      \strng{bibnamehash}{9bcac85a605b2ecde014cd9ef122fec2}
      \strng{authorbibnamehash}{9bcac85a605b2ecde014cd9ef122fec2}
      \strng{authornamehash}{9bcac85a605b2ecde014cd9ef122fec2}
      \strng{authorfullhash}{9bcac85a605b2ecde014cd9ef122fec2}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Published results of the gravitational constant, a measure of the strength of gravity, have failed to converge. An approach that uses cold atoms provides a new data point in the quest to determine this fundamental constant. See Letter p.518}
      \field{issn}{1476-4687}
      \field{issue}{7506}
      \field{journaltitle}{Nature}
      \field{langid}{english}
      \field{month}{6}
      \field{number}{7506}
      \field{title}{A Cool Way to Measure Big {{G}}}
      \field{urlday}{21}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{510}
      \field{year}{2014}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{478\bibrangedash 480}
      \range{pages}{3}
      \verb{doi}
      \verb 10.1038/nature13507
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/nature13507
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/nature13507
      \endverb
      \keyw{gelesen,Physics,Quantum physics,ungelesen,zitiert}
    \endentry
    \entry{singhChatGPTGoogle2023}{inproceedings}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=120c60aa3c6dddd565bf8fe43f383717}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Shashi\bibnamedelima Kant},
           giveni={S\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=2,uniquepart=given,hash=07ae5ff55b24b487840d06455cfbbf3d}{%
           family={Kumar},
           familyi={K\bibinitperiod},
           given={Shubham},
           giveni={S\bibinitperiod},
           givenun=2}}%
        {{un=0,uniquepart=base,hash=164deb724d1c67b85eb68e8d95f971b3}{%
           family={Mehra},
           familyi={M\bibinitperiod},
           given={Pawan\bibnamedelima Singh},
           giveni={P\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{2df9f87a0fa5b684a50db3fa3ee494aa}
      \strng{fullhash}{2df9f87a0fa5b684a50db3fa3ee494aa}
      \strng{bibnamehash}{2df9f87a0fa5b684a50db3fa3ee494aa}
      \strng{authorbibnamehash}{2df9f87a0fa5b684a50db3fa3ee494aa}
      \strng{authornamehash}{2df9f87a0fa5b684a50db3fa3ee494aa}
      \strng{authorfullhash}{2df9f87a0fa5b684a50db3fa3ee494aa}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In today's world, Artificial Intelligence is one of the deepest and newest things to learn and research. Research on Artificial Intelligence is based on some goals and the use of some particular tools. One of the most important and latest innovations in the field of Artificial Intelligence is Chat GPT. It has created a storm in the cyber world after the launch of its prototype. In this paper, we have done a survey analysis on Chat GPT. The emphasis is on Chatbots, Chat GPT and Google Bard AI. The objective of this paper is to let the readers know about the analysis of various reviews and research work done on the topics of Chatbots, Chat GPT and Google Bard AI along with a brief comparison between them. Through this paper, we have provided a pool of knowledge about Chatbots, Chat GPT or Google Bard AI which can pave way for researchers.}
      \field{booktitle}{2023 {{International Conference}} on {{IoT}}, {{Communication}} and {{Automation Technology}} ({{ICICAT}})}
      \field{eventtitle}{2023 {{International Conference}} on {{IoT}}, {{Communication}} and {{Automation Technology}} ({{ICICAT}})}
      \field{month}{6}
      \field{shorttitle}{Chat {{GPT}} \& {{Google Bard AI}}}
      \field{title}{Chat {{GPT}} \& {{Google Bard AI}}: {{A Review}}}
      \field{urlday}{23}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 6}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1109/ICICAT57735.2023.10263706
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\ZPZITWGX\\Singh et al_2023_Chat GPT & Google Bard AI.pdf;C\:\\Users\\simon\\Zotero\\storage\\D6YHP5BY\\10263706.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/10263706?casa_token=JMHwBzQgxnwAAAAA:70OnfYs5ECetZhuq8D_F3QXyua1Xu65rL0a_Ywve3mch00UAeSsOyVjhWCUvDuBpMX83NAbpUpM
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/10263706?casa_token=JMHwBzQgxnwAAAAA:70OnfYs5ECetZhuq8D_F3QXyua1Xu65rL0a_Ywve3mch00UAeSsOyVjhWCUvDuBpMX83NAbpUpM
      \endverb
      \keyw{Artificial Intelligence,Automation,Chat GPT,Chatbots,Google Bard AI,Internet,Prototypes,Storms,Surveys,Technological innovation,ungelesen,zitiert}
    \endentry
    \entry{sipolaArtificialIntelligenceIoT2022}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=279f55690419682f63192893abd988f0}{%
           family={Sipola},
           familyi={S\bibinitperiod},
           given={Tuomo},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fff0ba1b7ee906ba6ba2b1d50948271f}{%
           family={Alatalo},
           familyi={A\bibinitperiod},
           given={Janne},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9a39fa4e4c2be5181ae7ef548737c328}{%
           family={Kokkonen},
           familyi={K\bibinitperiod},
           given={Tero},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bcd015ef41392975a238c548ab3c7b9c}{%
           family={Rantonen},
           familyi={R\bibinitperiod},
           given={Mika},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{c7b0739618fc4300edc44cd9a00f8dcc}
      \strng{fullhash}{9a391c35d7eb114128c1f63c83cf51cf}
      \strng{bibnamehash}{9a391c35d7eb114128c1f63c83cf51cf}
      \strng{authorbibnamehash}{9a391c35d7eb114128c1f63c83cf51cf}
      \strng{authornamehash}{c7b0739618fc4300edc44cd9a00f8dcc}
      \strng{authorfullhash}{9a391c35d7eb114128c1f63c83cf51cf}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The modern trend of moving artificial intelligence computation near to the origin of data sources has increased the demand for new hardware and software suitable for such environments. We carried out a scoping study to find the current resources used when developing Edge AI applications. Due to the nature of the topic, the research combined scientific sources with product information and software project sources. The paper is structured as follows. In the first part, Edge AI applications are briefly discussed followed by hardware options and finally, the software used to develop AI models is described. There are various hardware products available, and we found as many as possible for this research to identify the best-known manufacturers. We describe the devices in the following categories: artificial intelligence accelerators and processors, field-programmable gate arrays, system-on-a-chip devices, system-on-modules, and full computers from development boards to servers. There seem to be three trends in Edge AI software development: neural network optimization, mobile device software and microcontroller software. We discussed these emerging fields and how the special challenges of low power consumption and machine learning computation are being taken into account. Our findings suggest that the Edge AI ecosystem is currently developing, and it has its own challenges to which vendors and developers are responding.}
      \field{booktitle}{2022 31st {{Conference}} of {{Open Innovations Association}} ({{FRUCT}})}
      \field{eventtitle}{2022 31st {{Conference}} of {{Open Innovations Association}} ({{FRUCT}})}
      \field{month}{4}
      \field{shorttitle}{Artificial {{Intelligence}} in the {{IoT Era}}}
      \field{title}{Artificial {{Intelligence}} in the {{IoT Era}}: {{A Review}} of {{Edge AI Hardware}} and {{Software}}}
      \field{urlday}{18}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{320\bibrangedash 331}
      \range{pages}{12}
      \verb{doi}
      \verb 10.23919/FRUCT54823.2022.9770931
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\64U7JGN4\\Sipola et al_2022_Artificial Intelligence in the IoT Era.pdf;C\:\\Users\\simon\\Zotero\\storage\\CGQ6GB2N\\9770931.html
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/9770931
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/9770931
      \endverb
      \keyw{Biological system modeling,Data models,Hardware,Microcontrollers,Program processors,Software,System-on-chip}
    \endentry
    \entry{SklearnDatasetsLoad_digits}{online}{}
      \list{organization}{1}{%
        {scikit-learn}%
      }
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{url}
      \field{labeltitlesource}{title}
      \field{abstract}{Examples using sklearn.datasets.load\_digits: Recognizing hand-written digits Recognizing hand-written digits A demo of K-Means clustering on the handwritten digits data A demo of K-Means clustering...}
      \field{langid}{english}
      \field{title}{Sklearn.Datasets.Load\_digits}
      \field{urlday}{10}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{urldateera}{ce}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\YB9N6MCW\sklearn.datasets.load_digits.html
      \endverb
      \verb{urlraw}
      \verb https://scikit-learn/stable/modules/generated/sklearn.datasets.load_digits.html
      \endverb
      \verb{url}
      \verb https://scikit-learn/stable/modules/generated/sklearn.datasets.load_digits.html
      \endverb
    \endentry
    \entry{spechtProbabilisticNeuralNetworks1990}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=13cae5ba7b4b286ebc2b1c7d803f9c0d}{%
           family={Specht},
           familyi={S\bibinitperiod},
           given={Donald\bibnamedelima F.},
           giveni={D\bibinitperiod\bibinitdelim F\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{13cae5ba7b4b286ebc2b1c7d803f9c0d}
      \strng{fullhash}{13cae5ba7b4b286ebc2b1c7d803f9c0d}
      \strng{bibnamehash}{13cae5ba7b4b286ebc2b1c7d803f9c0d}
      \strng{authorbibnamehash}{13cae5ba7b4b286ebc2b1c7d803f9c0d}
      \strng{authornamehash}{13cae5ba7b4b286ebc2b1c7d803f9c0d}
      \strng{authorfullhash}{13cae5ba7b4b286ebc2b1c7d803f9c0d}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{By replacing the sigmoid activation function often used in neural networks with an exponential function, a probabilistic neural network (PNN) that can compute nonlinear decision boundaries which approach the Bayes optimal is formed. Alternate activation functions having similar properties are also discussed. A fourlayer neural network of the type proposed can map any input pattern to any number of classifications. The decision boundaries can be modified in real-time using new data as they become available, and can be implemented using artificial hardware “neurons” that operate entirely in parallel. Provision is also made for estimating the probability and reliability of a classification as well as making the decision. The technique offers a tremendous speed advantage for problems in which the incremental adaptation time of back propagation is a significant fraction of the total computation time. For one application, the PNN paradigm was 200,000 times faster than back-propagation.}
      \field{day}{1}
      \field{issn}{0893-6080}
      \field{journaltitle}{Neural Networks}
      \field{month}{1}
      \field{number}{1}
      \field{shortjournal}{Neural Networks}
      \field{title}{Probabilistic Neural Networks}
      \field{urlday}{7}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{3}
      \field{year}{1990}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{109\bibrangedash 118}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1016/0893-6080(90)90049-Q
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\E2F3DQWV\089360809090049Q.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/089360809090049Q
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/089360809090049Q
      \endverb
      \keyw{“Neuron”,Associative memory,Bayes strategy,Neural network,Parallel processor,Parzen window,Pattern recognition,Probability density function}
    \endentry
    \entry{sungPerspectiveReviewMemristive2018}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=e24ae51b7f89a28e411d06f6fda435cb}{%
           family={Sung},
           familyi={S\bibinitperiod},
           given={Changhyuck},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e7a9a08ac6b99533bff0329ff977e100}{%
           family={Hwang},
           familyi={H\bibinitperiod},
           given={Hyunsang},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0ac28f149209bea06d6da1095df7ed07}{%
           family={Yoo},
           familyi={Y\bibinitperiod},
           given={In\bibnamedelima Kyeong},
           giveni={I\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{29ac61d59dca7e86ff43880700d9f415}
      \strng{fullhash}{29ac61d59dca7e86ff43880700d9f415}
      \strng{bibnamehash}{29ac61d59dca7e86ff43880700d9f415}
      \strng{authorbibnamehash}{29ac61d59dca7e86ff43880700d9f415}
      \strng{authornamehash}{29ac61d59dca7e86ff43880700d9f415}
      \strng{authorfullhash}{29ac61d59dca7e86ff43880700d9f415}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Neuromorphic computation is one of the axes of parallel distributed processing, and memristor-based synaptic weight is considered as a key component of this type of computation. However, the material properties of memristors, including material related physics, are not yet matured. In parallel with memristors, CMOS based Graphics Processing Unit, Field Programmable Gate Array, and Application Specific Integrated Circuit are also being developed as dedicated artificial intelligence (AI) chips for fast computation. Therefore, it is necessary to analyze the competitiveness of the memristor-based neuromorphic device in order to position the memristor in the appropriate position of the future AI ecosystem. In this article, the status of memristor-based neuromorphic computation was analyzed on the basis of papers and patents to identify the competitiveness of the memristor properties by reviewing industrial trends and academic pursuits. In addition, material issues and challenges are discussed for implementing the memristor-based neural processor.}
      \field{day}{5}
      \field{issn}{0021-8979}
      \field{journaltitle}{Journal of Applied Physics}
      \field{month}{10}
      \field{number}{15}
      \field{shortjournal}{Journal of Applied Physics}
      \field{shorttitle}{Perspective}
      \field{title}{Perspective: {{A}} Review on Memristive Hardware for Neuromorphic Computation}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{124}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{151903}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1063/1.5037835
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\78YATYXK\Sung et al. - 2018 - Perspective A review on memristive hardware for n.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1063/1.5037835
      \endverb
      \verb{url}
      \verb https://doi.org/10.1063/1.5037835
      \endverb
      \keyw{ungelesen}
    \endentry
    \entry{tanahashiApplicationIsingMachines2019}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=f55aaa984ca73c699fc58e1a55fb8591}{%
           family={Tanahashi},
           familyi={T\bibinitperiod},
           given={Kotaro},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f1b2619f55844eed3d59c59443340f32}{%
           family={Takayanagi},
           familyi={T\bibinitperiod},
           given={Shinichi},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0137383d3c81fcf7533c8234bd77aa85}{%
           family={Motohashi},
           familyi={M\bibinitperiod},
           given={Tomomitsu},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c3c66a6825a48b698bdcb127e385d4e7}{%
           family={Tanaka},
           familyi={T\bibinitperiod},
           given={Shu},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {The Physical Society of Japan}%
      }
      \strng{namehash}{d236c14e244d7a5f9b16cf4c59f74561}
      \strng{fullhash}{65e37f82ef4ef53a38db8f1093b737ac}
      \strng{bibnamehash}{65e37f82ef4ef53a38db8f1093b737ac}
      \strng{authorbibnamehash}{65e37f82ef4ef53a38db8f1093b737ac}
      \strng{authornamehash}{d236c14e244d7a5f9b16cf4c59f74561}
      \strng{authorfullhash}{65e37f82ef4ef53a38db8f1093b737ac}
      \field{sortinit}{T}
      \field{sortinithash}{9af77f0292593c26bde9a56e688eaee9}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{An online advertisement optimization, which can be represented by a combinatorial optimization problem is performed using D-Wave 2000Q, a quantum annealing machine. To optimize the online advertisement allocation optimization, we introduce a generalized version of the Markowitz mean-variance model which is a basic model of portfolio optimization. The obtained optimization performance using D-Wave 2000Q is higher than that using the greedy method which is a conventional method. Additionally, to conveniently use Ising machines including a quantum annealing machine, new software called PyQUBO is developed. The first half of the paper gives a review of several combinatorial optimization problems and how to represent them using the Ising model or the quadratic unconstrained binary optimization (QUBO) form. We show the results of the online advertisement allocation optimization and the explanation of PyQUBO in the last half of the paper.}
      \field{day}{15}
      \field{issn}{0031-9015}
      \field{journaltitle}{Journal of the Physical Society of Japan}
      \field{month}{6}
      \field{number}{6}
      \field{shortjournal}{J. Phys. Soc. Jpn.}
      \field{title}{Application of {{Ising Machines}} and a {{Software Development}} for {{Ising Machines}}}
      \field{urlday}{21}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{88}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{061010}
      \range{pages}{1}
      \verb{doi}
      \verb 10.7566/JPSJ.88.061010
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\7QUY6K2C\Tanahashi et al_2019_Application of Ising Machines and a Software Development for Ising Machines.pdf
      \endverb
      \verb{urlraw}
      \verb https://journals.jps.jp/doi/full/10.7566/JPSJ.88.061010
      \endverb
      \verb{url}
      \verb https://journals.jps.jp/doi/full/10.7566/JPSJ.88.061010
      \endverb
    \endentry
    \entry{upadhyaOverviewRestrictedBoltzmann2019}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=9c4f8df4e6e9cbf899f3e6e5e3accf9b}{%
           family={Upadhya},
           familyi={U\bibinitperiod},
           given={Vidyadhar},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fb590781732a695f7f12cfdf4b771fb3}{%
           family={Sastry},
           familyi={S\bibinitperiod},
           given={P.},
           giveni={P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{610e165039c618a39763c7575e5c14ab}
      \strng{fullhash}{610e165039c618a39763c7575e5c14ab}
      \strng{bibnamehash}{610e165039c618a39763c7575e5c14ab}
      \strng{authorbibnamehash}{610e165039c618a39763c7575e5c14ab}
      \strng{authornamehash}{610e165039c618a39763c7575e5c14ab}
      \strng{authorfullhash}{610e165039c618a39763c7575e5c14ab}
      \field{sortinit}{U}
      \field{sortinithash}{6901a00e45705986ee5e7ca9fd39adca}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The restricted Boltzmann machine (RBM) is a two-layered network of stochastic units with undirected connections between pairs of units in the two layers. The two layers of nodes are called visible and hidden nodes. In an RBM, there are no connections from visible to visible or hidden to hidden nodes. RBMs are used mainly as a generative model. They can be suitably modified to perform classification tasks also. They are among the basic building blocks of other deep learning models such as deep Boltzmann machine and deep belief networks. The aim of this article is to give a tutorial introduction to the restricted Boltzmann machines and to review the evolution of this model.}
      \field{day}{18}
      \field{journaltitle}{Journal of the Indian Institute of Science}
      \field{month}{2}
      \field{shortjournal}{Journal of the Indian Institute of Science}
      \field{title}{An {{Overview}} of {{Restricted Boltzmann Machines}}}
      \field{volume}{99}
      \field{year}{2019}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.1007/s41745-019-0102-z
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\JAGD4W2N\An Overview of Restricted Boltzmann Machines.pdf
      \endverb
      \keyw{ungelesen}
    \endentry
    \entry{uusitaloOverviewMethodsEvaluate2015}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=cb046984f988425a540415e9af940684}{%
           family={Uusitalo},
           familyi={U\bibinitperiod},
           given={Laura},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a51f0f8e0c7763f8a387a693b36014d1}{%
           family={Lehikoinen},
           familyi={L\bibinitperiod},
           given={Annukka},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0761208a4b3d4e35a9b1092f82a42a8a}{%
           family={Helle},
           familyi={H\bibinitperiod},
           given={Inari},
           giveni={I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4a3b472f54506c4971f2d90da6948074}{%
           family={Myrberg},
           familyi={M\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{4bd83519d625926120d7264cb13e7480}
      \strng{fullhash}{da6d317fda0a13b738620188c4d0e86e}
      \strng{bibnamehash}{da6d317fda0a13b738620188c4d0e86e}
      \strng{authorbibnamehash}{da6d317fda0a13b738620188c4d0e86e}
      \strng{authornamehash}{4bd83519d625926120d7264cb13e7480}
      \strng{authorfullhash}{da6d317fda0a13b738620188c4d0e86e}
      \field{sortinit}{U}
      \field{sortinithash}{6901a00e45705986ee5e7ca9fd39adca}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{There is an increasing need for environmental management advice that is wide-scoped, covering various interlinked policies, and realistic about the uncertainties related to the possible management actions. To achieve this, efficient decision support integrates the results of pre-existing models. Many environmental models are deterministic, but the uncertainty of their outcomes needs to be estimated when they are utilized for decision support. We review various methods that have been or could be applied to evaluate the uncertainty related to deterministic models' outputs. We cover expert judgement, model emulation, sensitivity analysis, temporal and spatial variability in the model outputs, the use of multiple models, and statistical approaches, and evaluate when these methods are appropriate and what must be taken into account when utilizing them. The best way to evaluate the uncertainty depends on the definitions of the source models and the amount and quality of information available to the modeller.}
      \field{day}{1}
      \field{issn}{1364-8152}
      \field{journaltitle}{Environmental Modelling \& Software}
      \field{month}{1}
      \field{shortjournal}{Environmental Modelling \& Software}
      \field{title}{An Overview of Methods to Evaluate Uncertainty of Deterministic Models in Decision Support}
      \field{urlday}{7}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{63}
      \field{year}{2015}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{24\bibrangedash 31}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1016/j.envsoft.2014.09.017
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\43USNQTT\\Uusitalo et al_2015_An overview of methods to evaluate uncertainty of deterministic models in.pdf;C\:\\Users\\simon\\Zotero\\storage\\Y4SYH9EN\\S1364815214002813.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S1364815214002813
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S1364815214002813
      \endverb
      \keyw{Decision support,Deterministic,Environmental modelling,Probabilistic,Uncertainty}
    \endentry
    \entry{verdonQuantumHamiltonianBasedModels2019}{online}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=8943c3f5d2e184d9ca754c0e84ec9196}{%
           family={Verdon},
           familyi={V\bibinitperiod},
           given={Guillaume},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=34c221397354314430edf7e70a8f7668}{%
           family={Marks},
           familyi={M\bibinitperiod},
           given={Jacob},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c4e8e964abed56e304602f6568817406}{%
           family={Nanda},
           familyi={N\bibinitperiod},
           given={Sasha},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=188e0a3433fcb8ba6f7bea5a6b6b82f1}{%
           family={Leichenauer},
           familyi={L\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=68cf2b43d6be63b1ffea346ffe45c1c2}{%
           family={Hidary},
           familyi={H\bibinitperiod},
           given={Jack},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{d384c2df485f36feacf19c5b580344ee}
      \strng{fullhash}{c6713c4fa5382f84b497df33dfa54318}
      \strng{bibnamehash}{c6713c4fa5382f84b497df33dfa54318}
      \strng{authorbibnamehash}{c6713c4fa5382f84b497df33dfa54318}
      \strng{authornamehash}{d384c2df485f36feacf19c5b580344ee}
      \strng{authorfullhash}{c6713c4fa5382f84b497df33dfa54318}
      \field{sortinit}{V}
      \field{sortinithash}{afb52128e5b4dc4b843768c0113d673b}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce a new class of generative quantum-neural-network-based models called Quantum Hamiltonian-Based Models (QHBMs). In doing so, we establish a paradigmatic approach for quantum-probabilistic hybrid variational learning, where we efficiently decompose the tasks of learning classical and quantum correlations in a way which maximizes the utility of both classical and quantum processors. In addition, we introduce the Variational Quantum Thermalizer (VQT) for generating the thermal state of a given Hamiltonian and target temperature, a task for which QHBMs are naturally well-suited. The VQT can be seen as a generalization of the Variational Quantum Eigensolver (VQE) to thermal states: we show that the VQT converges to the VQE in the zero temperature limit. We provide numerical results demonstrating the efficacy of these techniques in illustrative examples. We use QHBMs and the VQT on Heisenberg spin systems, we apply QHBMs to learn entanglement Hamiltonians and compression codes in simulated free Bosonic systems, and finally we use the VQT to prepare thermal Fermionic Gaussian states for quantum simulation.}
      \field{day}{4}
      \field{eprintclass}{quant-ph}
      \field{eprinttype}{arxiv}
      \field{month}{10}
      \field{pubstate}{preprint}
      \field{title}{Quantum {{Hamiltonian-Based Models}} and the {{Variational Quantum Thermalizer Algorithm}}}
      \field{urlday}{19}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 1910.02071
      \endverb
      \verb{file}
      \verb C\:\\Users\\simon\\Zotero\\storage\\3HTPGVXD\\Verdon et al_2019_Quantum Hamiltonian-Based Models and the Variational Quantum Thermalizer.pdf;C\:\\Users\\simon\\Zotero\\storage\\5WMRJVH6\\1910.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1910.02071
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1910.02071
      \endverb
      \keyw{Computer Science - Machine Learning,Quantum Physics,ungelesen,zitiert}
    \endentry
    \entry{wangOscillatorbasedIsingMachine2017}{online}{}
      \name{author}{2}{}{%
        {{un=1,uniquepart=given,hash=a957ec920fdcbeb994bb31f90ea3b62d}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Tianshi},
           giveni={T\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=f96a35329a8ed37e2473f0d5685c11a6}{%
           family={Roychowdhury},
           familyi={R\bibinitperiod},
           given={Jaijeet},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{32eaab9e052807a9baafd766b81cb7e6}
      \strng{fullhash}{32eaab9e052807a9baafd766b81cb7e6}
      \strng{bibnamehash}{32eaab9e052807a9baafd766b81cb7e6}
      \strng{authorbibnamehash}{32eaab9e052807a9baafd766b81cb7e6}
      \strng{authornamehash}{32eaab9e052807a9baafd766b81cb7e6}
      \strng{authorfullhash}{32eaab9e052807a9baafd766b81cb7e6}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many combinatorial optimization problems can be mapped to finding the ground states of the corresponding Ising Hamiltonians. The physical systems that can solve optimization problems in this way, namely Ising machines, have been attracting more and more attention recently. Our work shows that Ising machines can be realized using almost any nonlinear self-sustaining oscillators with logic values encoded in their phases. Many types of such oscillators are readily available for large-scale integration, with potentials in high-speed and low-power operation. In this paper, we describe the operation and mechanism of oscillator-based Ising machines. The feasibility of our scheme is demonstrated through several examples in simulation and hardware, among which a simulation study reports average solutions exceeding those from state-of-art Ising machines on a benchmark combinatorial optimization problem of size 2000.}
      \field{day}{12}
      \field{eprintclass}{physics}
      \field{eprinttype}{arxiv}
      \field{month}{10}
      \field{pubstate}{preprint}
      \field{title}{Oscillator-Based {{Ising Machine}}}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1709.08102
      \endverb
      \verb{eprint}
      \verb 1709.08102
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\Y6S4QDT9\Oscillator-based Ising Machine.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1709.08102
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1709.08102
      \endverb
      \keyw{Computer Science - Emerging Technologies,Physics - Computational Physics,ungelesen}
    \endentry
    \entry{wangResearchApplicationGradient2021}{inproceedings}{}
      \name{author}{3}{}{%
        {{un=1,uniquepart=given,hash=03d3e23852937fa39b0e34246c4ecd1d}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xin},
           giveni={X\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=a725e4498b81130660bd0b8edf33efe0}{%
           family={Yan},
           familyi={Y\bibinitperiod},
           given={Liting},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=1,uniquepart=given,hash=ff6de92665ee0c6e5d25194297f6a576}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Qizhi},
           giveni={Q\bibinitperiod},
           givenun=1}}%
      }
      \strng{namehash}{6428b840a3c8ebb84536f6ada61d8294}
      \strng{fullhash}{6428b840a3c8ebb84536f6ada61d8294}
      \strng{bibnamehash}{6428b840a3c8ebb84536f6ada61d8294}
      \strng{authorbibnamehash}{6428b840a3c8ebb84536f6ada61d8294}
      \strng{authornamehash}{6428b840a3c8ebb84536f6ada61d8294}
      \strng{authorfullhash}{6428b840a3c8ebb84536f6ada61d8294}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The gradient descent algorithm is a type of optimization algorithm that is widely used to solve machine learning algorithm model parameters. Through continuous iteration, it obtains the gradient of the objective function, gradually approaches the optimal solution of the objective function, and finally obtains the minimum loss function and related parameters. The gradient descent algorithm is frequently used in the solution process of logical regression, which is a common binary classification approach. This paper compares and analyzes the differences between batch gradient descent and its derivative algorithms — stochastic gradient descent algorithm and mini- batch gradient descent algorithm in terms of iteration number, loss function through experiments, and provides some suggestions on how to pick the best algorithm for the logistic regression binary task in machine learning.}
      \field{booktitle}{2021 {{International Conference}} on {{Computer Network}}, {{Electronic}} and {{Automation}} ({{ICCNEA}})}
      \field{eventtitle}{2021 {{International Conference}} on {{Computer Network}}, {{Electronic}} and {{Automation}} ({{ICCNEA}})}
      \field{month}{9}
      \field{title}{Research on the {{Application}} of {{Gradient Descent Algorithm}} in {{Machine Learning}}}
      \field{urlday}{8}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{11\bibrangedash 15}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1109/ICCNEA53019.2021.00014
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\EYETDMNB\Wang et al_2021_Research on the Application of Gradient Descent Algorithm in Machine Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/9603742
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/9603742
      \endverb
      \keyw{Classification algorithms,Gradient Descent,Linear programming,Logistic Regression,Machine learning,Machine Learning,Machine learning algorithms,Stochastic processes,Task analysis,Training data}
    \endentry
    \entry{wittpahlKuenstlicheIntelligenzTechnologie2019}{book}{}
      \name{editor}{1}{}{%
        {{un=0,uniquepart=base,hash=74ae3b93eb51c08b023e58a6dd91e895}{%
           family={Wittpahl},
           familyi={W\bibinitperiod},
           given={Volker},
           giveni={V\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{74ae3b93eb51c08b023e58a6dd91e895}
      \strng{fullhash}{74ae3b93eb51c08b023e58a6dd91e895}
      \strng{bibnamehash}{74ae3b93eb51c08b023e58a6dd91e895}
      \strng{editorbibnamehash}{74ae3b93eb51c08b023e58a6dd91e895}
      \strng{editornamehash}{74ae3b93eb51c08b023e58a6dd91e895}
      \strng{editorfullhash}{74ae3b93eb51c08b023e58a6dd91e895}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{editor}
      \field{labeltitlesource}{shorttitle}
      \field{isbn}{978-3-662-58041-7 978-3-662-58042-4}
      \field{langid}{ngerman}
      \field{shorttitle}{Künstliche Intelligenz}
      \field{title}{Künstliche Intelligenz: Technologie | Anwendung | Gesellschaft}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1007/978-3-662-58042-4
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\IVHYC2WF\Künstliche Intelligenz.pdf
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1007/978-3-662-58042-4
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/978-3-662-58042-4
      \endverb
      \keyw{Arbeitswelt 4.0,Breitbandausbau,Datenaufbereitung,Digitale Geschäftsmodelle,Digitales Lernen,Echtzeitvernetzung,Gesellschaftlicher Wandel,Industrie 4.0,Open Access,ungelesen,Wertschöpfung und Arbeitsmarkt,zitiert}
    \endentry
    \entry{yaoMassivelyParallelAssociative2013}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=4171f136d7aea3117be2aeaf43fe24af}{%
           family={Yao},
           familyi={Y\bibinitperiod},
           given={Zhe},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=30a2de0bd13df4c7d5c65eda37e8eecc}{%
           family={Gripon},
           familyi={G\bibinitperiod},
           given={Vincent},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=16922b89cdf492fe3becb64ab59c075a}{%
           family={Rabbat},
           familyi={R\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{bfa40eb291abaf5a4bba95fc4ffffb58}
      \strng{fullhash}{bfa40eb291abaf5a4bba95fc4ffffb58}
      \strng{bibnamehash}{bfa40eb291abaf5a4bba95fc4ffffb58}
      \strng{authorbibnamehash}{bfa40eb291abaf5a4bba95fc4ffffb58}
      \strng{authornamehash}{bfa40eb291abaf5a4bba95fc4ffffb58}
      \strng{authorfullhash}{bfa40eb291abaf5a4bba95fc4ffffb58}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Associative memories store content in such a way that the content can be later retrieved by presenting the memory with a small portion of the content, rather than presenting the memory with an address as in more traditional memories. Associative memories are used as building blocks for algorithms within database engines, anomaly detection systems, compression algorithms, and face recognition systems. A classical example of an associative memory is the Hopfield neural network. Recently, Gripon and Berrou have introduced an alternative construction which builds on ideas from the theory of error correcting codes and which greatly outperforms the Hopfield network in capacity, diversity, and efficiency. In this paper we implement a variation of the Gripon-Berrou associative memory on a general purpose graphical processing unit (GPU). The work of Gripon and Berrou proposes two retrieval rules, sum-of-sum and sum-of-max. The sum-of-sum rule uses only matrix-vector multiplication and is easily implemented on the GPU. The sum-of-max rule is much less straightforward to implement because it involves non-linear operations. However, the sum-of-max rule gives significantly better retrieval error rates. We propose a hybrid rule tailored for implementation on a GPU which achieves a 880-fold speedup without sacrificing any accuracy.}
      \field{day}{27}
      \field{month}{3}
      \field{title}{A {{Massively Parallel Associative Memory Based}} on {{Sparse Neural Networks}}}
      \field{year}{2013}
      \field{dateera}{ce}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\F5FHS8MI\Yao et al_2013_A Massively Parallel Associative Memory Based on Sparse Neural Networks.pdf
      \endverb
    \endentry
    \entry{zhaiDeepStructuredEnergy2016}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=992b754eafb36a8a18191bb3683ae7b8}{%
           family={Zhai},
           familyi={Z\bibinitperiod},
           given={Shuangfei},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f90fa099ce585c3d1d009abdb2ccf07c}{%
           family={Cheng},
           familyi={C\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=206efc21585ddcf16bb37a6cf4096e80}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Weining},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3a50857381a1ac69e8d795eecd1ad14f}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Zhongfei},
           giveni={Z\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{f408d576f91e0606212563ba81855453}
      \strng{fullhash}{a9279c8803776cfeb2c6a45fecc0b123}
      \strng{bibnamehash}{a9279c8803776cfeb2c6a45fecc0b123}
      \strng{authorbibnamehash}{a9279c8803776cfeb2c6a45fecc0b123}
      \strng{authornamehash}{f408d576f91e0606212563ba81855453}
      \strng{authorfullhash}{a9279c8803776cfeb2c6a45fecc0b123}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we attack the anomaly detection problem by directly modeling the data distribution with deep architectures. We hence propose deep structured energy based models (DSEBMs), where the energy function is the output of a deterministic deep neural network with structure. We develop novel model architectures to integrate EBMs with different types of data such as static data, sequential data, and spatial data, and apply appropriate model architectures to adapt to the data structure. Our training algorithm is built upon the recent development of score matching (Hyvarinen, 2005), which connects an EBM with a regularized autoencoder, eliminating the need for complicated sampling method. Statistically sound decision criterion can be derived for anomaly detection purpose from the perspective of the energy landscape of the data distribution. We investigate two decision criteria for performing anomaly detection: the energy score and the reconstruction error. Extensive empirical studies on benchmark anomaly detection tasks demonstrate that our proposed model consistently matches or outperforms all the competing methods.}
      \field{booktitle}{Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}}
      \field{day}{11}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{1938-7228}
      \field{langid}{english}
      \field{month}{6}
      \field{title}{Deep {{Structured Energy Based Models}} for {{Anomaly Detection}}}
      \field{urlday}{19}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1100\bibrangedash 1109}
      \range{pages}{10}
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\Y4E93TJV\Deep Structured Energy Based Models for Anomaly Detection.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v48/zhai16.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v48/zhai16.html
      \endverb
      \keyw{ungelesen,zitiert}
    \endentry
    \entry{zhangOverviewRestrictedBoltzmann2018}{article}{}
      \name{author}{4}{}{%
        {{un=1,uniquepart=given,hash=50541ce48b62a59478859829bdda8bad}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Nan},
           giveni={N\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=f793174e721d98bd38d1c1dee97f19bc}{%
           family={Ding},
           familyi={D\bibinitperiod},
           given={Shifei},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=112faefb37293f52cd92c3bb36fcc08b}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=92a401ac7ae29a6a6b975cd10e7eb823}{%
           family={Xue},
           familyi={X\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{7e8dde03e0d36ad2fee1aceacf851802}
      \strng{fullhash}{eb6023469884676b1f94a05edb610adb}
      \strng{bibnamehash}{eb6023469884676b1f94a05edb610adb}
      \strng{authorbibnamehash}{eb6023469884676b1f94a05edb610adb}
      \strng{authornamehash}{7e8dde03e0d36ad2fee1aceacf851802}
      \strng{authorfullhash}{eb6023469884676b1f94a05edb610adb}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The Restricted Boltzmann Machine (RBM) has aroused wide interest in machine learning fields during the past decade. This review aims to report the recent developments in theoretical research and applications of the RBM. We first give an overview of the general RBM from the theoretical perspective, including stochastic approximation methods, stochastic gradient methods, and preventing overfitting methods. And then this review focuses on the RBM variants which further improve the learning ability of the RBM under general or specific applications. The RBM has recently been extended for representational learning, document modeling, multi-label learning, weakly supervised learning and many other tasks. The RBM and RBM variants provide powerful tools for representing dependency in the data, and they can be used as the basic building blocks to create deep networks. Apart from the Deep Belief Network (DBN) and the Deep Boltzmann Machine (DBM), the RBM can also be combined with the Convolutional Neural Network (CNN) to create deep networks. This review provides a comprehensive view of these advances in the RBM together with its future perspectives.}
      \field{day}{31}
      \field{issn}{0925-2312}
      \field{journaltitle}{Neurocomputing}
      \field{month}{1}
      \field{shortjournal}{Neurocomputing}
      \field{title}{An Overview on {{Restricted Boltzmann Machines}}}
      \field{urlday}{15}
      \field{urlmonth}{2}
      \field{urlyear}{2024}
      \field{volume}{275}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1186\bibrangedash 1199}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1016/j.neucom.2017.09.065
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\739QIQAV\Zhang et al. - 2018 - An overview on Restricted Boltzmann Machines.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0925231217315849
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0925231217315849
      \endverb
      \keyw{Classification,Deep networks,Representational learning,Restricted Boltzmann Machine,ungelesen}
    \endentry
    \entry{zhouPhotonicMatrixMultiplication2022}{article}{}
      \name{author}{12}{}{%
        {{un=0,uniquepart=base,hash=78dd995bd34e44e0a0861618eb910c63}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Hailong},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=86afeb6d54c4a9a2c1fb5b4d45b71c22}{%
           family={Dong},
           familyi={D\bibinitperiod},
           given={Jianji},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3d827eaff42484502f96a8e4e25ca3e2}{%
           family={Cheng},
           familyi={C\bibinitperiod},
           given={Junwei},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=178d0cbbfaf1f5275b0599811ac103ce}{%
           family={Dong},
           familyi={D\bibinitperiod},
           given={Wenchan},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=50d06e6a867f52799d4c4f816b9b18b1}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Chaoran},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=62a82074c485f12c29bb533d5445542c}{%
           family={Shen},
           familyi={S\bibinitperiod},
           given={Yichen},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c5087171200b62829058cdd54f719eeb}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Qiming},
           giveni={Q\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b862c2353f1e5867de131b00b87be59f}{%
           family={Gu},
           familyi={G\bibinitperiod},
           given={Min},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b9b5881526bf7f87fc9c50495454eef7}{%
           family={Qian},
           familyi={Q\bibinitperiod},
           given={Chao},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bc1e0fa1d05586407ac8b7783ddae925}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Hongsheng},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=690e4326498fcd2cc3be0e957addb9a2}{%
           family={Ruan},
           familyi={R\bibinitperiod},
           given={Zhichao},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=030b9b812a01694a3404e459f67f0864}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Xinliang},
           giveni={X\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{e207caec99efbb221e562337ed07eb91}
      \strng{fullhash}{89f1608f9ed2195ef48021a514d8fca2}
      \strng{bibnamehash}{89f1608f9ed2195ef48021a514d8fca2}
      \strng{authorbibnamehash}{89f1608f9ed2195ef48021a514d8fca2}
      \strng{authornamehash}{e207caec99efbb221e562337ed07eb91}
      \strng{authorfullhash}{89f1608f9ed2195ef48021a514d8fca2}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Matrix computation, as a fundamental building block of information processing in science and technology, contributes most of the computational overheads in modern signal processing and artificial intelligence algorithms. Photonic accelerators are designed to accelerate specific categories of computing in the optical domain, especially matrix multiplication, to address the growing demand for computing resources and capacity. Photonic matrix multiplication has much potential to expand the domain of telecommunication, and artificial intelligence benefiting from its superior performance. Recent research in photonic matrix multiplication has flourished and may provide opportunities to develop applications that are unachievable at present by conventional electronic processors. In this review, we first introduce the methods of photonic matrix multiplication, mainly including the plane light conversion method, Mach–Zehnder interferometer method and wavelength division multiplexing method. We also summarize the developmental milestones of photonic matrix multiplication and the related applications. Then, we review their detailed advances in applications to optical signal processing and artificial neural networks in recent years. Finally, we comment on the challenges and perspectives of photonic matrix multiplication and photonic acceleration.}
      \field{day}{3}
      \field{issn}{2047-7538}
      \field{journaltitle}{Light: Science \& Applications}
      \field{langid}{english}
      \field{month}{2}
      \field{number}{1}
      \field{shortjournal}{Light Sci Appl}
      \field{title}{Photonic Matrix Multiplication Lights up Photonic Accelerator and Beyond}
      \field{urlday}{18}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{volume}{11}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{30}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/s41377-022-00717-8
      \endverb
      \verb{file}
      \verb C:\Users\simon\Zotero\storage\3KLKSXS8\Zhou et al_2022_Photonic matrix multiplication lights up photonic accelerator and beyond.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s41377-022-00717-8
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s41377-022-00717-8
      \endverb
      \keyw{Integrated optics,Optoelectronic devices and components,Photonic devices}
    \endentry
  \enddatalist
\endrefsection
\endinput

