\chapter{Implementation of the mem-HNN}

\section{Objectives and research methodology}
Upon establishing the precise research methodology, this chapter delves into the implementaiton of the simualtor pipeline.
First, the analysis phase of the \ac{DSR} process is executed with the goal to establish a model of the simulator pipeline,
which the requirements and conditions can be derived from. 
Next, a practical prototype is developed in iterative design cycles to fulfill the target requirements.
In the evaluation phase, the simulator pipeline is used to assess the performance of the \ac{mem-HNN} 
in the training of an examplary machine learning workload. 
This thesis utilizes performance metrics collected from the simulation to address the central research question, which explores potential speed and efficiency enhancements of the \ac{mem-HNN} compared to digital computers.


\section{Analysis phase}
\subsection{General conditions}

Following the first phase of the \ac{DSR}-cycle described in Chapter 3, the research outline is initially established from which the requirements for the simulator pipeline are derived.\footcite[cf.][278-279]{oesterleKonsortialforschung2010}
This analysis begins by describing the general conditions specified in Section 3.1.
Hereby, general conditions are permanent design decisions that are used as foundation for the implementation of the simulator pipleine.
The underlying motivation hereby is to research if the known proof of concepts are feasible on the complete \ac{mem-HNN}
and evaluate if that brings an actual acceleration, which is equivalent to answering the research question of this thesis. 

The implementaton is executed in the programming language Python since it offers a variety of third party libraries that are useful 
for machine learning that are state of the art, like pytorch, scikit learn etc..\footcite[cf.][306-307]{DiscreteContinuousModels}
Furthermore, Scikit Learn is chosen as machine learning library since it is one of the industry standards for classical machine learning, has a broad variety of features in terms of \ac{BM}s
and has a lower learning curve compared with e.g. Tensorflow.\footcite[cf.][5-6]{raschkaMachineLearningPython2020}
For simplicity and to save time, an \ac{RBM} is used as a test case with handwritten digit classification as workload.

The complete \ac{mem-HNN} is being simulated based on a design that has been developed by the Forschungszentrum Jülich and HPE.\footcite[cf.][3-4]{hizzaniMemristorbasedHardwareAlgorithms2023}  
This design describes an \ac{ASIC} design that realizes the noisy Hopfield Network shown in figure\ref{ModellHMM}.
It includes an energy model based on low-level circuit-simulations, which can derive the average energy consumption per clock cycle of the \ac{mem-HNN}.
In addition to that, the model includes latency estimations of the \ac{mem-HNN} to perform a full iteration. 
This simualtion approach is chosen as the device is still under development and hardware devices are not available yet.
Nonetheless, the complete hardware can be realized in software without compromising its functionality.
Such a simulation based performance evaluation is quite common in the \ac{ASIC} design flow.\footnote{cf.\cite{raoUltimateGuideASIC}, p. 1; cf.\cite{ASICDesignFlow}, p. 1}
A in-depth explanation of this model is out of scope for this thesis but core parameters are explained to understand the gathered energy values.
Lastly, the simulation is performed on a notebook.
Due to the limitations of the built in \ac{CPU}\footnote{\texttt{Intel i7-10610U, 1.80GHz, 2304 MHz, 4 Cores, 8 Logical Processors}}, efficient coding is required to ensure simulations are performed within an acceptable timeframe.

\subsection{Requirements}

To evaluate the performance of \ac{mem-HNN}s in training and inference of \ac{BM}s, a full simulator pipeline has to be modeled.
In Fig.\ref{Overall architecture} the envisioned model is shown, in which an \ac{mem-HNN} chip can be used to implement \ac{BM}s.
Here, a digital computer is used to implement and train machine learning models on vairous datasets.
The \ac{mem-HNN} chip is then used to perform the sampling during the \ac{BM} inference or training.
Training and inference on this system then involves the following five steps, which are handled by different components and desribe the iteraction between the digital computer and the analog \ac{mem-HNN} chip:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.80\linewidth]{graphics/Analysemodell.JPG}
    \caption{proposed solution architecture}
    \label{Overall architecture}
\end{figure}
\textbf{1.} initializes the machine larning model (neural network) including setting the weights and biases of the \ac{BM}. 

\textbf{2.} starts with the transfer of the weigths and biases to the \ac{mem-HNN} accelerator via a bus-system. 
The local memory saves the data and forwards them to the controller. 
The controller is able to programm the memristors in the crossbar array. 

\textbf{3.} is the Hopfield Neural Network (HNN), which contains the memristor crossbar array and the state register (SR).
Here, \ac{mem-HNN} performs a pre-defined amount of iterations, where a sample configuration is stored in the on-chip memory after each iteration.\footcite[cf.][18]{caiHarnessingIntrinsicNoise2019}
The state register includes the current neuron configuration and can lock und unlock specific neurons, so that it is possible to update neurons synchronously.\footcite[cf.][18]{caiHarnessingIntrinsicNoise2019}
This enables the possibility of the promissing N/2 update strategy.

\textbf{4.} After the \ac{mem-HNN} has performed all iterations, the stored sample configurations
of the visible \(v_{neg}\) and the hidden \(h_{neg}\) neurons are transferred back to the digital computer via the bus-system.

\textbf{5.} With the sample configurations, the digital computer calcualtes the activation probabilities and performs
the updates to the weights and biases according to the trainign rule in equation\ref{Update_weights}.
These training updates are repeatley performed starting again from step 2 until the model achieves sufficient performance.
Furthermore, the model can be evaluated in its performance in terms of chosen metrics like prediction accuracy or the negative likelihood etc..

In the simulator pipeline the behavior of the \ac{mem-HNN} in fig.\ref{Overall architecture} is mimicked, where the simulator acts as a drop-in replacement until hardware devices become availbale.
The simulator models the behavior of the chip, so that performance predictions are possible long before a hardware device can eventually be used.
Here, it is important to stress that the current hardware model described in the previous section currently does not contain modeling results for the on-chip memory, the controller and the bus system.
The simualtor is therefore solely focused on modeling the sampling step 3.
With more modeling results becoming available, accurate simualtions of step 2 and 4 can be added to the simualtor.
The next step is to derive the requirements and to establish the research outline.
The aim of generating requirements is to generate good quality, requirements that offers an acceptable level of risk to start the project.\footcite[cf.][11]{ebertSystematischesRequirementsEngineering2008}
These requirements need to cover the functions of the \ac{mem-HNN}, which then must be implemented by the respective software components.
Also, requirements may evolve over time and require adjustments when outcomes differ from initial expectations.
As a result, considering the research question and objectives, the following software requirements emerged:

\textbf{Digital Computer}
\begin{itemize}
    \item Initialize a \ac{BM}
    \item Utilization of any training data
    \item Initializing of other ML models that are used in conjucntion with the \ac{BM} for evaluation
    \item Training of the \ac{BM} with either Gibbs Sampling or Metropolis Hasting
    \item Setting individual parameters: sampling steps, training iterations, noise level, learning rate 
    \item Evaluation of the model's performance using prediction accuracy and negative likelihood
\end{itemize}
\textbf{Simulated Mem-HNN Accelerator}
\begin{itemize}
    \item Using any \ac{BM} as input
    \item Implementing the noisy Hopfield Neural Network update mechanism described in \ref{noisy_update_HNN_formula}
    \item Return sampled output of the neuron configurations 
    \item Selectively switching between the N/2 half updating method instead of single spin updates described in 2.4
    \item Simulation of computation speed (throughput) and energy consumption of the \ac{mem-HNN} chip
\end{itemize}
Furthermore, the python program should be split logically into the different modules and components to enable well structured code. 
With set requirements it is now possible to begin the iterative design and evaluation cycle with focusing on some requirements per iteration.

\section{First Design and Evaluation phase}

This \textbf{Design phase} has the goal of implementing the parts of the simulator pipeline that is executed 
on the digital compuer in fig.\ref{Overall architecture} requirements.
Accordingly the resulting prototyp of this iteration has to satisfy all requirements listed under digital computer in section 4.2.
The implementation is verfied with a test case based on classification of handwritten digits.
The first step in the described prototyping methodology within 3.3 is to perform the systemic analysis to categorize the prototype.
In the realm of prototyping, the following categorizations are made: the prototype type (1) is computational, and its fidelity level (2) is high, as it aims to model all functionalities closely to reality.
Furthermore, the complexity (3) is considered moderate because not all hardware components can be modeled in software.
Additionally, the scale (4) remains constant, and there are multiple iterations (5) executed sequentially to train and infer the \ac{RBM}.
This process of categorizing the protoype helps to make prototypes comporable as prototyping can be used very vague.
The second step, is to set up the Scikit Learn machine learning library as explained in 4.2.
Scikit Learn includes built-in models for \ac{BM}s that enable rapid development also includung popular datasets, delivering results that are comparable with those found in literature.
This is useful to answer the research question in a timely manner.
Especially, the test case used for the \ac{BM} is based on an example of the official scikit learn documentation.\footcite[cf.][1]{RestrictedBoltzmannMachine}

The following task is to set parameters like the learning rate, iterations, size of the hidden layer. 
With having a look in the literature and through testing a learning rate of \(0.2\), 10 training epochs with 72 iterations in one epoch, and an hidden layer of 100 neurons is chosen.\footnote{cf.\cite{hintonPracticalGuideTraining2012}, p. 11-12; cf.\cite{bohmNoiseinjectedAnalogIsing2022}, p. 1}
The size of the visible layer is automatically recognized by Scikit Learn, so for example to 64 to correspond with 8x8 images of a dataset.
Also, the dataset implementation can be customized as needed, such as for a breast cancer classification workload.

The training of the \ac{RBM} is performed in the \texttt{.fit} method and for the functionality to select the preferred sampling algorithm an additional sampling method need to be added.
This process includes modifying the \texttt{\_rbm.py} file in the basic scikit learn library.
The predefined sampling method is gibbs sampling and there is no option to access metropolis hasting within the basic library. 
Therefore, the metropolis hasting alhorithm, explained in 2.2.3 needs to be manually implemented.
The according adjustments are included from the code availability of a paper.\footcite[cf.][11-12]{bohmNoiseinjectedAnalogIsing2022}
This decision is made because the algorithm used there is the original metropolis algorithm by Metropolis et.al..\footcite[cf.][1087-1092]{metropolisEquationStateCalculations1953}
Furthermore, the implementation is performant with many numpy functions. 
To utilize this sampling method, some minor adjustments are made for the user friendliness. 
First, one function is fixed that has a small error, which produced an erroneous empty configuration as the first sampling iteration. 
The user friendliness is achieved by introducing a new parameter \texttt{sampling\_method} that dynamically allows to change the sampling method. 
Another change is the approach of evaluating the performance of the neural network after an x amount of iterations to meassure its performance on the test data while training.
A complete code overview of the metropolis hastings sampling algorithm can be found in the \texttt{mcmc2.py} file as part of the digital delivery with all
adjusted methods for the training of the \ac{RBM} in \texttt{\_rbm.py}, while the overall execution takes place in the \texttt{playground.py}.

To evaluate the results and functionalities the \textbf{Evaluation phase} in this iteration validates the functionalities through a training of the \ac{RBM}. with each sampling method and extract their prediction accuracy.
Scikit learn offers a variety of datasets that are already in a polished format, ready to use. 
The decision is to use a classification workload of handrwritten digits as a test case for validating the implementation.
One reason for this is that the ``load digits dataset'' is similar to the well known MNIST dataset representing an industry standard problem, but has a smaller resolution of 8x8 pixels and features around 1800 samples that can be categorized in 10 classes (integers 0-9).\footcite[cf.][1]{SklearnDatasetsLoad_digits}
The second reason is that the workload is already optimized and therefore can deliver relevant data for the research question.
In this case additionally a nudging of the data is chosen to create more samples, by a factor of five, and to bring more complexity in the workload. 
The split in the dataset is selected to be divided into the conventional 80\% training data and 20\% test data.\footnote{cf.\cite{charithaTypeIIDiabetesPrediction2022a}, p. 1-2; cf.\cite{supriAsianStockIndex2023}, p. 1}
For training, an \ac{RBM} is selected as the feature extractor and paired with a logistic regression classifier for prediction, thereby establishing a cohesive pipeline.
Following figure\ref{CD_baselines} shows the training results using the gibbs sampling approach. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{graphics/CD_combined_plot.png}
    \caption{Gibbs sampling baselines}
    \label{CD_baselines}
\end{figure}
The right plot shows that the initial prediction accuracy starts at 75\%, akin to that of just a single linear regression model.
This suggests that the untrained \ac{RBM} at the input of the classifier initially does not effect the model's classification performance.
Data points are colected after every iteration across the span of 720 iterations. 
After 650 iterations the accuracy slowly stagnates and has a maximum prediction accuracy
of 92.29\%. 
In the left plot picturing the negative likelihood, which is a measure of how well a statistical model represents the observed data.
When training a model the aim is to minimize the negative log-likelihood, which means that the model maximizes the probability of generating the observed data.
Hence, it is visible that in the beginning the model learns more rapidly and in steadily grows its knowledge with some smaller break-ins at the end.
The best value is a negative likelihood of -22.01.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{graphics/metropolis_combined_plot.png}
    \caption{Metropolis sampling baselines}
    \label{metropolis_baselines}
\end{figure}
In contrast, the fig.\ref{metropolis_baselines} used the sampling algorithm of metropolis hasting to train the \ac{RBM}.
Noteworthy is that the prediction accuracy in the right plot has a faster increase in the beginning but already starts to stagnate at around 400 iterations. 
Here, the maximum prediction accuracy achieves 94.15\%. 
The negative likelohood is also growing faster than in Gibbs sampling and exhibits less variablitiy showing a more continuously learning rate.
Here, the best negative likelihood value is -21.80.
Furthermore, to validate the accuracy of the results, they are compared against similar results to those from the literature.\footnote{cf.\cite{bohmNoiseinjectedAnalogIsing2022a}, p. 5; cf.\cite{RestrictedBoltzmannMachine}, p. 1}
Therefore, a good agreement is visible, suggesting that the implementation produces valid results and can thus be considered correct.
As a result, with each sampling method successfully undergoing a training, all the functionalities can be proven right and the prototype can be passed 
into the next design iteration.
The gathered data can later on be used as baseline against the desired new updating mehtod of sampling with a Hopfield Network.


\section{Second Design and Evaluation phase}

This \textbf{design phase} iteration has the goal of implementating the simulator for \ac{mem-HNN} chip shown in fig.\ref{Overall architecture}.
The functionality of this chip is that of a noisy Hopfield Neural Network, which has previously been described in 2.4.4.
The update mechanism is based of he update formula in eq.\ref{noisy_update_HNN_formula}, where noise is injected to imitate the stochastic behaviour of the neurons.
Hence, a \ac{BM} can be modelled by correctly tuning the noise of the Hopfield Neural Network.
Following subfunctionalities need to be established: drawing random neurons to update,
correct injection of the gaussian noise scale, calculating the weighted sum,
comparing the weighted sum + bias + noise against the threshold and saving the new neuron configuration.
Furthermore, the possibility of selectively switching between the N/2 half updating method instead of single spin updates
is to be included in this design phase. 
Hence, this iteration aims to break new ground as it involves implementing the Simulator Pipeline, which has not yet been validated, and integrates the noisy Hopfield Network with the capability for N/2 half updating.

The Hopfield Network is initialized with a size of just one neuron and an sampling iteration counter of \(1500\) iterations with a thermalization of \(100\) sampling steps before 
the neuron is updated.
Thermalization is included to allow the network to perform independent sampling steps and get into a flow to ensure unbiased sampling steps.
The threshold as defined in the update formula is \(0\). 
As experimented the update formula for the implementation of the Hopfield Network looks the following:

\begin{lstlisting}
    for x in range(self.iterations_per_theta):
                    
        self.neuron_index = np.random.randint(0, self.size) #pick a random neuron in the network
        # Calculate the weighted sum for the neuron, excluding its own state
        weighted_sum = sum(self.weights[self.neuron_index][j] * self.configuration[j] for j in range(len(self.configuration)) if j != self.neuron_index)

        self.new_configuration = deepcopy(self.configuration)   #copying the old configuration to create a new one and update it
        if (weighted_sum + self.bias + np.random.normal(0, scale=1.75)) >= self.threshold_theta:          
            self.new_configuration[self.neuron_index] = 1
        else:
            self.new_configuration[self.neuron_index] = 0
            
        self.configuration = deepcopy(self.new_configuration)   #Cloning current configuration and updating the cloned version to the new configuration after comparing with threshold

        if x >= self.thermalization:  
            self.summedConfigurations = self.sum_configurations(self.summedConfigurations, self.new_configuration)    
            self.iterationcounter += 1
        
    self.activationProbabilityPerNeuronDict[self.bias] = self.divide_array_elements(self.summedConfigurations, self.iterationcounter)
    self.bias += 0.025
\end{lstlisting}

The code represented here shows the update meachnism for the single spin update. 
In the beginnig a random neuron is drawn to be updated, which currently everytime is neuron number one because the network size is initialized with one neuron. 
Calculating the weighted sum can be seen as the core of the update formula and is executed first.
Therefore, the weight matrix is selected (indexed by the random drawn neuron) and multiplied by the current configuration resulting in 
the weighted sum that also ignores the weights connected from the drawn neurons.
Afterwards for the comparison against the threshold the according bias of the neuron is added together with the injected noise (scale).
To achieve the injection of noise, a Gaussian normal distribution is added which can modify the activation function, making it compatible with the sigmoid function.\footnote{cf.\cite{bohmNoiseinjectedAnalogIsing2022}, p. 1-2; cf.\cite{mahmoodiVersatileStochasticDot2019}, p. 2}
Technically this is performed by adding \texttt{np.random.normal(0, scale=1)} to the weighted sum and the bias, with 0  the mean of the distribuion and the scale representing the standard deviation. 
Hereby, it is important to find a standard deviation that is very close to the true activation probability is important, otherwise the training of the RBM would not work.
In addition to that, the standard deviation changes with neuron size and needs to be readjusted if changes are made to the networks structure.
Next, if the weighted sum plus bias and noise exceeds the threshold, the corresponding neuron state is set to 1; otherwise, it is set to 0.
Lastly, after enough iterations when the thermalization is exceeded the new configurations
are summed up to enable calculating the activation probability of the neurons. 

For the sake of readability the N/2 half update mechaism is seperated but the used selective method with an according parameter is available in attachment \ref{attachement:HNN_N/2Half Code}.
Hence, in a next step the possibility of the N/2 half updating methdod should be implemented as already mentioned in 2.4.3 and 3.1.
N/2 half is updating neurons synchronously instead of the conventional asynchronously (only one neuron is chosen and updated) used in the Hopfield Network updating mechanism.\footcite[cf.][23-24]{caiHarnessingIntrinsicNoise2019}
Following adsjustments are made in the code to achieve this behaviour:
\begin{lstlisting}
    self.neuron_index = np.random.randint(0, 2, self.size) #pick complete random neurons in the network, result [0,1,1,0,...]

                weighted_sum = np.dot(self.weights[:, :], self.configuration)   
                self.new_configuration = deepcopy(self.configuration)
                bias = self.bias 

                for i in range(len(self.neuron_index)):              
                    #updating function comparing against threshold
                    if self.neuron_index[i] > 0:
                        if (weighted_sum[i] + bias[i] + np.random.normal(0, scale=self.scale)) >= self.threshold_theta:          
                            self.new_configuration[i] = 1
                        else:
                            self.new_configuration[i] = 0
\end{lstlisting}
The randomly drawn neuron index now assigns either a 1 or a 0 for each neuron over the size of the network.
Hereby, a 1 means that the sum of this neuron is calculated and will be compared against the threshold. 
As a result, in a completely random process this would lead to about 25\% of all neurons updated (\(50\% drawn * 50\% updated\)).

Coming back to the last two lines of the first code block, the resulting activation function is obtained by summing all configurations within a single bias configuration.
In a next step, the configurations counted are divided by the number of total sampling iterations within the bias configuration (in this case 1500 iterations).

With the resulting file \texttt{\_hopfield\_network\_v1.py} the aim is to \textbf{Evaluate} the establihed noisy Hopfield Network with a single neuron 
and meassure its activation function.
Like mentioned in 2.4.4, a Hopfield Network has a binary activation function that needs to be made compatible with the sigmoid activation function of the \ac{RBM}.
The decision to use a single neuron enables fast iteration times and clear results on how the network behaves, facilitating the measurement of activation probability.
Once the noise injection proves stable and effective for a single neuron, it proves that it will work for coupled neurons.
The value of the bias ranges from \(-6; 6\) in step sizes of \(0.025\). After completing all sampling iterations beginning with \(-6\) the step size is added to the bias until all iterations are made.
This is sufficient to completely cover an ordinary Hopfield Network with its range of the neuron activation function. 
That following figure \ref{Noisy_acitivation_function_bad} visualizes the resulting activation probability of the single neurons. 
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graphics/combined_noise_activation_plots.png}
    \caption{Modification of the the Hopfield Network binary step activation function}
    \label{Noisy_acitivation_function_bad}
\end{figure}
In the left plot, visualized in blue, the activation probability of the neuron is shown without adding noise to the plot. 
The behaviour is like one would expect it; once the bias reaches 0 the neuron is activated all the time.
In the right figure with the the red line a noise of \(sigma=0.3\) is added.
The resulting activation is probabilistic and follows a rudimentary sigmoid shape, very similar to the activation function of a \ac{BM} shown in fig.\ref{logistic_sigmoid}.
Is is visible that the noise injection works, even though it doesn't perfectly copy the sigmoid function.
Hence, the standard deviation of the noise has a direct influence on the shape of the activation function, similar to the temperature in eq.2.12. 
To correctly mimic the behavior of a \ac{BM}, it is therefore important to select the noise strength such that it corresponds the temparature of a BM, which is \(T=1\).\footcite[cf.][3]{hintonBoltzmannMachines2014}
In following fig.\ref{Noisy_acitivation_function_good} the standard deviation of the noise injection is optimized \(scale=1.75\) and compared to the sigmoid activation function of a \ac{BM}.
The result verifys that the \ac{mem-HNN} can correctly imitate the behaviour of a \ac{BM} and its activation function:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graphics/Noisy_HNN_2.png}
    \caption{Noisy activation function of the Hopfield Network imitating the \ac{RBM}}
    \label{Noisy_acitivation_function_good}
\end{figure}
\section{Third Design and Evaluation phase}

Now, that the proof of concept has been validated, the third \textbf{Desing Phase} has the goal
integrating the \ac{mem-HNN} simulator into Scikit Learn. 
This integration enables the execution of the full Simulator Pipeline shown in fig.\ref{Overall architecture} and allow for the simulation of training a \ac{BM}.
This includes using the weights and biases of the \ac{BM} as an input and performing the sampling with the input.
Finally, the sampled output of the visible and hidden neuron configurations need to be returned, so that the digital computer can update the weights.
With these subgoals the total goal is to enable a complete training of the \ac{BM} with the sampling method ``Hopfield Network''. 
The first technical step is to extend the \texttt{\_rbm.py} to fit the new sampling method: 
\begin{lstlisting}
        if sampling_method == SamplingMethod.GIBBS:
                v_neg = self._sample_visibles(self.h_samples_, rng)
                h_neg = self._mean_hiddens(v_neg)

        elif sampling_method == SamplingMethod.METROPOLIS_HASTING:
            h_neg,v_neg=mcmc_sample(10000,len(self.components_))

        elif sampling_method == SamplingMethod.HOPFIELD_NETWORK:  
            # Hopfield Network Sampling
            v_neg, h_neg = interface_hopfield_sampling(self.components_, self.intercept_visible_, self.intercept_hidden_, iterations_per_theta, N2_HALF=False)    
\end{lstlisting}
Here, the compontents represent the weights of the neurons in the network, while intercept\_visible and intercept\_hidden represent the bias of the neurons. 
Within the, \texttt{hopfield\_network\_interface\_v2.py}, the parameters are taken and an object of the class is initiated. 
Furthermore, the boolean N2\_HALF can be assigned to determine which updating approach is desired. 
\begin{lstlisting}
    def interface_hopfield_sampling(components_, intercept_visible_, intercept_hidden_):
   
        H_net = Hopfield_Net(components_, intercept_visible_, intercept_hidden_)
        H_net.update_network_state()
        
        return H_net.v_neg , H_net.h_neg
\end{lstlisting}
Inside the class, the initialization of all the parameters and weights are performed. 
The update formula needs to calculate the weighted sum, which necessarily requires to know all the weights between the neuron itself, to all the other neurons. 
The decision is to create a weight matrix shown in attachment \ref{attachement:weight_matrix}. 
The function begins by defining the total number of hidden and visible neurons based on the class properties parameters used as input.
These quantities dictate the dimensions of the weight matrix, which, in this instance, results in a matrix of size (100, 64).
This square matrix represents the fully interconnected network, where each neuron can potentially connect to every other neuron, including itself.
Again the \ac{RBM} is used as simple test case and therefore as mentioned in 2.2.3, the diagonal elements (self-connections) are set to zero.

It is important to maintain the model's symmetry in the weight matrix, which is crucial for the energy-based nature of \ac{RBM}s and the dynamics of Hopfield networks.
Hence, for this reason matrix is initialized as a symmetric matrix using NumPy's np.zeros function.
This ensures all initial weights are set to zero before explicitly being defined through the components weights.
Scikit Learn randomly initializes the weights by default close to zero, while the biases are set to zero. 
This small weights allow to support an effective gradient distribution which protects against rapid saturation or inefficient learning,
while the bias set to zero allows the network to begin in a neutral position and learn on its own. 

The subsequent nested loops iterates over the indices for hidden and visible neurons to fill the weight matrix.
For each pair of hidden and visible neurons, the corresponding weights are extracted from the components matrix.
This matrix essentially serves as the template for the interactions between hidden and visible layers.
Indexing within the weight matrix is handled carefully, to respect the structure of the \ac{BM}. 
Therefore, the decision is to set weights between a hidden neuron i and a visible neuron j at positions \texttt{[i, j+num\_hidden]} and \texttt{[j+num\_hidden, i]} to ensure symmetry.

With some more adjustments necessary to the code like transposing the returned neuron configurations, the first training with the sampling method of the Hopfield Network can begin. 
As in the Design Phase 1 the same test case of \ac{RBM} predicting on handwritten digits classification is chosen to make results comparable with the earlier established baselines.
The Hyperparmeters first are set to a scale of 1.75 and thermalization of 100 as used for the single neuron in fig.\ref{Noisy_acitivation_function_good} but with more sampling steps (5000) than before. 
The results of the training are visualized in fig.\ref{HNN_training}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graphics/HNN_combined_plot.png}
    \caption{Hopfield Network sampling baselines}
    \label{HNN_training}
\end{figure}
First of all, it can be recognized that the training was successful, which was uncertain. 
Noteworthy, that with this method the output had to be transposed to be able to parse the data Scikit Learn.
In a second look it can be seen that the negative likelihood is far more stable as gibbs sampling and rather has similarities with metropolis hastings sampling.
Here, the best value is a likelihood of -22.3, so slightly worse than metropolis hastings and gibbs sampling. 
The right plot showing the prediction accuracy has the highest ascent of all the three graphs, which proofs that the 
less iterations are required to receive good results compared to the other two methods.
Still, The best prediction value is 90.81\% and therefore worse than gibbs and metropolis hastings. 
The reason for this can be the hyperparameter tuning.
Since the noisy Hopfield Network method can become sensitive fast or is not adjusted correctly 
for this amount of neurons in the network, receiving this result without further adjustments already looks promissing. 

As a result, the following hyperparameters are tuned to possibly received better outcomes. 
Specifically, the scale, which represents the standard deviation and is used as noise is tuned. 
In addition to that the amount of sampling iterations within a single training iteration is tuned.
One extended parameter could be the learning rate and the total iterations but to have an appropriate benchmark against the other two 
sampling methods these parameters are locked in. 
Last but not least, the possibility of tuning the thermalization could help out too even if slightly less significant than the other two hyperparameters.
Since the training takes around 40 minutes to complete tuning too many hypaerparameters takes too much time for the period of this thesis. 
First hyperparameter researched is the influence of the standard deviation (scale) to the maximum prediction accuracy. 
Especially the maximum prediction accuracy of the last 50 training iterations are gathered since this is the relevant area for inference.
Given that the Hopfield Network operates as a statistical sampling method, the standard deviation is also calculated for these final 50 training iterations.
This is done for the scale range of 1.0 to 2.0 within step sizes of 0.05, totalling to 21 single trainings done.
The result is illustrated in figure\ref{Hyperparamers_Scale_ohne}:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{graphics/NEW_Scale_Ohne_N2_Half_Pred_Acc.png}
    \caption{Hopfield Network Hyperparametertuning scale}
    \label{Hyperparamers_Scale_ohne}
\end{figure}
The result shows that beginning with a scale of 1.0 the prediction accuracy slowly rises until the scale of 1.6
Here, the maximum value is 94.77\% and with that surpasses the performance of both metropolis hastings and gibbs sampling. 
After a scale of 1.75 the prediction accuracy falls off a cliff indicating that the activation function of the \ac{RBM} is not modelled correctly anymore.
This shows that depending an network size and workload the adjustment within this method is important to achieve good results. 
The standard deviation follows the maxmimum prediction accuracy pretty closely and has no outliers indicating that the prediction accuracy is only a lucky random training.
Close to the sclae of 1.0 the deivation is slightly lower compared to the rest of the plot, showing that the scale could be too low to model the sigmoid function correctly. 

In a next step, the best fit with a scale of 1.6 is used for the second hyperparameters. 
The sampling iterations within one training iterations are now tuned.
Hence, the decision is to begin with 1000 sampling iteration continiunig with an increase of 1000 iterations until 15000 iterations are reached. 
With that the training showed that the interesting are is around 1000 to 4000 iteration and that the step size of 1000 is too big for that. 
To be more granular two extra trainings with iterations 1500 ad 2500 werde completed, totalling to 17 trainings performed.
The values are extracted as before, by considering the last 50 iterations and then calculating both the maximum value and the standard deviation from this subset.
The visualized results can be found in following figure\ref{Hyperparamers_Iteraions_ohne}:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{graphics/Iterations_Ohne_N2_Half_Pred_Acc.png}
    \caption{Hopfield Network Hyperparametertuning sampling iterations}
    \label{Hyperparamers_Iteraions_ohne}
\end{figure}
The line of maximum prediction accuracy starts at the first iteration with a value close to 0.3 and rises rapidly to reach a value just above 0.92 after around 2500 iterations.
From the point of 4000 iterations onwards, the accuracy remains largely constant with slight fluctuations.
The best value is at 15000 iterations with an prediction accuracy of 94.5\%, while at 4000 iterations the accuracy is at 93.35\%.
The error bars indicating the standard deviation are large at the beginning of the graph, which indicates that not enough neurons were updated in the sampling process.
However, with the number of iterations increasing, the error bars become smaller resulting in a more stable accuracy.
Key take away is that after 4000 iterations there are no significant changes to the outcome of the prediction accuracy.

To create a good comparison, the same parameters scale and sampling iterations are tuned for this updating method. 
In this method, 21 complete training sessions are also carried out for this purpose.
Beginning with the scale the result is visualized in figure\ref{Hyperparamers_Scale_mit} :
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{graphics/NEW_Scale_MIT_N2_Half_Pred_Acc.png}
    \caption{Hopfield Network Hyperparametertuning scale for N/2}
    \label{Hyperparamers_Scale_mit}
\end{figure}
On a first glance, the prediction accuracy beginning from left to right is constantly rising until it reaches the scale of 1.6.
Here, the maximum prediction accuracy tops out at 94.76\%.
What is interesting, that in \ref{Hyperparamers_Scale_ohne} the exact same scale has also the best performance. 
With increasing the scale after the top of 1.6, the accuracy falls off a cliff. 
Noteworthy, is that in comparison with \ref{Hyperparamers_Scale_ohne} the N/2 half updating method has nearly equal prediction accuracy
but is more sensitive. 
This means that the configuration of the hyperparameters is tougher for this updating method and results can get worse more easily. 
The standard deviation represented in the error bars is high at the lower scale values validating that the sigmoid function is not properly mapped. 
This is similar for too high values beginning at a scale of 1.8.

The second hyperparameter are the amount of sampling iterations required to achieve good results. 
In the training process the plan is to begin with the same step sizes like earlier on. Soon it was clear,
that the granularity needs to be much finer too achieve good results. 
Therefore, the decision is to start at iteration 201 (1st iteration after the 200 thermaliation steps) and ending with a sampling iteration of 250 with a step size of 1.
This totals to 50 complete training sessions. The new results are illustrated in figure\ref{Hyperparamers_Iteraions_mit}:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{graphics/Iterations_MIT_N2_Half_Pred_Acc.png}
    \caption{Hopfield Network Hyperparametertuning sampling iterations for N/2 half}
    \label{Hyperparamers_Iteraions_mit}
\end{figure}
The figure shows that with only 10 sampling iterations after the thermalization good prediction accuracies of 94\% can be achieved. 
The maximum prediction value is 95.10\% at 221 sampling iterations, which surpasses the earlier achieved results.
Nonetheless, the key message here is that far less sampling iterations are needed within one big training iteration of the \ac{RBM}
to achieve good results. 
When comparing the number of 221 to the around 4000 sampling iterations required in \ref{Hyperparamers_Iteraions_ohne} it would be more efficient by a factor of 
\(18.09x\). In return, this updating methods saves time and also has less energy consumption required.

With this third design and evaluation phase ending all the functionalities of the prototype are implemented and the prototype itself is complete. 

\section{Final Evaluation phase}
In this final evaluation phase the functional prototype should now be validated within the evaluation phase of the \ac{DSR} framework.
The goal is to answer the research question ``Can Boltzmann Machines be \textbf{efficiently} implemented on physics-inspired
Hardware accelerators by analog noise injection?''.
Since the key word here is efficiently, a simulation will be performed to meassure the parameters required for evaluation.
With the model purpose set, the model scope(time frame) of the simulation introduced in 3.4 is to be considered as short with only mutliple weeks and only one person working on the simulation. 

The next step is to define the result variables. The two variables that are of interest in literature to meassure the performance of the
\ac{mem-HNN} accelerator are troughput (samples/sec) and energy consumption (energy/operation). 
These two metrics are widely used in literature and can create a good comparison.\footnote{cf.\cite{bellettiJanusFPGABasedSystem2009}, p. 54-55; cf.\cite{aaditAcceleratingAdaptiveParallel2023}, p. 1-2; cf.\cite{ortega-zamoranoFPGAHardwareAcceleration2016}, p. 16-17}
Hereby, throughput can be defined as the time needed per Hopfield cycle (sampling iteration) per second.\footcite[cf.][6-7]{bohmNoiseinjectedAnalogIsing2022} 
Meanwhile, the energy consumption is defined by summing over all the single energy consumptions within one sampling iteation.
The resulting unit is called energy/operation. 

Now that the result variables are set the input parameters neeed to be clarified. 
For the throughput knowledge about the autocorrelation of the sampling is required.
Autocorrelation is a statistical measure that captures the degree of correlation between successive configurations generated by timeseries, in this case sampling algorithm for the training of a \ac{RBM}.\footcite[cf.][1-6]{tanakaReductionAutocorrelationHMC2017}
Long correlations between configurations can reduce the effective sample size and lead to inefficiency impacting the precision of the model.
This is important for the result variable ``troughput'' because it allows to know when the sampling is statistically independent and ready to use and therefore how many sampling iterations neeed to be done for an effective training. 
The equation to calculate the statistical dependency of 2 successive samples is the auto-covariance function: 
\begin{equation}
    K_{XX}(t_1, t_2) = \mathbb{E}[(X_{t_1} - \mu_{t_1})(X_{t_2} - \mu_{t_2})] = \mathbb{E}[X_{t_1} X_{t_2}] - \mu_{t_1}\mu_{t_2},
\end{equation}
with \(t_1,t_2\) being two distinct points in time and \(X_{t1},X_{t2}\) are random variables representing the values of the stochastic process at the distinct time points. 
\(\mu_1,\mu_2\) are the mean (expected) values of the random variables \(X_{t1},X_{t2}\). 
The \(\mathbb{E}\) is an expectation operator and is used to calculate the expected value of the expression within the brackets.
Also, the cycle speed of the \ac{mem-HNN} accelerator is needed. 
The clock frequency is around 700MHz and therefore can sample 700 million samples per second, which is about 1.44ns for a single clock cylce.\footcite[cf. table1][7-8]{caiPowerefficientCombinatorialOptimization2020} 
Noteworthy, these numbers are takem from a neural network with 111 neurons. 
On the other hand, the only input variable for the energy consumption is the energy model of the \ac{mem-HNN}, which is introduced in 4.2.1 and is developed by HPE in combination with the Forschungszentrum Jülich.
This allows to meassure each of the individual energy consumptions that are configured to the specifications of the \ac{mem-HNN} hardware accelerator.\footcite[cf.][1-5]{hizzaniMemristorbasedHardwareAlgorithms2023}
For both of the methods one input of course is the finished prototype, that mirrors the functionalities of the \ac{ASIC} on a high level.

With the simulation model specified, the autocorrelation can be implemented into the prototype. 
The decision is to average the values of the output configurations from row to row to an average of 60.
This allows to extract a smoother autocorrelation plot that is not too biased in terms of the statistical approach.
In the attachment\ref{attachement:autocorrelation} the full implementation of the autocorrelation function is available.
To compare the performance of the individual sampling methods in terms of the autocorrelation a threshold is required. 
Here, \(1/\mathrm{e}\) is chosen since it is inspired by many fields, like physics(diffusion length), chemistry and mathematics(half-life as a threshold) etc..\footnote{cf.\cite{archieStatisticalAnalysisHeterozygosity1985}, p. 624-630; cf.\cite{bohmNoiseinjectedAnalogIsing2022}, p. 7-13}
Folliwng results are visualized in following figure\ref{Autocorr comparison}:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graphics/Visualisierungen_Autocorr_individual_7.png}
    \caption{Autocorrelation for the three sampling methods}
    \label{Autocorr comparison}
\end{figure}

The first row shows the conventional \textbf{Metropolis Hastings} sampling method. On the right plot the autocorrelation
and the according sampling steps are visualized. It can be seen, that the value falls below the threshold at around \textbf{100 sampling steps}, 
Falling below the threshold of \(1/\mathrm{e}\) symbolizes that statistical independent samples are generated.
The left plot therefore meassures the first sampling step within each training iteration that falls below this threshold. 
Here, named correlation time. It can be seen that the the scale on the x-axis in the left plot is higher compared to the other two sampling approaches, 
meaning that metropolis hastings is \textbf{more sensitive}. 

The second row is the \textbf{single neuron Hopfield Network} sampling approach. 
In the right plot it shows that the autocorrelation threshold is reached around after around \textbf{200 sampling steps}.
Even if the value is \textbf{2x worse} than with metropolis Hastings the correlation time for the whole training is \textbf{more stable}.
So even if the initial autocorrelation takes longer over a whole training period the end result has an \textbf{better average} than Metropolis Hastings.

The last approach is the \textbf{N/2 half Hopfield Network}. 
The result verifys the new methods right to exist with only about \textbf{3 iterations} to surpass the threshold in the right plot.
Furthermore, the methods correlation time in the left plot shows that it is by far more \textbf{stable} than the other 2 approaches.
When setting this into perspective even with a conservative average of 5 iterations as correlation time, N2/Half updating therefore
performs \textbf{40x better}, than the single neuron Hopfield Network and \textbf{20x better} than Metropolis Hastings.
When comparing the correlation at the end of the traing iterations, the performance even increases: \textbf{34x better} than the single neuron Hopfield Network (correlation time value of 170) and \textbf{46,6x better} than Metropolis Hastings (correlatin time value of 233).
Surprisingly for this updating method large statistical dependency could be seen in two out of the 720 training iterations.
This means that for these two iterations the autocorrelation \textbf{doesn not fall under the threshold} of \(1/\mathrm{e}\).
The training was attempted three times, and in each instance, the phenomenon occurred between iterations 300 and 500.
Still, this has no impact on the performance of the training and therefore can be seen as outlier.
It is open for further research to identify why this doesn not happen with the other two approeaches and what is the cause.
Since the correct value for not falling under the threshold would be infinity the scale in the plot would be too large. 
Therefore, the same plot with these two outliers can be found in the attachment\ref{attachement:autocorrelation_errors}.

After identifying the autocorrelation of the desired metric, "throughput," it can be calculated by combining the autocorrelation with the cycle speed of the \ac{mem-HNN}.
This is done by taking the autocorrelation time of both Hopfield Network approaches (with N/2 Half and without) and mutlitplying them with the cycle time of the \ac{mem-HNN}.
In addition to that the inverse of the result is calculated resulting in the desired throughput metrics `` samples/second''.
Noteworthy, the cycle time for the \ac{mem-HNN} is 1.44ns for 111 neurons in the network. 
Because the network in this thesis has 164 neurons, the time used for the calculation is estimated to be about 2ns, which can be seen as conservative estimate.
The subsequent visualization\ref{Throughput comparison} shows the throughput for the Hopfield Network:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{graphics/Visualisierungen_throughput_log_2.png}
    \caption{throughput}
    \label{Throughput comparison}
\end{figure}
The following results can be gathered when comparing the two lines in terms of efficiency and stability:
The Normal Hopfield Network (green line) displays a stable throughput performance throughout the iterations, consistently above \(\mathbf{10^6}\)
samples per second. This suggests a stable and predictable behavior.
The N2 Half Hopfield Network (red line) exhibits significant variability in throughput. 
While it generally hovers around \(\mathbf{10^8}\) \textbf{to} \(\mathbf{10^9}\) the throughput is more sensitive and fluctuates more compared to the single update approach. 
To get a better feeling of the data, the average of the throughput is calculated: N/2 Half Hopfield Network has
an average of \textbf{144 megasamples/second}, while the
Normal Hopfield Network has an average of: \textbf{2,3 megasamples/second}. 
This means that the computation speed of the N/2 Half method \textbf{is faster} by a factor of \textbf{62.72x}.
Also in the attachment\ref{attachement:throughput_errors} is a version with the outliers included symolizing a throughput of zero for these two iterations.

The results of the autocorrelation implies a low computing time and low energy consumption than with conventional hardware.
To confirm this theory, the second metric to simulate is the \textbf{energy consumption} in energy/operation. 
Hence, the mentioned energy model needs to be implemented into the Hopfield Network interface.
The first adaptation is to \textbf{intialize the energy model} with the size of the neural network (164) as it impacts the size of the crossbar array and influences the overall energy consumption.
An increase in the number of neurons causes a higher energy consumption due to the enlarged crossbar structure.

Afterwards, the two parameters ``a\_pattern'' and ``a\_WL'' need to be calculated.
Hereby, a\_pattern refers to the currents that flow through the respective bitlines.
The current for each bitline is determined by the average of the weighted sum and is accumulated with each iteration.
Subsequently, it is divided by the number of sampling iterations to obtain an average for the respective training iteration.
For an correct calculation and imitation of the \ac{mem-HNN}, there is one more restriction to solve. 
The digital computer generates negative and positive weights and biases, which is not possible in the hardware.
Therefore, the weight matrix needs to be adjusted to handle positive and negastive weights seperately. 
Specifically, the weight matrix is discretized to the according bit resolution of the \ac{mem-HNN}, which is 5bit.
In the ealier design phases, the values were calculated with perfect resolution but for the energy model of the accelerator this is possible. 
Despite the low bit resolution there are many papers that proof that even with a small resolution a good performance can be achieved.\footnote{cf.\cite{maEra1bitLLMs2024}, p. 1; cf.\cite{GitHubHtqinQuantSR}, p. 1; cf.\cite{rouhaniMicroscalingDataFormats2023}, p. 1; cf.\cite{rouhaniSharedMicroexponentsLittle2023}, p. 1}
Since only positive weights can exist within the accelerator, these are separated and written into their own matrix.
This modification is essential to accommodate the hardware constraints that prevent the use of negative weights.

On the other hand, A\_Wl is the average configuration change.
This means This parameter tracks the average changes in configuration within the wordline.
Each time the state changes from 0 (no current) to 1 (current flows), energy is consumed to initially let the metal ions flow.
Therefore, every position in the configuration must be compared with the following configuration position wise.
Subsequently, an average change rate is calculated by dividing by the number of iterations.
This approach quantifies the energy cost associated with state transitions within the network configuration.
An estimate for this, especially for N/2 half is that 25\% of the neurons are updated in one training iteration (50\% randomly drawn and 50\% of that updated).
All the adjustments for the energy model to the interface can be found in the version 5 of the hopfield interface as part of the digital delivery.
The result of the energy consimption is illustrated in following figure\ref{Energy output_2}:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graphics/energy_and_averages_plot_4.png}
    \caption{Energy consumption of the \ac{mem-HNN}}
    \label{Energy output_2}
\end{figure}
The left plot shows the energy consumed per sampling iteration (eq.\ to 1 clock cylce of the \ac{mem-HNN}) in piko Joules.
In the beginning of the training the average clock cycle has an total energy consumption of around 45 piko Joules.
Further in the training at atround 100 training iterations the consumption drops to around 35 piko Joules. 
In the end of the training the value falls just below the 20 piko Joules mark. 
The decrease of the energy consumption over the time comes from the weights of the neural netowotk that get better and better configured over time.
Noteworthy, the Memcrossbar energy is the only variable energy consumer that actively changes, while the digital-to-analog converter and the pseudo-random number generator stay horizontal.
A composition of the energy consumption is shown in the right plot.
Here 85\% of the energy is consumed entirely by the Memristor Crossbar and the support entities consume around 15\% of the energy.
The results of the energy model an the energy consumption can be verified by the following paper.\footcite[cf.][4]{hizzaniMemristorbasedHardwareAlgorithms2023}

Finally, it is important to mention that not all hardware components are included. and especially the communication and updating of 
For example the memory and the controller of the \ac{mem-HNN} and are not included in the plot.
Also to consider are the weights in the digital computer, that probably consume 10x the energy of the training on the analog \ac{mem-HNN}. 

In a next step the power consumption of the training is targeted as this delivers a good comparison to other
hardware components like \ac{CPU}s, \ac{GPU}, \ac{ASIC}s or \ac{FPGA}s.
Therefore, the energy per sampling iteration is divided by the time of one clock cycle (2ns) since \(P = \frac{\Delta E}{\Delta t}\).
Furthermore, the right plot is based on the power and cumulates the power multiplied with the sampling iteration to vissualize how much energy the training the neural network consumes for this workload.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{graphics/energy_consumption_cumulative_plot.png}
    \caption{Power consumption of the \ac{mem-HNN}}
    \label{Power consumption}
\end{figure}
As shown in figure\ref{Power consumption} the power required to train is around 22.5 mW, which compared to a basic CPU(20-40W)
is significant less. 
Again with training iterations continuing to grow the power consumption decreases. 
At the end of the training around 620 iterations, an power consumption of around 10mW is reached. 
The entire power consumption is mostly stable with some larger fluctuatins in the beginning of the training areound 0 to 70 iterations.
When looking at the right plot, which symbolizes the cumulative energy required for the training in mJ.
Here, it is visible that a complete training with 720 iterations requires around 6.2 mJ. 
Again it is important to keep in mind that these numbers consist of only the \ac{mem-HNN} and do not include the digital computer.                                                                  	                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    

With that the evaluation phase can successfully verify that the models purpose of the simulation, which is to answer the reasearch question, is achieved.
Therefore, Boltzmann Machines can indeed be efficiently implemented on the physics-inspired Hardware accelerator by analog noise injection. 