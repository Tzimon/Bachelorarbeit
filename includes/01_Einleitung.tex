\chapter{Einleitung}


\section{Motivation}

In the research and development of generative AI models, the computing speed and energy efficiency
are increasingly becoming the focus\footcite[Vgl.][1]{luccioniPowerHungryProcessing2023}
The authors of Open AI confirm, that the growth rate of machine learning models 
surpassed the growth of efficiency within computerchips.
The required computing power of the models double each 3-4 months but the power of computerchips, after Moore’s Law 
double only every 2 years.\footcite[Vgl.][1]{darioamodeiAICompute}
Focusing on current problems like rising energy consumption of datacenters and the associated
greenhouse gas emissions, the search for more efficient solutions is essential for the future.
Worldwide energy consumption of datacaenters increase yearly around 20-40\%, which
means that in 2022 about 1,3\% of the total global energy consumption and about 1\%
of energy-related global greenhouse gas emissions was caused by them.\footcite[Vgl.][1]{hintemannDataCenters20212022} 
However, it is not clear how large the AI share contributed to the total numbers.

\section{Problem statement}

One approach that is already well known is the use of AI accelerators based on
ASICs (application-specific integrated circuits) - i.e. circuits that are used application specific,
such as Google TPUs (Tensor Processing Unit).\footcite[Vgl.][39]{wittpahlKuenstlicheIntelligenzTechnologie2019} 
This is useful because the usage of multimodels for discirminating tasks compared to
task specific models are more energy intense.\footcite[Vgl.][5]{luccioniPowerHungryProcessing2023}
One promissing alternative concept in research is the usage on physics-inspired hardware accelerators, that are primarly used for optimization problems
because of their ability to solve problems faster and more efficient than GPUs.\footcite[Vgl.][1]{mohseniIsingMachinesHardware2022}
A scalable physics-inspired hardware accelerator (also called Ising-machine),
that surpasses the power of existing standard digital computers, could have a large influence
on practical applications for a variety of optimization problems.\footcite[Vgl.][1]{mohseniIsingMachinesHardware2022}

Such physics-inspired hardware accelerator offer, due to their special calculation method,
potential for efficient processing of computationally intensive tasks. 
Specifically, the acceleration in contrast to digital computers is achieved by calculating
the computationally intense tasks with analog signals.
On top of that the implementation on dedicated hardware offers the possibility to exlpoit the parallelization
of digitwal hardware accelerators and analog computation.\footcite[Vgl.][4]{mohseniIsingMachinesHardware2022}

Interesting enough, despite their different applications, the energy function of the hardware accelerator that is used
in Ising-machines shows big parallels to those used in \ac{BM}, therefore it can be suggested that Ising-machines could work well for AI.\footcite[Vgl.][10]{caiHarnessingIntrinsicNoise2019}
Ising-machines strive to minimize their energy, which is defined by the pairwise 
interaction of binary variables (Spins).\footcite[Vgl.][1]{wangOscillatorbasedIsingMachine2017} 
In contrast, \ac{BM}s are energy-based neuronal networks that are used for classification tasks by 
acllocating a skalar energy for each configuration of variables.
Minimizing the total network energy is therefore equal with the solution of a optimization problem.\footcite[Vgl.][2]{nazmbojnordiMemristiveBoltzmannMachine2016} 
Current problems with \ac{BM}s are the high complexity and high requirements for the all-to-allcommunication between the processing units,
which causes the implemention on conventional digital computers to be inefficient, but also 
an inherently slow convergence in certain processes such as simulated annealing.\footcite[Vgl.][1]{nazmbojnordiMemristiveBoltzmannMachine2016} 
Theese challenges complicate the training and the usage of \ac{BM}s, especially for large data volumes and complex optimization tasksl.\footcite[Vgl.][2]{nazmbojnordiMemristiveBoltzmannMachine2016} 
Nevertheless the similaritis of both models implicate, that Ising-machines could be able to execute this 
specific AI-model with higher energy efficiency and with higher computing speed.
Currently there are only few concepts that exists on how to achieve a implementation of a \ac{BM} within on a Ising-machine.
The paper of the authors Mahdi, Nazm, BojnordiEngin and Engin Ipek is a promissing approach,
However it could not be shown how a implementation on a real accelerator chip could function.

With the given background, the following central research question for this theis arises:

\begin{enumerate}
    \item Can Boltzmann Machines be efficiently implemented on physics-inspired Hardware accelerators by analog noise injection?
        \begin{itemize}
            \item What is the accuracy of the AI-model on the hardware accelerator?
                \begin{itemize}
                    \item Metric: Prediction accuracy and negative Likelihood
                \end{itemize}
            \item Comparison between other hardware accelerators, FPGA, GPU, or CPU within the literature in terms of energy efficiency and computing speed.
                \begin{itemize}
                    \item Metrics: Throughput (Samples/Sec), Energy usage (Energy/Operation)
                \end{itemize}
        \end{itemize}  
\end{enumerate}

It is therefore necessary to test whether this generative AI model is compatible with Ising machines
machines and whether this solution is efficient or not.


\section{Objective}

The primary objective of this bachelor thesis ist the research and extension of a existing physics-inspired
hardware accelerator(Ising machine) for the implementation and evaluation of \ac{BM}s as an energy based 
AI-model. The aim is to answer the posed research question. 
In addition to that, it would be beneficial if rules for the influence of hyperparameters could be established
since there is no data available for this new method.

To initially accomplish this objective it is necessary to establish a simulator pipeline to the
hardware accelerator that translates \ac{BM} on top of it.
The simulator pipeline consists of an existing machine learning library and an existing hardware accelerator
that are connected to each other.
With the simulator pipeline it needs to be shown that it is possible for the hardware accelerator
to realize \ac{BM}s.

Within the simulator pipeline, the activation probabilities of individual neurons are measured on the simulated hardware.
If this process proves successful, it is then expanded to simulate a complete neuronal network.
The final step is that the hardware accelerator can be used for training and iterference
and is comparable to conventional machine learning libraries.
This phase includes a carefully adjustment and possibly extension of the existing accelerator to be compliant 
with the specific requirements of \ac{BM}s.

If the simulator pipeline can be validated a workload consisting of a standard data set
to recognize handwritten digits will be tested.
The prediction accuracy, throughput (samples/sec) and the energy consumption (energy/operation)
of the Boltzmann machines on the Ising hardware accelerator will be investigated as metrics in order to
thereby answering the second part of the posed research questions.


\section{Research method}

Design Science Research

\begin{enumerate}
    \item \textbf{Problemorientierung:} DSR fokussiert auf die Lösung praktischer Probleme, wie die Forschung zur Steigerung der Effizienz und Rechengeschwindigkeit in KI-Modellen.
    \item \textbf{Artefakt Entwicklung:} Zentral in DSR ist die Entwicklung innovativer Artefakte. Die Arbeit zielt darauf ab, ein solches Artefakt in Form des physikinspirierten Hardwarebeschleunigers weiterzuentwickeln und für KI-Modelle einzusetzen.
    \item \textbf{Iterative Evaluation:} Durch die iterative Vorgehensweise in DSR kann die Ausarbeitung der Lösung fortlaufend verbessert und angepasst werden, was für die Entwicklung und Optimierung von KI-Systemen entscheidend ist (ebenfalls das Konzept).
    \item \textbf{Beitrag zur Wissensbasis und Praxisrelevanz:} DSR unterstützt die Generierung neuer Erkenntnisse und stellt sicher, dass Forschungsergebnisse sowohl theoretisch fundiert als auch praktisch anwendbar sind, was mit den Zielen Ihres Projekts im Einklang steht. Untermethodik könnte hierbei eine Simulation sein. Variabel, je nach Verlauf der Forschung.
\end{enumerate}


\section{Aufbau der Arbeit}