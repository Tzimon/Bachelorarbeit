@article{ackleyLearningAlgorithmBoltzmann1985,
  title = {A Learning Algorithm for Boltzmann Machines},
  author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
  date = {1985-01-01},
  journaltitle = {Cognitive Science},
  shortjournal = {Cognitive Science},
  volume = {9},
  number = {1},
  pages = {147--169},
  issn = {0364-0213},
  doi = {10.1016/S0364-0213(85)80012-4},
  url = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
  urldate = {2024-02-16},
  abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\42UB7SBL\Ackley et al_1985_A learning algorithm for boltzmann machines.pdf}
}

@article{ahadNeuralNetworksWireless2016,
  title = {Neural Networks in Wireless Networks: {{Techniques}}, Applications and Guidelines},
  shorttitle = {Neural Networks in Wireless Networks},
  author = {Ahad, Nauman and Qadir, Junaid and Ahsan, Nasir},
  date = {2016-06-01},
  journaltitle = {Journal of Network and Computer Applications},
  shortjournal = {Journal of Network and Computer Applications},
  volume = {68},
  pages = {1--27},
  issn = {1084-8045},
  doi = {10.1016/j.jnca.2016.04.006},
  url = {https://www.sciencedirect.com/science/article/pii/S1084804516300492},
  urldate = {2024-02-28},
  abstract = {The design of modern wireless networks, which involves decision making and parameter optimization, is quite challenging due to the highly dynamic, and often unknown, environmental conditions that characterize wireless networks. There is a common trend in modern networks to incorporate artificial intelligence (AI) techniques to cope with this design complexity. While a number of AI techniques have been profitably employed in the wireless networks community, the well-established AI framework of neural networks (NNs), well known for their remarkable generality and versatility, has been applied in a wide variety of settings in wireless networks. In particular, NNs are especially popular for tasks involving classification, learning, or optimization. In this paper, we provide both an exposition of common NN models and a comprehensive survey of the applications of NNs in wireless networks. We also identify pitfalls and challenges of implementing NNs especially when we consider alternative AI models and techniques. While various surveys on NNs exist in the literature, our paper is the first paper, to the best of our knowledge, which focuses on the applications of NNs in wireless networks.},
  keywords = {Artificial intelligence,Computational intelligence,Neural networks,Wireless networks},
  file = {C:\Users\simon\Zotero\storage\I74EW2SD\S1084804516300492.html}
}

@article{amariInformationGeometryBoltzmann1992,
  title = {Information Geometry of {{Boltzmann}} Machines},
  author = {Amari, S. and Kurata, K. and Nagaoka, H.},
  date = {1992-03},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {3},
  number = {2},
  pages = {260--271},
  issn = {1941-0093},
  doi = {10.1109/72.125867},
  url = {https://ieeexplore.ieee.org/abstract/document/125867},
  urldate = {2024-02-16},
  abstract = {A Boltzmann machine is a network of stochastic neurons. The set of all the Boltzmann machines with a fixed topology forms a geometric manifold of high dimension, where modifiable synaptic weights of connections play the role of a coordinate system to specify networks. A learning trajectory, for example, is a curve in this manifold. It is important to study the geometry of the neural manifold, rather than the behavior of a single network, in order to know the capabilities and limitations of neural networks of a fixed topology. Using the new theory of information geometry, a natural invariant Riemannian metric and a dual pair of affine connections on the Boltzmann neural network manifold are established. The meaning of geometrical structures is elucidated from the stochastic and the statistical point of view. This leads to a natural modification of the Boltzmann machine learning rule.{$<>$}},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Computer architecture,Information geometry,Information processing,Machine learning,Manifolds,Network topology,Neural networks,Neurons,Probability distribution,Stochastic processes,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\5Q29TWP2\Information_geometry_of_Boltzmann_machines.pdf}
}

@article{amirsoleimaniInMemoryVectorMatrixMultiplication2020,
  title = {In-{{Memory Vector-Matrix Multiplication}} in {{Monolithic Complementary Metal}}–{{Oxide}}–{{Semiconductor-Memristor Integrated Circuits}}: {{Design Choices}}, {{Challenges}}, and {{Perspectives}}},
  shorttitle = {In-{{Memory Vector-Matrix Multiplication}} in {{Monolithic Complementary Metal}}–{{Oxide}}–{{Semiconductor-Memristor Integrated Circuits}}},
  author = {Amirsoleimani, Amirali and Alibart, Fabien and Yon, Victor and Xu, Jianxiong and Pazhouhandeh, M. Reza and Ecoffey, Serge and Beilliard, Yann and Genov, Roman and Drouin, Dominique},
  date = {2020},
  journaltitle = {Advanced Intelligent Systems},
  volume = {2},
  number = {11},
  pages = {2000115},
  issn = {2640-4567},
  doi = {10.1002/aisy.202000115},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202000115},
  urldate = {2024-02-15},
  abstract = {The low communication bandwidth between memory and processing units in conventional von Neumann machines does not support the requirements of emerging applications that rely extensively on large sets of data. More recent computing paradigms, such as high parallelization and near-memory computing, help alleviate the data communication bottleneck to some extent, but paradigm-shifting concepts are required. In-memory computing has emerged as a prime candidate to eliminate this bottleneck by colocating memory and processing. In this context, resistive switching (RS) memory devices is a key promising choice, due to their unique intrinsic device-level properties, enabling both storing and computing with a small, massively-parallel footprint at low power. Theoretically, this directly translates to a major boost in energy efficiency and computational throughput, but various practical challenges remain. A qualitative and quantitative analysis of several key existing challenges in implementing high-capacity, high-volume RS memories for accelerating the most computationally demanding computation in machine learning (ML) inference, that of vector-matrix multiplication (VMM), is presented. The monolithic integration of RS memories with complementary metal–oxide–semiconductor (CMOS) integrated circuits is presented as the core underlying technology. The key existing design choices in terms of device-level physical implementation, circuit-level design, and system-level considerations is reviewed and an outlook for future directions is provided.},
  langid = {english},
  keywords = {complementary metal–oxide–semiconductor,in-memory computing,inference,memristors,redox-based random access memories,resistive switching memories,ungelesen,vector-matrix multiplications},
  file = {C:\Users\simon\Zotero\storage\SZ37ATSG\In‐Memory Vector‐Matrix Multiplication in Monolithic Complementary.pdf}
}

@online{baiEfficiencySystematicSurvey2024,
  title = {Beyond {{Efficiency}}: {{A Systematic Survey}} of {{Resource-Efficient Large Language Models}}},
  shorttitle = {Beyond {{Efficiency}}},
  author = {Bai, Guangji and Chai, Zheng and Ling, Chen and Wang, Shiyu and Lu, Jiaying and Zhang, Nan and Shi, Tingwei and Yu, Ziyang and Zhu, Mengdan and Zhang, Yifei and Yang, Carl and Cheng, Yue and Zhao, Liang},
  date = {2024-01-03},
  eprint = {2401.00625},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.00625},
  url = {http://arxiv.org/abs/2401.00625},
  urldate = {2024-02-23},
  abstract = {The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between various resources and corresponding optimization techniques. A standardized set of evaluation metrics and datasets is also presented to facilitate consistent and fair comparisons across different models and techniques. By offering a comprehensive overview of the current sota and identifying open research avenues, this survey serves as a foundational reference for researchers and practitioners, aiding them in developing more sustainable and efficient LLMs in a rapidly evolving landscape.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\simon\\Zotero\\storage\\YPEGVJL8\\Bai et al_2024_Beyond Efficiency.pdf;C\:\\Users\\simon\\Zotero\\storage\\XTGM8EVM\\2401.html}
}

@article{barraEquivalenceHopfieldNetworks2012,
  title = {On the Equivalence of {{Hopfield}} Networks and {{Boltzmann Machines}}},
  author = {Barra, Adriano and Bernacchia, Alberto and Santucci, Enrica and Contucci, Pierluigi},
  date = {2012-10-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {34},
  pages = {1--9},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.06.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608012001608},
  urldate = {2024-02-16},
  abstract = {A specific type of neural networks, the Restricted Boltzmann Machines (RBM), are implemented for classification and feature detection in machine learning. They are characterized by separate layers of visible and hidden units, which are able to learn efficiently a generative model of the observed data. We study a “hybrid” version of RBMs, in which hidden units are analog and visible units are binary, and we show that thermodynamics of visible units are equivalent to those of a Hopfield network, in which the N visible units are the neurons and the P hidden units are the learned patterns. We apply the method of stochastic stability to derive the thermodynamics of the model, by considering a formal extension of this technique to the case of multiple sets of stored patterns, which may act as a benchmark for the study of correlated sets. Our results imply that simulating the dynamics of a Hopfield network, requiring the update of N neurons and the storage of N(N−1)/2 synapses, can be accomplished by a hybrid Boltzmann Machine, requiring the update of N+P neurons but the storage of only NP synapses. In addition, the well known glass transition of the Hopfield network has a counterpart in the Boltzmann Machine: it corresponds to an optimum criterion for selecting the relative sizes of the hidden and visible layers, resolving the trade-off between flexibility and generality of the model. The low storage phase of the Hopfield model corresponds to few hidden units and hence a overly constrained RBM, while the spin-glass phase (too many hidden units) corresponds to unconstrained RBM prone to overfitting of the observed data.},
  keywords = {Boltzmann Machines,Hopfield networks,Statistical mechanics,ungelesen},
  file = {C\:\\Users\\simon\\Zotero\\storage\\FBU8CDS7\\On the equivalence of Hopfield networks and Boltzmann Machines.pdf;C\:\\Users\\simon\\Zotero\\storage\\NFHUABGB\\S0893608012001608.html}
}

@article{beichlMetropolisAlgorithm2000,
  title = {The {{Metropolis Algorithm}}},
  author = {Beichl, I. and Sullivan, F.},
  date = {2000-01},
  journaltitle = {Computing in Science \& Engineering},
  volume = {2},
  number = {1},
  pages = {65--69},
  issn = {1558-366X},
  doi = {10.1109/5992.814660},
  url = {https://ieeexplore.ieee.org/document/814660},
  urldate = {2024-02-27},
  abstract = {The Metropolis Algorithm has been the most successful and influential of all the members of the computational species that used to be called the "Monte Carlo method". Today, topics related to this algorithm constitute an entire field of computational science supported by a deep theory and having applications ranging from physical simulations to the foundations of computational complexity. Since the rejection method invention (J. von Neumann), it has been developed extensively and applied in a wide variety of settings. The Metropolis Algorithm can be formulated as an instance of the rejection method used for generating steps in a Markov chain.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {Computational complexity,Computational modeling,Distributed computing,Distribution functions,Hospitals,Monte Carlo methods,Physics computing,Probability distribution,Sampling methods},
  file = {C\:\\Users\\simon\\Zotero\\storage\\HWDNSHCW\\Beichl_Sullivan_2000_The Metropolis Algorithm.pdf;C\:\\Users\\simon\\Zotero\\storage\\ZTYBSUIQ\\814660.html}
}

@article{bohmNoiseinjectedAnalogIsing2022,
  title = {Noise-Injected Analog {{Ising}} Machines Enable Ultrafast Statistical Sampling and Machine Learning},
  author = {Böhm, Fabian and Alonso-Urquijo, Diego and Verschaffelt, Guy and Van der Sande, Guy},
  date = {2022-10-04},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {5847},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-33441-3},
  url = {https://www.nature.com/articles/s41467-022-33441-3},
  urldate = {2024-02-15},
  abstract = {Ising machines are a promising non-von-Neumann computational concept for neural network training and combinatorial optimization. However, while various neural networks can be implemented with Ising machines, their inability to perform fast statistical sampling makes them inefficient for training neural networks compared to digital computers. Here, we introduce a universal concept to achieve ultrafast statistical sampling with analog Ising machines by injecting noise. With an opto-electronic Ising machine, we experimentally demonstrate that this can be used for accurate sampling of Boltzmann distributions and for unsupervised training of neural networks, with equal accuracy as software-based training. Through simulations, we find that Ising machines can perform statistical sampling orders-of-magnitudes faster than software-based methods. This enables the use of Ising machines beyond combinatorial optimization and makes them into efficient tools for machine learning and other applications.},
  issue = {1},
  langid = {english},
  keywords = {Complex networks,Computer science,Information theory and computation,Optoelectronic devices and components,ungelesen},
  file = {C:\Users\simon\Zotero\storage\CUQQWVHL\Noise-injected analog Ising machines enable ultrafast statistical sampling and machine learning.pdf}
}

@article{bohmNoiseinjectedAnalogIsing2022a,
  title = {Noise-Injected Analog {{Ising}} Machines Enable Ultrafast Statistical Sampling and Machine Learning},
  author = {Böhm, Fabian and Alonso-Urquijo, Diego and Verschaffelt, Guy and Van der Sande, Guy},
  date = {2022-10-04},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {5847},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-33441-3},
  url = {https://www.nature.com/articles/s41467-022-33441-3},
  urldate = {2024-02-15},
  abstract = {Ising machines are a promising non-von-Neumann computational concept for neural network training and combinatorial optimization. However, while various neural networks can be implemented with Ising machines, their inability to perform fast statistical sampling makes them inefficient for training neural networks compared to digital computers. Here, we introduce a universal concept to achieve ultrafast statistical sampling with analog Ising machines by injecting noise. With an opto-electronic Ising machine, we experimentally demonstrate that this can be used for accurate sampling of Boltzmann distributions and for unsupervised training of neural networks, with equal accuracy as software-based training. Through simulations, we find that Ising machines can perform statistical sampling orders-of-magnitudes faster than software-based methods. This enables the use of Ising machines beyond combinatorial optimization and makes them into efficient tools for machine learning and other applications.},
  issue = {1},
  langid = {english},
  keywords = {Complex networks,Computer science,Information theory and computation,Optoelectronic devices and components,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\JC236XT8\Noise-injected analog Ising machines enable ultrafast statistical sampling and machine learning.pdf}
}

@online{caiHarnessingIntrinsicNoise2019,
  title = {Harnessing {{Intrinsic Noise}} in {{Memristor Hopfield Neural Networks}} for {{Combinatorial Optimization}}},
  author = {Cai, Fuxi and Kumar, Suhas and Van Vaerenbergh, Thomas and Liu, Rui and Li, Can and Yu, Shimeng and Xia, Qiangfei and Yang, J. Joshua and Beausoleil, Raymond and Lu, Wei and Strachan, John Paul},
  date = {2019-04-03},
  eprint = {1903.11194},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1903.11194},
  url = {http://arxiv.org/abs/1903.11194},
  urldate = {2024-02-15},
  abstract = {We describe a hybrid analog-digital computing approach to solve important combinatorial optimization problems that leverages memristors (two-terminal nonvolatile memories). While previous memristor accelerators have had to minimize analog noise effects, we show that our optimization solver harnesses such noise as a computing resource. Here we describe a memristor-Hopfield Neural Network (mem-HNN) with massively parallel operations performed in a dense crossbar array. We provide experimental demonstrations solving NP-hard max-cut problems directly in analog crossbar arrays, and supplement this with experimentally-grounded simulations to explore scalability with problem size, providing the success probabilities, time and energy to solution, and interactions with intrinsic analog noise. Compared to fully digital approaches, and present-day quantum and optical accelerators, we forecast the mem-HNN to have over four orders of magnitude higher solution throughput per power consumption. This suggests substantially improved performance and scalability compared to current quantum annealing approaches, while operating at room temperature and taking advantage of existing CMOS technology augmented with emerging analog non-volatile memristors.},
  pubstate = {preprint},
  keywords = {Computer Science - Emerging Technologies,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\FLZE6J2J\Cai et al. - 2019 - Harnessing Intrinsic Noise in Memristor Hopfield N.pdf}
}

@article{cichyDeepNeuralNetworks2019,
  title = {Deep {{Neural Networks}} as {{Scientific Models}}},
  author = {Cichy, Radoslaw M. and Kaiser, Daniel},
  date = {2019-04-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {4},
  eprint = {30795896},
  eprinttype = {pmid},
  pages = {305--317},
  publisher = {{Elsevier}},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2019.01.009},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(19)30034-8},
  urldate = {2024-02-23},
  langid = {english},
  keywords = {deep learning,explanation,exploration,gelesen,neural network,prediction,scientific model,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\4CWZNGI3\Cichy_Kaiser_2019_Deep Neural Networks as Scientific Models.pdf}
}

@article{clinchGrandChallenges2022,
  title = {Grand {{Challenges}}},
  author = {Clinch, Sarah and Intille, Stephen},
  date = {2022-07-01},
  journaltitle = {IEEE Pervasive Computing},
  volume = {21},
  number = {03},
  pages = {7--8},
  publisher = {{IEEE Computer Society}},
  issn = {1536-1268},
  doi = {10.1109/MPRV.2022.3198813},
  url = {https://www.computer.org/csdl/magazine/pc/2022/03/09903273/1GZo7D5rB8A},
  urldate = {2024-02-15},
  abstract = {The articles in this special section focus on new applications for pervasive computing.},
  langid = {english},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\VZMMDAK3\Grand Challenges.pdf}
}

@online{darioamodeiAICompute,
  title = {{{AI}} and Compute},
  author = {{Dario Amodei} and {Danny Hernandez}},
  url = {https://openai.com/research/ai-and-compute},
  urldate = {2024-02-15},
  abstract = {We’re releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore’s Law had a 2-year doubling period)[\^footnote-correction]. Since 2012, this metric has grown by more than 300,000x (a 2-year doubling period would yield only a 7x increase). Improvements in compute have been a key component of AI progress, so as long as this trend continues, it’s worth preparing for the implications of systems far outside today’s capabilities.},
  langid = {american},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\LW5E6CXF\ai-and-compute.html}
}

@incollection{dramschChapterOne702020,
  title = {Chapter {{One}} - 70 Years of Machine Learning in Geoscience in Review},
  booktitle = {Advances in {{Geophysics}}},
  author = {Dramsch, Jesper Sören},
  editor = {Moseley, Ben and Krischer, Lion},
  date = {2020-01-01},
  series = {Machine {{Learning}} in {{Geosciences}}},
  volume = {61},
  pages = {1--55},
  publisher = {{Elsevier}},
  doi = {10.1016/bs.agph.2020.08.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0065268720300054},
  urldate = {2024-02-28},
  abstract = {This review gives an overview of the development of machine learning in geoscience. A thorough analysis of the codevelopments of machine learning applications throughout the last 70 years relates the recent enthusiasm for machine learning to developments in geoscience. I explore the shift of kriging toward a mainstream machine learning method and the historic application of neural networks in geoscience, following the general trend of machine learning enthusiasm through the decades. Furthermore, this chapter explores the shift from mathematical fundamentals and knowledge in software development toward skills in model validation, applied statistics, and integrated subject matter expertise. The review is interspersed with code examples to complement the theoretical foundations and illustrate model validation and machine learning explainability for science. The scope of this review includes various shallow machine learning methods, e.g., decision trees, random forests, support-vector machines, and Gaussian processes, as well as, deep neural networks, including feed-forward neural networks, convolutional neural networks, recurrent neural networks, and generative adversarial networks. Regarding geoscience, the review has a bias toward geophysics but aims to strike a balance with geochemistry, geostatistics, and geology, however, excludes remote sensing, as this would exceed the scope. In general, I aim to provide context for the recent enthusiasm surrounding deep learning with respect to research, hardware, and software developments that enable successful application of shallow and deep machine learning in all disciplines of Earth science.},
  keywords = {Deep learning,Earth science,Geology,Geophysics,Geoscience,Kriging,Machine learning,Neural networks,Review},
  file = {C\:\\Users\\simon\\Zotero\\storage\\L5UC2QWA\\Dramsch_2020_Chapter One - 70 years of machine learning in geoscience in review.pdf;C\:\\Users\\simon\\Zotero\\storage\\DSKCIMKK\\S0065268720300054.html}
}

@online{duImplicitGenerationGeneralization2020,
  title = {Implicit {{Generation}} and {{Generalization}} in {{Energy-Based Models}}},
  author = {Du, Yilun and Mordatch, Igor},
  date = {2020-06-29},
  eprint = {1903.08689},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1903.08689},
  url = {http://arxiv.org/abs/1903.08689},
  urldate = {2024-02-23},
  abstract = {Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning,ungelesen,zitiert},
  file = {C\:\\Users\\simon\\Zotero\\storage\\RDJVNPK3\\Du_Mordatch_2020_Implicit Generation and Generalization in Energy-Based Models.pdf;C\:\\Users\\simon\\Zotero\\storage\\IRNZEZX7\\1903.html}
}

@online{duModelBasedPlanning2021,
  title = {Model {{Based Planning}} with {{Energy Based Models}}},
  author = {Du, Yilun and Lin, Toru and Mordatch, Igor},
  date = {2021-03-08},
  eprint = {1909.06878},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1909.06878},
  url = {http://arxiv.org/abs/1909.06878},
  urldate = {2024-02-19},
  abstract = {Model-based planning holds great promise for improving both sample efficiency and generalization in reinforcement learning (RL). We show that energy-based models (EBMs) are a promising class of models to use for model-based planning. EBMs naturally support inference of intermediate states given start and goal state distributions. We provide an online algorithm to train EBMs while interacting with the environment, and show that EBMs allow for significantly better online learning than corresponding feed-forward networks. We further show that EBMs support maximum entropy state inference and are able to generate diverse state space plans. We show that inference purely in state space - without planning actions - allows for better generalization to previously unseen obstacles in the environment and prevents the planner from exploiting the dynamics model by applying uncharacteristic action sequences. Finally, we show that online EBM training naturally leads to intentionally planned state exploration which performs significantly better than random exploration.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning,ungelesen,zitiert},
  file = {C\:\\Users\\simon\\Zotero\\storage\\VJYEAH2U\\Du et al_2021_Model Based Planning with Energy Based Models.pdf;C\:\\Users\\simon\\Zotero\\storage\\TVYW9SDL\\1909.html}
}

@article{durstewitzDeepNeuralNetworks2019,
  title = {Deep Neural Networks in Psychiatry},
  author = {Durstewitz, Daniel and Koppe, Georgia and Meyer-Lindenberg, Andreas},
  date = {2019-11},
  journaltitle = {Molecular Psychiatry},
  shortjournal = {Mol Psychiatry},
  volume = {24},
  number = {11},
  pages = {1583--1598},
  publisher = {{Nature Publishing Group}},
  issn = {1476-5578},
  doi = {10.1038/s41380-019-0365-9},
  url = {https://www.nature.com/articles/s41380-019-0365-9},
  urldate = {2024-02-23},
  abstract = {Machine and deep learning methods, today’s core of artificial intelligence, have been applied with increasing success and impact in many commercial and research settings. They are powerful tools for large scale data analysis, prediction and classification, especially in very data-rich environments (“big data”), and have started to find their way into medical applications. Here we will first give an overview of machine learning methods, with a focus on deep and recurrent neural networks, their relation to statistics, and the core principles behind them. We will then discuss and review directions along which (deep) neural networks can be, or already have been, applied in the context of psychiatry, and will try to delineate their future potential in this area. We will also comment on an emerging area that so far has been much less well explored: by embedding semantically interpretable computational models of brain dynamics or behavior into a statistical machine learning context, insights into dysfunction beyond mere prediction and classification may be gained. Especially this marriage of computational models with statistical inference may offer insights into neural and behavioral mechanisms that could open completely novel avenues for psychiatric treatment.},
  issue = {11},
  langid = {english},
  keywords = {gelesen,Neuroscience,Psychiatric disorders,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\RHCV9KWS\Durstewitz et al_2019_Deep neural networks in psychiatry.pdf}
}

@book{fahlmanMassivelyParallelArchitectures1983,
  title = {Massively {{Parallel Architectures}} for {{AI}}: {{NETL}}, {{Thistle}}, and {{Boltzmann Machines}}.},
  shorttitle = {Massively {{Parallel Architectures}} for {{AI}}},
  author = {Fahlman, Scott and Hinton, Geoffrey and Sejnowski, Terrence},
  date = {1983-01-01},
  journaltitle = {[No source information available]},
  pages = {113},
  abstract = {ABSTRACT It is becoming,increasingly apparent that some,aspects of intelligent behavior require enormous ,computational power,and,that some,sort of massively parallel computing architecture is the most plausible way to deliver sueh power. Parallelism, rather than raw speed of the computing,elements. seems,to be the way that the brain gets such jobs done. But even if the need for massive parallelism is admitted, there is still the question of what kind of parallel architecture best fits the needs of various A1 tasks. In this paper we will attempt to isolate a number(\$\#\$\#\$\#CommaToBean intelligent system must perform. We will describe several families of massively parallel computing architectures, and we will see which of these computational tasks can be handled by each of these families. In particular, we will describe a new architecture, which we call the Boltzmann machine, whose abilities appear to include a number,of tasks that are inefficient},
  pagetotal = {109},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\5QXANLI6\1983 - Massively Parallel Architectures for Al NETL, Thistle, and Boltzmann Machines.pdf}
}

@inproceedings{fischerIntroductionRestrictedBoltzmann2012,
  title = {An {{Introduction}} to {{Restricted Boltzmann Machines}}},
  booktitle = {Progress in {{Pattern Recognition}}, {{Image Analysis}}, {{Computer Vision}}, and {{Applications}}},
  author = {Fischer, Asja and Igel, Christian},
  editor = {Alvarez, Luis and Mejail, Marta and Gomez, Luis and Jacobo, Julio},
  date = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {14--36},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-33275-3_2},
  abstract = {Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided.},
  isbn = {978-3-642-33275-3},
  langid = {english},
  keywords = {gelesen,Gibbs Sampling,Markov Chain,Markov Chain Monte Carlo,Markov Chain Monte Carlo Method,Restrict Boltzmann Machine,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\6E9KS4GX\Fischer_Igel_2012_An Introduction to Restricted Boltzmann Machines.pdf}
}

@article{gawlikowskiSurveyUncertaintyDeep2023,
  title = {A Survey of Uncertainty in Deep Neural Networks},
  author = {Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and Shahzad, Muhammad and Yang, Wen and Bamler, Richard and Zhu, Xiao Xiang},
  date = {2023-10-01},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {56},
  number = {1},
  pages = {1513--1589},
  issn = {1573-7462},
  doi = {10.1007/s10462-023-10562-9},
  url = {https://doi.org/10.1007/s10462-023-10562-9},
  urldate = {2024-02-23},
  abstract = {Over the last decade, neural networks have reached almost every field of science and become a crucial part of various real world applications. Due to the increasing spread, confidence in neural network predictions has become more and more important. However, basic neural networks do not deliver certainty estimates or suffer from over- or under-confidence, i.e. are badly calibrated. To overcome this, many researchers have been working on understanding and quantifying uncertainty in a neural network’s prediction. As a result, different types and sources of uncertainty have been identified and various approaches to measure and quantify uncertainty in neural networks have been proposed. This work gives a comprehensive overview of uncertainty estimation in neural networks, reviews recent advances in the field, highlights current challenges, and identifies potential research opportunities. It is intended to give anyone interested in uncertainty estimation in neural networks a broad overview and introduction, without presupposing prior knowledge in this field. For that, a comprehensive introduction to the most crucial sources of uncertainty is given and their separation into reducible model uncertainty and irreducible data uncertainty is presented. The modeling of these uncertainties based on deterministic neural networks, Bayesian neural networks (BNNs), ensemble of neural networks, and test-time data augmentation approaches is introduced and different branches of these fields as well as the latest developments are discussed. For a practical application, we discuss different measures of uncertainty, approaches for calibrating neural networks, and give an overview of existing baselines and available implementations. Different examples from the wide spectrum of challenges in the fields of medical image analysis, robotics, and earth observation give an idea of the needs and challenges regarding uncertainties in the practical applications of neural networks. Additionally, the practical limitations of uncertainty quantification methods in neural networks for mission- and safety-critical real world applications are discussed and an outlook on the next steps towards a broader usage of such methods is given.},
  langid = {english},
  keywords = {Bayesian deep neural networks,Calibration,Ensembles,gelesen,Test-time augmentation,Uncertainty,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\39QQTBNQ\Gawlikowski et al_2023_A survey of uncertainty in deep neural networks.pdf}
}

@incollection{gerthKuenstlicheIntelligenzZwischen2023,
  title = {Künstliche Intelligenz zwischen Utopie und Realität: Aktuelle und zukünftige Entwicklungen von KI am Beispiel von Human-Machine-Interaction, Blockchain, Green Tech und Mobilität},
  shorttitle = {Künstliche Intelligenz zwischen Utopie und Realität},
  booktitle = {Entrepreneurship der Zukunft: Voraussetzung, Implementierung und Anwendung von Künstlicher Intelligenz im Rahmen datenbasierter Geschäftsmodelle},
  author = {Gerth, Sebastian and Heim, Lars},
  editor = {Heim, Lars and Gerth, Sebastian},
  date = {2023},
  pages = {421--458},
  publisher = {{Springer Fachmedien}},
  location = {{Wiesbaden}},
  doi = {10.1007/978-3-658-42060-4_17},
  url = {https://doi.org/10.1007/978-3-658-42060-4_17},
  urldate = {2024-02-15},
  abstract = {Der vorliegende Beitrag wagt auf der Basis vergangener und aktueller Entwicklungen von Künstlicher Intelligenz (KI) einen Ausblick auf zukünftige Leistungen von KI in spezifischen Gebieten. Es wird argumentiert, dass KI ein eigenständiger Megatrend mit zahlreichen Auswirkungen auf die Gesellschaft und v.~a. auch auf die Wirtschaft in unterschiedlichen Bereichen und Ebenen mit zahlreichen Anknüpfungspunkten diverser Geschäftsbereiche und -modelle ist. Es wird gezeigt, dass die wesentliche Stärke von KI einerseits die exakte Analyse und das zur Erreichung von vordefinierten Zielen optimale Inbeziehungsetzen spezifischer Parameter ist. Andererseits vermag KI bestimmte Prognosen zu fundieren. Aus diesem Grund wird der Chatbot ChatGPT des Entwicklers OpenAI zur Zukunft von KI befragt – eine KI trifft also Vorhersagen über KI. Dieser vielversprechende Ansatz wird dennoch kritisch reflektiert und der Beitrag mit einer Beleuchtung von starker und schwacher KI im Unternehmenskontext sowie einer prüfenden Deskription der bevorstehenden KI-Evolution abgeschlossen.},
  isbn = {978-3-658-42060-4},
  langid = {ngerman},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\F8N7CZ5G\Künstliche Intelligenz zwischen Utopie und Realitä.pdf}
}

@inproceedings{gustafssonEnergyBasedModelsDeep2020,
  title = {Energy-{{Based Models}} for {{Deep Probabilistic Regression}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Gustafsson, Fredrik K. and Danelljan, Martin and Bhat, Goutam and Schön, Thomas B.},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {325--343},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58565-5_20},
  abstract = {While deep learning-based classification is generally tackled using standardized approaches, a wide variety of techniques are employed for regression. In computer vision, one particularly popular such technique is that of confidence-based regression, which entails predicting a confidence value for each input-target pair (x,~y). While this approach has demonstrated impressive results, it requires important task-dependent design choices, and the predicted confidences lack a natural probabilistic meaning. We address these issues by proposing a general and conceptually simple regression method with a clear probabilistic interpretation. In our proposed approach, we create an energy-based model of the conditional target density p(y|x), using a deep neural network to predict the un-normalized density from (x,~y). This model of p(y|x) is trained by directly minimizing the associated negative log-likelihood, approximated using Monte Carlo sampling. We perform comprehensive experiments on four computer vision regression tasks. Our approach outperforms direct regression, as well as other probabilistic and confidence-based methods. Notably, our model achieves a \$\$2.2\textbackslash\%\$\$2.2\%AP improvement over Faster-RCNN for object detection on the COCO dataset, and sets a new state-of-the-art on visual tracking when applied for bounding box estimation. In contrast to confidence-based methods, our approach is also shown to be directly applicable to more general tasks such as age and head-pose estimation. Code is available at https://github.com/fregu856/ebms\_regression.},
  isbn = {978-3-030-58565-5},
  langid = {english},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\NAN4GFF3\Gustafsson et al_2020_Energy-Based Models for Deep Probabilistic Regression.pdf}
}

@online{helmenstineHowManyAtoms2022,
  title = {How {{Many Atoms Are}} in the {{World}}?},
  author = {Helmenstine, Anne},
  date = {2022-05-10T19:00:47+00:00},
  url = {https://sciencenotes.org/how-many-atoms-are-in-the-world/},
  urldate = {2024-02-21},
  abstract = {Learn how many atoms are in the world and how to perform the calculation. Compare this number to the number of way to order playing cards.},
  langid = {american},
  organization = {{Science Notes and Projects}},
  keywords = {gelesen,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\I5MHB32W\how-many-atoms-are-in-the-world.html}
}

@online{hernandezMeasuringAlgorithmicEfficiency2020,
  title = {Measuring the {{Algorithmic Efficiency}} of {{Neural Networks}}},
  author = {Hernandez, Danny and Brown, Tom B.},
  date = {2020-05-08},
  eprint = {2005.04305},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2005.04305},
  url = {http://arxiv.org/abs/2005.04305},
  urldate = {2024-02-15},
  abstract = {Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning,ungelesen},
  file = {C:\Users\simon\Zotero\storage\S692LSEN\Measuring the Algorithmic Efficiency of Neural Networks.pdf}
}

@book{hintemannDataCenters20212022,
  title = {Data Centers 2021: {{Data}} Center Boom in {{Germany}} Continues - {{Cloud}} Computing Drives the Growth of the Data Center Industry and Its Energy Consumption},
  shorttitle = {Data Centers 2021},
  author = {Hintemann, Ralph and Hinterholzer, Simon},
  date = {2022-08-05},
  doi = {10.13140/RG.2.2.31826.43207},
  abstract = {The energy consumption of data centers continues to increase. At 17 billion kWh, data centers consumed 6.5 \% more electricity in 2021 than in 2020. The main reason for the growth in energy consumption is the expansion of cloud data centers in Germany and the associated increase in the number of large data centers. However, traditional data centers operated by companies themselves also continue to have a high share of data center capacities in Germany.},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\8S6LNT75\Data centers 2021 Data center boom in Germany continues.pdf}
}

@incollection{hintonPracticalGuideTraining2012,
  title = {A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {Hinton, Geoffrey E.},
  editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  date = {2012},
  volume = {7700},
  pages = {599--619},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_32},
  url = {http://link.springer.com/10.1007/978-3-642-35289-8_32},
  urldate = {2024-02-15},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\HGALEWG5\Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf}
}

@incollection{hintonPracticalGuideTraining2012a,
  title = {A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {Hinton, Geoffrey E.},
  editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  date = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {599--619},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_32},
  url = {https://doi.org/10.1007/978-3-642-35289-8_32},
  urldate = {2024-02-15},
  abstract = {Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.},
  isbn = {978-3-642-35289-8},
  langid = {english},
  keywords = {gelesen,Hide Unit,Learning Rate,Reconstruction Error,Restrict Boltzmann Machine,Training Case,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\VZ5ZDZTH\Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf}
}

@online{hizzaniMemristorbasedHardwareAlgorithms2023,
  title = {Memristor-Based Hardware and Algorithms for Higher-Order {{Hopfield}} Optimization Solver Outperforming Quadratic {{Ising}} Machines},
  author = {Hizzani, Mohammad and Heittmann, Arne and Hutchinson, George and Dobrynin, Dmitrii and Van Vaerenbergh, Thomas and Bhattacharya, Tinish and Renaudineau, Adrien and Strukov, Dmitri and Strachan, John Paul},
  date = {2023-11-02},
  eprint = {2311.01171},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.01171},
  url = {http://arxiv.org/abs/2311.01171},
  urldate = {2024-02-15},
  abstract = {Ising solvers offer a promising physics-based approach to tackle the challenging class of combinatorial optimization problems. However, typical solvers operate in a quadratic energy space, having only pair-wise coupling elements which already dominate area and energy. We show that such quadratization can cause severe problems: increased dimensionality, a rugged search landscape, and misalignment with the original objective function. Here, we design and quantify a higher-order Hopfield optimization solver, with 28nm CMOS technology and memristive couplings for lower area and energy computations. We combine algorithmic and circuit analysis to show quantitative advantages over quadratic Ising Machines (IM)s, yielding 48x and 72x reduction in time-to-solution (TTS) and energy-to-solution (ETS) respectively for Boolean satisfiability problems of 150 variables, with favorable scaling.},
  pubstate = {preprint},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Hardware Architecture,ungelesen},
  file = {C:\Users\simon\Zotero\storage\UWNBRWXG\Hizzani et al. - 2023 - Memristor-based hardware and algorithms for higher.pdf}
}

@article{hopfieldNeuralNetworksPhysical1982,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities.},
  author = {Hopfield, J J},
  date = {1982-04},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {79},
  number = {8},
  pages = {2554--2558},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.79.8.2554},
  url = {https://www.pnas.org/doi/10.1073/pnas.79.8.2554},
  urldate = {2024-02-19},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\BF4BJIPP\Hopfield_1982_Neural networks and physical systems with emergent collective computational.pdf}
}

@incollection{huberIntroductionMetropolisRosenbluth1997,
  title = {Introduction to {{Metropolis}}, {{Rosenbluth}}, {{Rosenbluth}}, {{Teller}}, and {{Teller}} (1953) {{Equations}} of {{State Calculations}} by {{Fast Computing Machines}}. {{J}}. {{Chem}}. {{Phys}}.,21, 1087–1092. and {{Geman}} and {{Geman}} (1984) {{Stochastic Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}. {{IEEE Trans}}. {{Pattern Anal}}. {{Machine Intelligence}},6, 721–741.},
  booktitle = {Breakthroughs in {{Statistics}}},
  author = {Huber, Peter J.},
  editor = {Kotz, Samuel and Johnson, Norman L.},
  date = {1997},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {123--139},
  publisher = {{Springer}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4612-0667-5_6},
  url = {https://doi.org/10.1007/978-1-4612-0667-5_6},
  urldate = {2024-02-27},
  abstract = {The breakthrough to be discussed here occurred in two steps, separated in time by over three decades. The first, conceptually decisive step, but lacking direct relevance to mainstream statistics, was the invention of Markov Chain Monte Carlo methods by Metropolis et al. (1953). The second step, decisive for applications in statistics, occurred when Geman and Geman (1984) wrestled the seminal idea of Metropolis et al. from its statistical mechanics surroundings, modified it, and applied it to Bayesian modeling and the computation of posterior distributions in otherwise intractable situations.},
  isbn = {978-1-4612-0667-5},
  langid = {english},
  keywords = {Configuration Space,Markov Chain Monte Carlo Method,Markov Random Fields,Radial Distribution Function,Virial Coefficient}
}

@article{huembeliPhysicsEnergybasedModels2022,
  title = {The Physics of Energy-Based Models},
  author = {Huembeli, Patrick and Arrazola, Juan Miguel and Killoran, Nathan and Mohseni, Masoud and Wittek, Peter},
  date = {2022-01-06},
  journaltitle = {Quantum Machine Intelligence},
  shortjournal = {Quantum Mach. Intell.},
  volume = {4},
  number = {1},
  pages = {1},
  issn = {2524-4914},
  doi = {10.1007/s42484-021-00057-7},
  url = {https://doi.org/10.1007/s42484-021-00057-7},
  urldate = {2024-02-19},
  abstract = {Energy-based models (EBMs) are experiencing a resurgence of interest in both the physics community and the machine learning community. This article provides an intuitive introduction to EBMs, without requiring any background in machine learning, connecting elementary concepts from physics with basic concepts and tools in generative models, and finally giving a perspective where current research in the field is heading. This article, in its original form, was written as an online lecture note in HTML and Javascript and contains interactive graphics. We recommend the reader to also visit the interactive version.},
  langid = {english},
  keywords = {Energy-based models,gelesen,Gibbs sampling,Machine learning,Spin glasses,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\5SE9BGQ4\The physics of energy-based models.pdf}
}

@inproceedings{larochelleClassificationUsingDiscriminative2008,
  title = {Classification Using Discriminative Restricted {{Boltzmann}} Machines},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  author = {Larochelle, Hugo and Bengio, Yoshua},
  date = {2008-07-05},
  series = {{{ICML}} '08},
  pages = {536--543},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1390156.1390224},
  url = {https://dl.acm.org/doi/10.1145/1390156.1390224},
  urldate = {2024-02-22},
  abstract = {Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.},
  isbn = {978-1-60558-205-4},
  keywords = {gelesen,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\EPN7UF5H\Larochelle_Bengio_2008_Classification using discriminative restricted Boltzmann machines.pdf}
}

@article{larochelleLearningAlgorithmsClassification2012,
  title = {Learning Algorithms for the Classification Restricted {{Boltzmann}} Machine},
  author = {Larochelle, Hugo and Mandel, Michael and Pascanu, Razvan and Bengio, Y.},
  date = {2012-03-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {The Journal of Machine Learning Research},
  volume = {13},
  pages = {643--669},
  abstract = {Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely semi-supervised and multitask learning.},
  keywords = {gelesen,ungelesen},
  file = {C:\Users\simon\Zotero\storage\BE788SPH\Larochelle et al_2012_Learning algorithms for the classification restricted Boltzmann machine.pdf}
}

@article{liTemperatureBasedRestricted2016,
  title = {Temperature Based {{Restricted Boltzmann Machines}}},
  author = {Li, Guoqi and Deng, Lei and Xu, Yi and Wen, Changyun and Wang, Wei and Pei, Jing and Shi, Luping},
  date = {2016-01-13},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {6},
  number = {1},
  pages = {19133},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep19133},
  url = {https://www.nature.com/articles/srep19133},
  urldate = {2024-02-27},
  abstract = {Restricted Boltzmann machines (RBMs), which apply graphical models to learning probability distribution over a set of inputs, have attracted much attention recently since being proposed as building blocks of multi-layer learning systems called deep belief networks (DBNs). Note that temperature is a key factor of the Boltzmann distribution that RBMs originate from. However, none of existing schemes have considered the impact of temperature in the graphical model of DBNs. In this work, we propose temperature based restricted Boltzmann machines (TRBMs) which reveals that temperature is an essential parameter controlling the selectivity of the firing neurons in the hidden layers. We theoretically prove that the effect of temperature can be adjusted by setting the parameter of the sharpness of the logistic function in the proposed TRBMs. The performance of RBMs can be improved by adjusting the temperature parameter of TRBMs. This work provides a comprehensive insights into the deep belief networks and deep learning architectures from a physical point of view.},
  issue = {1},
  langid = {english},
  keywords = {Applied physics,Computational science},
  file = {C:\Users\simon\Zotero\storage\2EB9SJC3\Li et al_2016_Temperature based Restricted Boltzmann Machines.pdf}
}

@online{luccioniPowerHungryProcessing2023,
  title = {Power {{Hungry Processing}}: {{Watts Driving}} the {{Cost}} of {{AI Deployment}}?},
  shorttitle = {Power {{Hungry Processing}}},
  author = {Luccioni, Alexandra Sasha and Jernite, Yacine and Strubell, Emma},
  date = {2023-11-28},
  eprint = {2311.16863},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.16863},
  url = {http://arxiv.org/abs/2311.16863},
  urldate = {2024-02-15},
  abstract = {Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of "generality" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\66VJIJMH\Power Hungry Processing Watts Driving the Cost of AI Deployment.pdf}
}

@book{mackayInformationTheoryInference2003,
  title = {Information {{Theory}}, {{Inference}} and {{Learning Algorithms}}},
  author = {MacKay, David J. C.},
  date = {2003-09-25},
  eprint = {AKuMj4PN_EMC},
  eprinttype = {googlebooks},
  publisher = {{Cambridge University Press}},
  abstract = {Information theory and inference, often taught separately, are here united in one entertaining textbook. These topics lie at the heart of many exciting areas of contemporary science and engineering - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics, and cryptography. This textbook introduces theory in tandem with applications. Information theory is taught alongside practical communication systems, such as arithmetic coding for data compression and sparse-graph codes for error-correction. A toolbox of inference techniques, including message-passing algorithms, Monte Carlo methods, and variational approximations, are developed alongside applications of these tools to clustering, convolutional codes, independent component analysis, and neural networks. The final part of the book describes the state of the art in error-correcting codes, including low-density parity-check codes, turbo codes, and digital fountain codes -- the twenty-first century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, David MacKay's groundbreaking book is ideal for self-learning and for undergraduate or graduate courses. Interludes on crosswords, evolution, and sex provide entertainment along the way. In sum, this is a textbook on information, communication, and coding for a new generation of students, and an unparalleled entry point into these subjects for professionals in areas as diverse as computational biology, financial engineering, and machine learning.},
  isbn = {978-0-521-64298-9},
  langid = {english},
  pagetotal = {694},
  keywords = {Computers / Artificial Intelligence / Computer Vision & Pattern Recognition,Computers / Computer Science,Computers / Data Science / Data Modeling & Design,Computers / Data Science / Neural Networks,Computers / Information Theory,Mathematics / Algebra / General,Philosophy / Logic,Science / Physics / General,Technology & Engineering / Electronics / General}
}

@article{mallComprehensiveReviewDeep2023,
  title = {A Comprehensive Review of Deep Neural Networks for Medical Image Processing: {{Recent}} Developments and Future Opportunities},
  shorttitle = {A Comprehensive Review of Deep Neural Networks for Medical Image Processing},
  author = {Mall, Pawan Kumar and Singh, Pradeep Kumar and Srivastav, Swapnita and Narayan, Vipul and Paprzycki, Marcin and Jaworska, Tatiana and Ganzha, Maria},
  date = {2023-12-01},
  journaltitle = {Healthcare Analytics},
  shortjournal = {Healthcare Analytics},
  volume = {4},
  pages = {100216},
  issn = {2772-4425},
  doi = {10.1016/j.health.2023.100216},
  url = {https://www.sciencedirect.com/science/article/pii/S2772442523000837},
  urldate = {2024-02-23},
  abstract = {Artificial Intelligence (AI) solutions have been widely used in healthcare, and recent developments in deep neural networks have contributed to significant advances in medical image processing. Much ongoing research is aimed at helping medical practitioners by providing automated systems to analyze images and diagnose acute diseases, such as brain tumors, bone cancer, breast cancer, bone fracture, and many others. This comprehensive review delivers an overview of recent advances in medical imaging using deep neural networks. In addition to the comprehensive literature review, a summary of openly available data sources and future research directions are outlined.},
  keywords = {Artificial intelligence,Deep neural networks,gelesen,Machine learning,Medical imaging diagnostic analytics,Predictive analytics,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\ZH4THS93\S2772442523000837.html}
}

@article{marinoDeepNeuralNetworks2023,
  title = {Deep Neural Networks Compression: {{A}} Comparative Survey and Choice Recommendations},
  shorttitle = {Deep Neural Networks Compression},
  author = {Marinó, Giosué Cataldo and Petrini, Alessandro and Malchiodi, Dario and Frasca, Marco},
  date = {2023-02-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {520},
  pages = {152--170},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.11.072},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231222014643},
  urldate = {2024-02-23},
  abstract = {The state-of-the-art performance for several real-world problems is currently reached by deep and, in particular, convolutional neural networks (CNN). Such learning models exploit recent results in the field of deep learning, leading to highly performing, yet very large neural networks with typically millions to billions of parameters. As a result, such models are often redundant and excessively oversized, with a detrimental effect on the environment in terms of unnecessary energy consumption and a limitation to their deployment on low-resource devices. The necessity for compression techniques able to reduce the number of model parameters and their resource demand is thereby increasingly felt by the research community. In this paper we propose the first extensive comparison, to the best of our knowledge, of the main lossy and structure-preserving approaches to compress pre-trained CNNs, applicable in principle to any existing model. Our study is intended to provide a first and preliminary guidance to choose the most suitable compression technique when there is the need to reduce the occupancy of pre-trained models. Both convolutional and fully-connected layers are included in the analysis. Our experiments involved two pre-trained state-of-the-art CNNs (proposed to solve classification or regression problems) and five benchmarks, and gave rise to important insights about the applicability and performance of such techniques w.r.t.the type of layer to be compressed and the category of problem tackled.},
  keywords = {CNN compression,Connection pruning,Huffman coding,Succinct Deep Neural Networks,Weight quantization,Weight sharing},
  file = {C:\Users\simon\Zotero\storage\TBJBQ945\S0925231222014643.html}
}

@article{metropolisEquationStateCalculations1953,
  title = {Equation of {{State Calculations}} by {{Fast Computing Machines}}},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  date = {1953-06-01},
  journaltitle = {The Journal of Chemical Physics},
  shortjournal = {The Journal of Chemical Physics},
  volume = {21},
  number = {6},
  pages = {1087--1092},
  issn = {0021-9606},
  doi = {10.1063/1.1699114},
  url = {https://doi.org/10.1063/1.1699114},
  urldate = {2024-02-27},
  abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.}
}

@article{mocanuTopologicalInsightRestricted2016,
  title = {A Topological Insight into Restricted {{Boltzmann}} Machines},
  author = {Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
  date = {2016-09-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {104},
  number = {2},
  pages = {243--270},
  issn = {1573-0565},
  doi = {10.1007/s10994-016-5570-z},
  url = {https://doi.org/10.1007/s10994-016-5570-z},
  urldate = {2024-02-22},
  abstract = {Restricted Boltzmann Machines (RBMs) and models derived from them have been successfully used as basic building blocks in deep artificial neural networks for automatic features extraction, unsupervised weights initialization, but also as density estimators. Thus, their generative and discriminative capabilities, but also their computational time are instrumental to a wide range of applications. Our main contribution is to look at RBMs from a topological perspective, bringing insights from network science. Firstly, here we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which naturally have a small-world topology. Secondly, we demonstrate both on synthetic and real-world datasets that by constraining RBMs and GRBMs to a scale-free topology (while still considering local neighborhoods and data distribution), we reduce the number of weights that need to be computed by a few orders of magnitude, at virtually no loss in generative performance. Thirdly, we show that, for a fixed number of weights, our proposed sparse models (which by design have a higher number of hidden neurons) achieve better generative capabilities than standard fully connected RBMs and GRBMs (which by design have a smaller number of hidden neurons), at no additional computational costs.},
  langid = {english},
  keywords = {Complex networks,Deep learning,Scale-free networks,Small-world networks,Sparse restricted Boltzmann machines},
  file = {C:\Users\simon\Zotero\storage\524Q4ZX2\Mocanu et al_2016_A topological insight into restricted Boltzmann machines.pdf}
}

@online{mohseniIsingMachinesHardware2022,
  title = {Ising Machines as Hardware Solvers of Combinatorial Optimization Problems},
  author = {Mohseni, Naeimeh and McMahon, Peter L. and Byrnes, Tim},
  date = {2022-04-01},
  eprint = {2204.00276},
  eprinttype = {arxiv},
  eprintclass = {physics, physics:quant-ph},
  doi = {10.48550/arXiv.2204.00276},
  url = {http://arxiv.org/abs/2204.00276},
  urldate = {2024-02-15},
  abstract = {Ising machines are hardware solvers which aim to find the absolute or approximate ground states of the Ising model. The Ising model is of fundamental computational interest because it is possible to formulate any problem in the complexity class NP as an Ising problem with only polynomial overhead. A scalable Ising machine that outperforms existing standard digital computers could have a huge impact for practical applications for a wide variety of optimization problems. In this review, we survey the current status of various approaches to constructing Ising machines and explain their underlying operational principles. The types of Ising machines considered here include classical thermal annealers based on technologies such as spintronics, optics, memristors, and digital hardware accelerators; dynamical-systems solvers implemented with optics and electronics; and superconducting-circuit quantum annealers. We compare and contrast their performance using standard metrics such as the ground-state success probability and time-to-solution, give their scaling relations with problem size, and discuss their strengths and weaknesses.},
  pubstate = {preprint},
  keywords = {Physics - Applied Physics,Quantum Physics,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\VWIC5SI6\Ising machines Hardware solvers for combinatorial.pdf}
}

@online{montufarRestrictedBoltzmannMachines2018,
  title = {Restricted {{Boltzmann Machines}}: {{Introduction}} and {{Review}}},
  shorttitle = {Restricted {{Boltzmann Machines}}},
  author = {Montufar, Guido},
  date = {2018-06-19},
  eprint = {1806.07066},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.1806.07066},
  url = {http://arxiv.org/abs/1806.07066},
  urldate = {2024-02-15},
  abstract = {The restricted Boltzmann machine is a network of stochastic units with undirected interactions between pairs of visible and hidden units. This model was popularized as a building block of deep learning architectures and has continued to play an important role in applied and theoretical machine learning. Restricted Boltzmann machines carry a rich structure, with connections to geometry, applied algebra, probability, statistics, machine learning, and other areas. The analysis of these models is attractive in its own right and also as a platform to combine and generalize mathematical tools for graphical models with hidden variables. This article gives an introduction to the mathematical analysis of restricted Boltzmann machines, reviews recent results on the geometry of the sets of probability distributions representable by these models, and suggests a few directions for further investigation.},
  pubstate = {preprint},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Machine Learning,ungelesen},
  file = {C:\Users\simon\Zotero\storage\3DLWASIX\Montufar - 2018 - Restricted Boltzmann Machines Introduction and Re.pdf}
}

@book{nazmbojnordiMemristiveBoltzmannMachine2016,
  title = {Memristive {{Boltzmann}} Machine: {{A}} Hardware Accelerator for Combinatorial Optimization and Deep Learning},
  shorttitle = {Memristive {{Boltzmann}} Machine},
  author = {Nazm Bojnordi, Mahdi and Ipek, Engin},
  date = {2016-03-01},
  pages = {13},
  doi = {10.1109/HPCA.2016.7446049},
  pagetotal = {1},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\A9DK674U\Memristive Boltzmann machine A hardware accelerator for combinatorial optimization and deep learning.pdf}
}

@inproceedings{NIPS2000_1f1baa5b,
  title = {Recognizing Hand-Written Digits Using Hierarchical Products of Experts},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mayraz, Guy and Hinton, Geoffrey E},
  editor = {Leen, T. and Dietterich, T. and Tresp, V.},
  date = {2000},
  volume = {13},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\J6D4HNWV\Mayraz und Hinton - 2000 - Recognizing hand-written digits using hierarchical.pdf}
}

@book{pandianIntelligentComputingOptimization,
  title = {Intelligent {{Computing}} and {{Optimization}}},
  author = {Pandian, Vasant and Vladimir Panchenko},
  url = {https://link.springer.com/book/9783031508868},
  urldate = {2024-02-04},
  abstract = {This book covers the 7th edition of International Conference on Intelligent Computing and Optimization took place at Baitong Hotel \& Resort},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\WPJ3SWAN\9783031508868.html}
}

@online{patronOptimalRelaxationRate2024,
  title = {On the Optimal Relaxation Rate for the {{Metropolis}} Algorithm in One Dimension},
  author = {Patrón, A. and Chepelianskii, A. D. and Prados, A. and Trizac, E.},
  date = {2024-02-17},
  eprint = {2402.11267},
  eprinttype = {arxiv},
  eprintclass = {cond-mat, physics:math-ph},
  doi = {10.48550/arXiv.2402.11267},
  url = {http://arxiv.org/abs/2402.11267},
  urldate = {2024-02-27},
  abstract = {We study the relaxation of the Metropolis Monte Carlo algorithm corresponding to a single particle trapped in a one-dimensional confining potential, with even jump distributions that ensure that the dynamics verifies detailed balance. Previous work suggested that, for smooth jump distributions, the fastest relaxation rate is obtained as a result of the competition between diffusive and rejection-dominated dynamics. In this work, we show that a new regime comes into play for two-peaked jump distributions, where the relaxation dynamics is neither dominated by diffusion nor rejection: the eigenmodes adopt an oscillatory form, reminiscent of charge density waves (CDW) -- thus we term this new regime the CDW regime. Using a combination of numerical and analytical techniques, the parameter regions corresponding to diffusion, rejection, and CDW are characterised, as well as the transition lines between them -- i.e. a phase diagram is built. The optimal relaxation rate is located at the triple point of phase coexistence, where the transition lines (diffusive-rejection, diffusive-CDW, and CDW-rejection) intersect. Our theoretical framework is checked versus the numerical diagonalisation of the master equation. We also briefly discuss more sophisticated attempts at optimising the relaxation rate to equilibrium.},
  pubstate = {preprint},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematical Physics},
  file = {C\:\\Users\\simon\\Zotero\\storage\\LVWVYVHT\\Patrón et al_2024_On the optimal relaxation rate for the Metropolis algorithm in one dimension.pdf;C\:\\Users\\simon\\Zotero\\storage\\AX9IPXGS\\2402.html}
}

@online{pedrettiXTIMEInmemoryEngine2024,
  title = {X-{{TIME}}: {{An}} in-Memory Engine for Accelerating Machine Learning on Tabular Data with {{CAMs}}},
  shorttitle = {X-{{TIME}}},
  author = {Pedretti, Giacomo and Moon, John and Bruel, Pedro and Serebryakov, Sergey and Roth, Ron M. and Buonanno, Luca and Gajjar, Archit and Ziegler, Tobias and Xu, Cong and Foltin, Martin and Faraboschi, Paolo and Ignowski, Jim and Graves, Catherine E.},
  date = {2024-02-02},
  eprint = {2304.01285},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.01285},
  url = {http://arxiv.org/abs/2304.01285},
  urldate = {2024-02-15},
  abstract = {Structured, or tabular, data is the most common format in data science. While deep learning models have proven formidable in learning from unstructured data such as images or speech, they are less accurate than simpler approaches when learning from tabular data. In contrast, modern tree-based Machine Learning (ML) models shine in extracting relevant information from structured data. An essential requirement in data science is to reduce model inference latency in cases where, for example, models are used in a closed loop with simulation to accelerate scientific discovery. However, the hardware acceleration community has mostly focused on deep neural networks and largely ignored other forms of machine learning. Previous work has described the use of an analog content addressable memory (CAM) component for efficiently mapping random forests. In this work, we focus on an overall analog-digital architecture implementing a novel increased precision analog CAM and a programmable network on chip allowing the inference of state-of-the-art tree-based ML models, such as XGBoost and CatBoost. Results evaluated in a single chip at 16nm technology show 119x lower latency at 9740x higher throughput compared with a state-of-the-art GPU, with a 19W peak power consumption.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,ungelesen},
  file = {C:\Users\simon\Zotero\storage\AG7EV9DE\Pedretti et al. - 2024 - X-TIME An in-memory engine for accelerating machi.pdf}
}

@online{ramsauerHopfieldNetworksAll2021,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2021-04-28},
  eprint = {2008.02217},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2008.02217},
  url = {http://arxiv.org/abs/2008.02217},
  urldate = {2024-02-28},
  abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\simon\\Zotero\\storage\\TZQL3XBP\\Ramsauer et al_2021_Hopfield Networks is All You Need.pdf;C\:\\Users\\simon\\Zotero\\storage\\U2TRQQSW\\2008.html}
}

@online{robertMetropolisHastingsAlgorithm2016,
  title = {The {{Metropolis-Hastings}} Algorithm},
  author = {Robert, Christian P.},
  date = {2016-01-27},
  eprint = {1504.01896},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1504.01896},
  url = {http://arxiv.org/abs/1504.01896},
  urldate = {2024-02-27},
  abstract = {This short note is a self-contained and basic introduction to the Metropolis-Hastings algorithm, this ubiquitous tool used for producing dependent simulations from an arbitrary distribution. The document illustrates the principles of the methodology on simple examples with R codes and provides references to the recent extensions of the method.},
  pubstate = {preprint},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\simon\\Zotero\\storage\\DPSLD9DJ\\Robert_2016_The Metropolis-Hastings algorithm.pdf;C\:\\Users\\simon\\Zotero\\storage\\KKAV6T6V\\1504.html}
}

@article{rosenthalOptimalProposalDistributions2009,
  title = {Optimal {{Proposal Distributions}} and {{Adaptive MCMC}}},
  author = {Rosenthal, S.},
  date = {2009-09-29},
  journaltitle = {Handbook of Markov Chain Monte Carlo},
  shortjournal = {Handbook of Markov Chain Monte Carlo},
  abstract = {We review recent work concerning optimal proposal scalings for Metropolis-Hastings MCMC algorithms, and adaptive MCMC algorithms for trying to improve the algorithm on the y.}
}

@inproceedings{salakhutdinovDeepBoltzmannMachines2009,
  title = {Deep {{Boltzmann Machines}}},
  booktitle = {Proceedings of the {{Twelth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  date = {2009-04-15},
  pages = {448--455},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v5/salakhutdinov09a.html},
  urldate = {2024-02-16},
  abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\V8LJQ6Z2\Deep Boltzmann Machines.pdf}
}

@article{sarkerMachineLearningAlgorithms2021,
  title = {Machine {{Learning}}: {{Algorithms}}, {{Real-World Applications}} and {{Research Directions}}},
  shorttitle = {Machine {{Learning}}},
  author = {Sarker, Iqbal H.},
  date = {2021-03-22},
  journaltitle = {SN Computer Science},
  shortjournal = {SN COMPUT. SCI.},
  volume = {2},
  number = {3},
  pages = {160},
  issn = {2661-8907},
  doi = {10.1007/s42979-021-00592-x},
  url = {https://doi.org/10.1007/s42979-021-00592-x},
  urldate = {2024-02-04},
  abstract = {In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated~applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study’s key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity~systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers~in various real-world situations and~application areas, particularly from the technical point of view.},
  langid = {english},
  keywords = {Artificial intelligence,Data science,Data-driven decision-making,Deep learning,Intelligent applications,Machine learning,Predictive analytics},
  file = {C:\Users\simon\Zotero\storage\2FB2ANZT\Sarker - 2021 - Machine Learning Algorithms, Real-World Applicati.pdf}
}

@article{schlammingerCoolWayMeasure2014,
  title = {A Cool Way to Measure Big {{G}}},
  author = {Schlamminger, Stephan},
  date = {2014-06},
  journaltitle = {Nature},
  volume = {510},
  number = {7506},
  pages = {478--480},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature13507},
  url = {https://www.nature.com/articles/nature13507},
  urldate = {2024-02-21},
  abstract = {Published results of the gravitational constant, a measure of the strength of gravity, have failed to converge. An approach that uses cold atoms provides a new data point in the quest to determine this fundamental constant. See Letter p.518},
  issue = {7506},
  langid = {english},
  keywords = {gelesen,Physics,Quantum physics,ungelesen,zitiert}
}

@inproceedings{singhChatGPTGoogle2023,
  title = {Chat {{GPT}} \& {{Google Bard AI}}: {{A Review}}},
  shorttitle = {Chat {{GPT}} \& {{Google Bard AI}}},
  booktitle = {2023 {{International Conference}} on {{IoT}}, {{Communication}} and {{Automation Technology}} ({{ICICAT}})},
  author = {Singh, Shashi Kant and Kumar, Shubham and Mehra, Pawan Singh},
  date = {2023-06},
  pages = {1--6},
  doi = {10.1109/ICICAT57735.2023.10263706},
  url = {https://ieeexplore.ieee.org/abstract/document/10263706?casa_token=JMHwBzQgxnwAAAAA:70OnfYs5ECetZhuq8D_F3QXyua1Xu65rL0a_Ywve3mch00UAeSsOyVjhWCUvDuBpMX83NAbpUpM},
  urldate = {2024-02-23},
  abstract = {In today's world, Artificial Intelligence is one of the deepest and newest things to learn and research. Research on Artificial Intelligence is based on some goals and the use of some particular tools. One of the most important and latest innovations in the field of Artificial Intelligence is Chat GPT. It has created a storm in the cyber world after the launch of its prototype. In this paper, we have done a survey analysis on Chat GPT. The emphasis is on Chatbots, Chat GPT and Google Bard AI. The objective of this paper is to let the readers know about the analysis of various reviews and research work done on the topics of Chatbots, Chat GPT and Google Bard AI along with a brief comparison between them. Through this paper, we have provided a pool of knowledge about Chatbots, Chat GPT or Google Bard AI which can pave way for researchers.},
  eventtitle = {2023 {{International Conference}} on {{IoT}}, {{Communication}} and {{Automation Technology}} ({{ICICAT}})},
  keywords = {Artificial Intelligence,Automation,Chat GPT,Chatbots,Google Bard AI,Internet,Prototypes,Storms,Surveys,Technological innovation,ungelesen,zitiert},
  file = {C\:\\Users\\simon\\Zotero\\storage\\ZPZITWGX\\Singh et al_2023_Chat GPT & Google Bard AI.pdf;C\:\\Users\\simon\\Zotero\\storage\\D6YHP5BY\\10263706.html}
}

@article{sungPerspectiveReviewMemristive2018,
  title = {Perspective: {{A}} Review on Memristive Hardware for Neuromorphic Computation},
  shorttitle = {Perspective},
  author = {Sung, Changhyuck and Hwang, Hyunsang and Yoo, In Kyeong},
  date = {2018-10-05},
  journaltitle = {Journal of Applied Physics},
  shortjournal = {Journal of Applied Physics},
  volume = {124},
  number = {15},
  pages = {151903},
  issn = {0021-8979},
  doi = {10.1063/1.5037835},
  url = {https://doi.org/10.1063/1.5037835},
  urldate = {2024-02-15},
  abstract = {Neuromorphic computation is one of the axes of parallel distributed processing, and memristor-based synaptic weight is considered as a key component of this type of computation. However, the material properties of memristors, including material related physics, are not yet matured. In parallel with memristors, CMOS based Graphics Processing Unit, Field Programmable Gate Array, and Application Specific Integrated Circuit are also being developed as dedicated artificial intelligence (AI) chips for fast computation. Therefore, it is necessary to analyze the competitiveness of the memristor-based neuromorphic device in order to position the memristor in the appropriate position of the future AI ecosystem. In this article, the status of memristor-based neuromorphic computation was analyzed on the basis of papers and patents to identify the competitiveness of the memristor properties by reviewing industrial trends and academic pursuits. In addition, material issues and challenges are discussed for implementing the memristor-based neural processor.},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\78YATYXK\Sung et al. - 2018 - Perspective A review on memristive hardware for n.pdf}
}

@article{upadhyaOverviewRestrictedBoltzmann2019,
  title = {An {{Overview}} of {{Restricted Boltzmann Machines}}},
  author = {Upadhya, Vidyadhar and Sastry, P.},
  date = {2019-02-18},
  journaltitle = {Journal of the Indian Institute of Science},
  shortjournal = {Journal of the Indian Institute of Science},
  volume = {99},
  doi = {10.1007/s41745-019-0102-z},
  abstract = {The restricted Boltzmann machine (RBM) is a two-layered network of stochastic units with undirected connections between pairs of units in the two layers. The two layers of nodes are called visible and hidden nodes. In an RBM, there are no connections from visible to visible or hidden to hidden nodes. RBMs are used mainly as a generative model. They can be suitably modified to perform classification tasks also. They are among the basic building blocks of other deep learning models such as deep Boltzmann machine and deep belief networks. The aim of this article is to give a tutorial introduction to the restricted Boltzmann machines and to review the evolution of this model.},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\JAGD4W2N\An Overview of Restricted Boltzmann Machines.pdf}
}

@online{verdonQuantumHamiltonianBasedModels2019,
  title = {Quantum {{Hamiltonian-Based Models}} and the {{Variational Quantum Thermalizer Algorithm}}},
  author = {Verdon, Guillaume and Marks, Jacob and Nanda, Sasha and Leichenauer, Stefan and Hidary, Jack},
  date = {2019-10-04},
  eprint = {1910.02071},
  eprinttype = {arxiv},
  eprintclass = {quant-ph},
  url = {http://arxiv.org/abs/1910.02071},
  urldate = {2024-02-19},
  abstract = {We introduce a new class of generative quantum-neural-network-based models called Quantum Hamiltonian-Based Models (QHBMs). In doing so, we establish a paradigmatic approach for quantum-probabilistic hybrid variational learning, where we efficiently decompose the tasks of learning classical and quantum correlations in a way which maximizes the utility of both classical and quantum processors. In addition, we introduce the Variational Quantum Thermalizer (VQT) for generating the thermal state of a given Hamiltonian and target temperature, a task for which QHBMs are naturally well-suited. The VQT can be seen as a generalization of the Variational Quantum Eigensolver (VQE) to thermal states: we show that the VQT converges to the VQE in the zero temperature limit. We provide numerical results demonstrating the efficacy of these techniques in illustrative examples. We use QHBMs and the VQT on Heisenberg spin systems, we apply QHBMs to learn entanglement Hamiltonians and compression codes in simulated free Bosonic systems, and finally we use the VQT to prepare thermal Fermionic Gaussian states for quantum simulation.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Quantum Physics,ungelesen,zitiert},
  file = {C\:\\Users\\simon\\Zotero\\storage\\3HTPGVXD\\Verdon et al_2019_Quantum Hamiltonian-Based Models and the Variational Quantum Thermalizer.pdf;C\:\\Users\\simon\\Zotero\\storage\\5WMRJVH6\\1910.html}
}

@online{wangOscillatorbasedIsingMachine2017,
  title = {Oscillator-Based {{Ising Machine}}},
  author = {Wang, Tianshi and Roychowdhury, Jaijeet},
  date = {2017-10-12},
  eprint = {1709.08102},
  eprinttype = {arxiv},
  eprintclass = {physics},
  doi = {10.48550/arXiv.1709.08102},
  url = {http://arxiv.org/abs/1709.08102},
  urldate = {2024-02-15},
  abstract = {Many combinatorial optimization problems can be mapped to finding the ground states of the corresponding Ising Hamiltonians. The physical systems that can solve optimization problems in this way, namely Ising machines, have been attracting more and more attention recently. Our work shows that Ising machines can be realized using almost any nonlinear self-sustaining oscillators with logic values encoded in their phases. Many types of such oscillators are readily available for large-scale integration, with potentials in high-speed and low-power operation. In this paper, we describe the operation and mechanism of oscillator-based Ising machines. The feasibility of our scheme is demonstrated through several examples in simulation and hardware, among which a simulation study reports average solutions exceeding those from state-of-art Ising machines on a benchmark combinatorial optimization problem of size 2000.},
  pubstate = {preprint},
  keywords = {Computer Science - Emerging Technologies,Physics - Computational Physics,ungelesen},
  file = {C:\Users\simon\Zotero\storage\Y6S4QDT9\Oscillator-based Ising Machine.pdf}
}

@book{wittpahlKuenstlicheIntelligenzTechnologie2019,
  title = {Künstliche Intelligenz: Technologie | Anwendung | Gesellschaft},
  shorttitle = {Künstliche Intelligenz},
  editor = {Wittpahl, Volker},
  date = {2019},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-58042-4},
  url = {http://link.springer.com/10.1007/978-3-662-58042-4},
  urldate = {2024-02-15},
  isbn = {978-3-662-58041-7 978-3-662-58042-4},
  langid = {ngerman},
  keywords = {Arbeitswelt 4.0,Breitbandausbau,Datenaufbereitung,Digitale Geschäftsmodelle,Digitales Lernen,Echtzeitvernetzung,Gesellschaftlicher Wandel,Industrie 4.0,Open Access,ungelesen,Wertschöpfung und Arbeitsmarkt,zitiert},
  file = {C:\Users\simon\Zotero\storage\IVHYC2WF\Künstliche Intelligenz.pdf}
}

@article{yaoMassivelyParallelAssociative2013,
  title = {A {{Massively Parallel Associative Memory Based}} on {{Sparse Neural Networks}}},
  author = {Yao, Zhe and Gripon, Vincent and Rabbat, Michael},
  date = {2013-03-27},
  abstract = {Associative memories store content in such a way that the content can be later retrieved by presenting the memory with a small portion of the content, rather than presenting the memory with an address as in more traditional memories. Associative memories are used as building blocks for algorithms within database engines, anomaly detection systems, compression algorithms, and face recognition systems. A classical example of an associative memory is the Hopfield neural network. Recently, Gripon and Berrou have introduced an alternative construction which builds on ideas from the theory of error correcting codes and which greatly outperforms the Hopfield network in capacity, diversity, and efficiency. In this paper we implement a variation of the Gripon-Berrou associative memory on a general purpose graphical processing unit (GPU). The work of Gripon and Berrou proposes two retrieval rules, sum-of-sum and sum-of-max. The sum-of-sum rule uses only matrix-vector multiplication and is easily implemented on the GPU. The sum-of-max rule is much less straightforward to implement because it involves non-linear operations. However, the sum-of-max rule gives significantly better retrieval error rates. We propose a hybrid rule tailored for implementation on a GPU which achieves a 880-fold speedup without sacrificing any accuracy.},
  file = {C:\Users\simon\Zotero\storage\F5FHS8MI\Yao et al_2013_A Massively Parallel Associative Memory Based on Sparse Neural Networks.pdf}
}

@inproceedings{zhaiDeepStructuredEnergy2016,
  title = {Deep {{Structured Energy Based Models}} for {{Anomaly Detection}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Zhai, Shuangfei and Cheng, Yu and Lu, Weining and Zhang, Zhongfei},
  date = {2016-06-11},
  pages = {1100--1109},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v48/zhai16.html},
  urldate = {2024-02-19},
  abstract = {In this paper, we attack the anomaly detection problem by directly modeling the data distribution with deep architectures. We hence propose deep structured energy based models (DSEBMs), where the energy function is the output of a deterministic deep neural network with structure. We develop novel model architectures to integrate EBMs with different types of data such as static data, sequential data, and spatial data, and apply appropriate model architectures to adapt to the data structure. Our training algorithm is built upon the recent development of score matching (Hyvarinen, 2005), which connects an EBM with a regularized autoencoder, eliminating the need for complicated sampling method. Statistically sound decision criterion can be derived for anomaly detection purpose from the perspective of the energy landscape of the data distribution. We investigate two decision criteria for performing anomaly detection: the energy score and the reconstruction error. Extensive empirical studies on benchmark anomaly detection tasks demonstrate that our proposed model consistently matches or outperforms all the competing methods.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\Y4E93TJV\Deep Structured Energy Based Models for Anomaly Detection.pdf}
}

@article{zhangOverviewRestrictedBoltzmann2018,
  title = {An Overview on {{Restricted Boltzmann Machines}}},
  author = {Zhang, Nan and Ding, Shifei and Zhang, Jian and Xue, Yu},
  date = {2018-01-31},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {275},
  pages = {1186--1199},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.09.065},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231217315849},
  urldate = {2024-02-15},
  abstract = {The Restricted Boltzmann Machine (RBM) has aroused wide interest in machine learning fields during the past decade. This review aims to report the recent developments in theoretical research and applications of the RBM. We first give an overview of the general RBM from the theoretical perspective, including stochastic approximation methods, stochastic gradient methods, and preventing overfitting methods. And then this review focuses on the RBM variants which further improve the learning ability of the RBM under general or specific applications. The RBM has recently been extended for representational learning, document modeling, multi-label learning, weakly supervised learning and many other tasks. The RBM and RBM variants provide powerful tools for representing dependency in the data, and they can be used as the basic building blocks to create deep networks. Apart from the Deep Belief Network (DBN) and the Deep Boltzmann Machine (DBM), the RBM can also be combined with the Convolutional Neural Network (CNN) to create deep networks. This review provides a comprehensive view of these advances in the RBM together with its future perspectives.},
  keywords = {Classification,Deep networks,Representational learning,Restricted Boltzmann Machine,ungelesen},
  file = {C:\Users\simon\Zotero\storage\739QIQAV\Zhang et al. - 2018 - An overview on Restricted Boltzmann Machines.pdf}
}
