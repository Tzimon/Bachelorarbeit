@inproceedings{aaditAcceleratingAdaptiveParallel2023,
  title = {Accelerating {{Adaptive Parallel Tempering}} with {{FPGA-based}} p-Bits},
  booktitle = {2023 {{IEEE Symposium}} on {{VLSI Technology}} and {{Circuits}} ({{VLSI Technology}} and {{Circuits}})},
  author = {Aadit, Navid Anjum and Mohseni, Masoud and Camsari, Kerem Y.},
  date = {2023-06},
  pages = {1--2},
  issn = {2158-9682},
  doi = {10.23919/VLSITechnologyandCir57934.2023.10185207},
  url = {https://ieeexplore.ieee.org/abstract/document/10185207},
  urldate = {2024-04-10},
  abstract = {Special-purpose hardware to solve optimization problems formulated as Ising models has generated great excitement recently. Despite a large diversity in hardware, most solvers employ standard variations of the classical (simulated) annealing (CA) algorithm. Here, we show how powerful replica-based Parallel Tempering (PT) algorithms can significantly outperform CA, using FPGA-based probabilistic computers. Using a massively parallel (graph-colored) architecture, we implement the Adaptive PT (APT) algorithm, generating problem-dependent temperature profiles to equalize replica swap probabilities. We benchmark our p-computer against analytical results from classical Ising theory and use our machine to solve spin-glass instances formulated as hard optimization problems. APT outperforms heuristic choices of temperature profiles used in conventional PT and a replica-based version of CA. Our machine provides 6,000X speedup over optimized CPU, with orders of magnitude further speedup projected for scaled implementations. The developed co-design techniques may be useful for a broad range of Ising machines beyond p-computers.},
  eventtitle = {2023 {{IEEE Symposium}} on {{VLSI Technology}} and {{Circuits}} ({{VLSI Technology}} and {{Circuits}})},
  keywords = {Computational modeling,Computer architecture,Computers,Probabilistic logic,Simulated annealing,Temperature distribution,Very large scale integration},
  file = {C\:\\Users\\simon\\Zotero\\storage\\UEWGDRCY\\Aadit et al_2023_Accelerating Adaptive Parallel Tempering with FPGA-based p-bits.pdf;C\:\\Users\\simon\\Zotero\\storage\\NMUS4SFU\\10185207.html}
}

@inproceedings{aaditAcceleratingAdaptiveParallel2023a,
  title = {Accelerating {{Adaptive Parallel Tempering}} with {{FPGA-based}} p-Bits},
  booktitle = {2023 {{IEEE Symposium}} on {{VLSI Technology}} and {{Circuits}} ({{VLSI Technology}} and {{Circuits}})},
  author = {Aadit, Navid Anjum and Mohseni, Masoud and Camsari, Kerem Y.},
  date = {2023-06},
  pages = {1--2},
  issn = {2158-9682},
  doi = {10.23919/VLSITechnologyandCir57934.2023.10185207},
  url = {https://ieeexplore.ieee.org/abstract/document/10185207},
  urldate = {2024-04-16},
  abstract = {Special-purpose hardware to solve optimization problems formulated as Ising models has generated great excitement recently. Despite a large diversity in hardware, most solvers employ standard variations of the classical (simulated) annealing (CA) algorithm. Here, we show how powerful replica-based Parallel Tempering (PT) algorithms can significantly outperform CA, using FPGA-based probabilistic computers. Using a massively parallel (graph-colored) architecture, we implement the Adaptive PT (APT) algorithm, generating problem-dependent temperature profiles to equalize replica swap probabilities. We benchmark our p-computer against analytical results from classical Ising theory and use our machine to solve spin-glass instances formulated as hard optimization problems. APT outperforms heuristic choices of temperature profiles used in conventional PT and a replica-based version of CA. Our machine provides 6,000X speedup over optimized CPU, with orders of magnitude further speedup projected for scaled implementations. The developed co-design techniques may be useful for a broad range of Ising machines beyond p-computers.},
  eventtitle = {2023 {{IEEE Symposium}} on {{VLSI Technology}} and {{Circuits}} ({{VLSI Technology}} and {{Circuits}})},
  keywords = {Computational modeling,Computer architecture,Computers,Probabilistic logic,Simulated annealing,Temperature distribution,Very large scale integration},
  file = {C:\Users\simon\Zotero\storage\7Y36FBSH\10185207.html}
}

@article{abarAgentBasedModelling2017,
  title = {Agent {{Based Modelling}} and {{Simulation}} Tools: {{A}} Review of the State-of-Art Software},
  shorttitle = {Agent {{Based Modelling}} and {{Simulation}} Tools},
  author = {Abar, Sameera and Theodoropoulos, Georgios K. and Lemarinier, Pierre and O’Hare, Gregory M. P.},
  date = {2017-05-01},
  journaltitle = {Computer Science Review},
  shortjournal = {Computer Science Review},
  volume = {24},
  pages = {13--33},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2017.03.001},
  url = {https://www.sciencedirect.com/science/article/pii/S1574013716301198},
  urldate = {2024-04-02},
  abstract = {The key intent of this work is to present a comprehensive comparative literature survey of the state-of-art in software agent-based computing technology and its incorporation within the modelling and simulation domain. The original contribution of this survey is two-fold: (1) Present a concise characterization of almost the entire spectrum of agent-based modelling and simulation tools, thereby highlighting the salient features, merits, and shortcomings of such multi-faceted application software; this article covers eighty five agent-based toolkits that may assist the system designers and developers with common tasks, such as constructing agent-based models and portraying the real-time simulation outputs in tabular/graphical formats and visual recordings. (2) Provide a usable reference that aids engineers, researchers, learners and academicians in readily selecting an appropriate agent-based modelling and simulation toolkit for designing and developing their system models and prototypes, cognizant of both their expertise and those requirements of their application domain. In a nutshell, a significant synthesis of Agent Based Modelling and Simulation (ABMS) resources has been performed in this review that stimulates further investigation into this topic.},
  keywords = {Agent Based Modelling and Simulation (ABMS) tools,Artificial life / social science simulations,Modelling complex systems,Multi-agent computing,Software agent,Swarm intelligence},
  file = {C:\Users\simon\Zotero\storage\JPPXWS87\S1574013716301198.html}
}

@article{ackleyLearningAlgorithmBoltzmann1985,
  title = {A Learning Algorithm for Boltzmann Machines},
  author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
  date = {1985-01-01},
  journaltitle = {Cognitive Science},
  shortjournal = {Cognitive Science},
  volume = {9},
  number = {1},
  pages = {147--169},
  issn = {0364-0213},
  doi = {10.1016/S0364-0213(85)80012-4},
  url = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
  urldate = {2024-02-16},
  abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\42UB7SBL\Ackley et al_1985_A learning algorithm for boltzmann machines.pdf}
}

@article{adlemanMolecularComputationSolutions1994,
  title = {Molecular {{Computation}} of {{Solutions}} to {{Combinatorial Problems}}},
  author = {Adleman, Leonard M.},
  date = {1994-11-11},
  journaltitle = {Science},
  volume = {266},
  number = {5187},
  pages = {1021--1024},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.7973651},
  url = {https://www.science.org/doi/10.1126/science.7973651},
  urldate = {2024-05-01},
  abstract = {The tools of molecular biology were used to solve an instance of the directed Hamiltonian path problem. A small graph was encoded in molecules of DNA, and the "operations" of the computation were performed with standard protocols and enzymes. This experiment demonstrates the feasibility of carrying out computations at the molecular level.}
}

@article{ahadNeuralNetworksWireless2016,
  title = {Neural Networks in Wireless Networks: {{Techniques}}, Applications and Guidelines},
  shorttitle = {Neural Networks in Wireless Networks},
  author = {Ahad, Nauman and Qadir, Junaid and Ahsan, Nasir},
  date = {2016-06-01},
  journaltitle = {Journal of Network and Computer Applications},
  shortjournal = {Journal of Network and Computer Applications},
  volume = {68},
  pages = {1--27},
  issn = {1084-8045},
  doi = {10.1016/j.jnca.2016.04.006},
  url = {https://www.sciencedirect.com/science/article/pii/S1084804516300492},
  urldate = {2024-02-28},
  abstract = {The design of modern wireless networks, which involves decision making and parameter optimization, is quite challenging due to the highly dynamic, and often unknown, environmental conditions that characterize wireless networks. There is a common trend in modern networks to incorporate artificial intelligence (AI) techniques to cope with this design complexity. While a number of AI techniques have been profitably employed in the wireless networks community, the well-established AI framework of neural networks (NNs), well known for their remarkable generality and versatility, has been applied in a wide variety of settings in wireless networks. In particular, NNs are especially popular for tasks involving classification, learning, or optimization. In this paper, we provide both an exposition of common NN models and a comprehensive survey of the applications of NNs in wireless networks. We also identify pitfalls and challenges of implementing NNs especially when we consider alternative AI models and techniques. While various surveys on NNs exist in the literature, our paper is the first paper, to the best of our knowledge, which focuses on the applications of NNs in wireless networks.},
  keywords = {Artificial intelligence,Computational intelligence,Neural networks,Wireless networks},
  file = {C:\Users\simon\Zotero\storage\I74EW2SD\S1084804516300492.html}
}

@article{ahmadOptimizingHardwareAccelerated2020,
  title = {Optimizing {{Hardware Accelerated General Matrix-Matrix Multiplication}} for {{CNNs}} on {{FPGAs}}},
  author = {Ahmad, Afzal and Pasha, Muhammad Adeel},
  date = {2020-11},
  journaltitle = {IEEE Transactions on Circuits and Systems II: Express Briefs},
  volume = {67},
  number = {11},
  pages = {2692--2696},
  issn = {1558-3791},
  doi = {10.1109/TCSII.2020.2965154},
  url = {https://ieeexplore.ieee.org/abstract/document/8954788?casa_token=FflZ-99s30MAAAAA:l-4hcDRIsH2x9gRN3bGMy8BAo1nbQbrJEhqZpdRnAR5IJSe2naviSLmKFiAuYV_yuWVlAPPPdbs},
  urldate = {2024-03-18},
  abstract = {Convolution is inarguably the most complex operation utilized in Convolutional Neural Networks (convnets). Owing to the billions of independent multiply-adds involved, convolution is being massively parallelized by the simultaneous utilization of many cores of Graphical Processing Units (GPUs). Although GPUs have shown significant performance improvements in both training and inference stages, they are not well-suited for mobile vision applications where both energy and real-time constraints need to be satisfied. In contrast, Field Programmable Gate Arrays (FPGAs) have demonstrated massive parallelization capabilities, with fast DSPs and on-chip memory, at a lower energy cost than GPUs. Hence, they are being utilized to design convnet accelerators for embedded applications. In this brief, we design an FPGA-based accelerator for general matrix-matrix multiplication (GeMM) to improve the efficiency of convolutional layers of Shufflenet, an efficient convnet architecture. Experimental results show significant performance improvements against the state-of-the-art FPGA-based implementations of both efficient convnets that are tailored towards mobile vision applications, and complex convnets that are used in traditional applications.},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems II}}: {{Express Briefs}}},
  keywords = {Complexity theory,Computer architecture,convnets,Convolution,FPGAs,Hardware,Hardware acceleration,Kernel,Shape,Tensors},
  file = {C\:\\Users\\simon\\Zotero\\storage\\RWTCIR59\\Ahmad_Pasha_2020_Optimizing Hardware Accelerated General Matrix-Matrix Multiplication for CNNs.pdf;C\:\\Users\\simon\\Zotero\\storage\\GN8KR79F\\8954788.html}
}

@inproceedings{ahmedPrototypingFrameworkHumanCentered2021,
  title = {A {{Prototyping Framework}} for {{Human-Centered Product Design}}: {{Preliminary Validation Study}}},
  shorttitle = {A {{Prototyping Framework}} for {{Human-Centered Product Design}}},
  booktitle = {Design, {{User Experience}}, and {{Usability}}:  {{UX Research}} and {{Design}}},
  author = {Ahmed, Salman and Demirel, H. Onan},
  editor = {Soares, Marcelo M. and Rosenzweig, Elizabeth and Marcus, Aaron},
  date = {2021},
  pages = {3--14},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-78221-4_1},
  abstract = {Prototyping is a crucial aspect of the product design and development process. Successful development and deployment of products hinge on building correct prototypes. However, current prototyping frameworks often do not provide specific prototyping guidelines that are practical and readily applicable and do not consider Human Factors Engineering (HFE) design guidelines thoroughly. This paper presents a novel prototyping framework that integrates prototyping and HFE guidelines to address the research gap in the prototyping literature. The prototyping framework presents a database in the form of a graphical user interface (GUI), which contains userform data entry. The GUI userform helps the designer to develop prototyping strategies based on the prototyping best practices and HFE principles. Further, the GUI userform suggests tools and technologies that can be used to fabricate the prototype. Thus, the framework helps designers plan prototyping activities and reduces the reliance on intuition when fabricating prototypes. This paper also presents a preliminary validation study that focuses on exploring whether there is a statistical difference between the intervention and control group in developing prototyping strategies for various prototyping problems. The intervention and control group are tested using twelve prototyping problems, and the Prototyping Success score is measured for each group. An independent sample t-test is performed. From the statistical analysis, it can be inferred that the participants who use the prototyping framework produce mean Prototyping Success scores that are higher than that of the control group.},
  isbn = {978-3-030-78221-4},
  langid = {english},
  keywords = {Design,Digital human modeling,Ergonomics,Human factors engineering,Human-centered design,Prototyping framework,Validation},
  file = {C:\Users\simon\Zotero\storage\GM5EJ3MG\Ahmed_Demirel_2021_A Prototyping Framework for Human-Centered Product Design.pdf}
}

@inproceedings{albreemGreenInternetThings2017,
  title = {Green Internet of Things ({{IoT}}): {{An}} Overview},
  shorttitle = {Green Internet of Things ({{IoT}})},
  booktitle = {2017 {{IEEE}} 4th {{International Conference}} on {{Smart Instrumentation}}, {{Measurement}} and {{Application}} ({{ICSIMA}})},
  author = {Albreem, Mahmoud A. M. and El-Saleh, Ayman A. and Isa, Muzamir and Salah, Wael and Jusoh, M. and Azizan, M.M and Ali, A},
  date = {2017-11},
  pages = {1--6},
  doi = {10.1109/ICSIMA.2017.8312021},
  url = {https://ieeexplore.ieee.org/abstract/document/8312021},
  urldate = {2024-03-10},
  abstract = {Internet of Things (IoT) connects everything in the smart world, and thus, energy consumption of IoT technology is a challenge and attractive research area. Motivated by achieving a low power consumption IoT, a green IoT is proposed. This paper provides an overview regarding green IoT. It also discusses the life cycle of green IoT which contains green design, green production, green utilization, and green recycling. Furthermore, green IoT technologies such as green tags, green sensing networks and green internet technologies are discussed. In addition, studies of IoT in 5G and IoT for smart cities are presented. Finally, future research directions and open challenges about green IoT are presented.},
  eventtitle = {2017 {{IEEE}} 4th {{International Conference}} on {{Smart Instrumentation}}, {{Measurement}} and {{Application}} ({{ICSIMA}})},
  keywords = {5G,5G mobile communication,Air pollution,cloud computing,energy efficiency,green IoT,Green products,Internet of Things,Internet of Things (IoT),RFID tags,smart cities,wireless sensor networks,Wireless sensor networks},
  file = {C\:\\Users\\simon\\Zotero\\storage\\JHMUDGWT\\Albreem et al_2017_Green internet of things (IoT).pdf;C\:\\Users\\simon\\Zotero\\storage\\4ZXVN2YF\\8312021.html}
}

@article{amariInformationGeometryBoltzmann1992,
  title = {Information Geometry of {{Boltzmann}} Machines},
  author = {Amari, S. and Kurata, K. and Nagaoka, H.},
  date = {1992-03},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {3},
  number = {2},
  pages = {260--271},
  issn = {1941-0093},
  doi = {10.1109/72.125867},
  url = {https://ieeexplore.ieee.org/abstract/document/125867},
  urldate = {2024-02-16},
  abstract = {A Boltzmann machine is a network of stochastic neurons. The set of all the Boltzmann machines with a fixed topology forms a geometric manifold of high dimension, where modifiable synaptic weights of connections play the role of a coordinate system to specify networks. A learning trajectory, for example, is a curve in this manifold. It is important to study the geometry of the neural manifold, rather than the behavior of a single network, in order to know the capabilities and limitations of neural networks of a fixed topology. Using the new theory of information geometry, a natural invariant Riemannian metric and a dual pair of affine connections on the Boltzmann neural network manifold are established. The meaning of geometrical structures is elucidated from the stochastic and the statistical point of view. This leads to a natural modification of the Boltzmann machine learning rule.{$<>$}},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Computer architecture,Information geometry,Information processing,Machine learning,Manifolds,Network topology,Neural networks,Neurons,Probability distribution,Stochastic processes,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\5Q29TWP2\Information_geometry_of_Boltzmann_machines.pdf}
}

@inproceedings{amersfoortUncertaintyEstimationUsing2020,
  title = {Uncertainty {{Estimation Using}} a {{Single Deep Deterministic Neural Network}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Amersfoort, Joost Van and Smith, Lewis and Teh, Yee Whye and Gal, Yarin},
  date = {2020-11-21},
  pages = {9690--9700},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/van-amersfoort20a.html},
  urldate = {2024-03-07},
  abstract = {We propose a method for training a deterministic deep model that can find and reject out of distribution data points at test time with a single forward pass. Our approach, deterministic uncertainty quantification (DUQ), builds upon ideas of RBF networks. We scale training in these with a novel loss function and centroid updating scheme and match the accuracy of softmax models. By enforcing detectability of changes in the input using a gradient penalty, we are able to reliably detect out of distribution data. Our uncertainty quantification scales well to large datasets, and using a single model, we improve upon or match Deep Ensembles in out of distribution detection on notable difficult dataset pairs such as FashionMNIST vs. MNIST, and CIFAR-10 vs. SVHN.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\simon\\Zotero\\storage\\7PDSUKJD\\Amersfoort et al. - 2020 - Uncertainty Estimation Using a Single Deep Determi.pdf;C\:\\Users\\simon\\Zotero\\storage\\WQI5R86F\\Amersfoort et al_2020_Uncertainty Estimation Using a Single Deep Deterministic Neural Network.pdf}
}

@article{amirsoleimaniInMemoryVectorMatrixMultiplication2020,
  title = {In-{{Memory Vector-Matrix Multiplication}} in {{Monolithic Complementary Metal}}–{{Oxide}}–{{Semiconductor-Memristor Integrated Circuits}}: {{Design Choices}}, {{Challenges}}, and {{Perspectives}}},
  shorttitle = {In-{{Memory Vector-Matrix Multiplication}} in {{Monolithic Complementary Metal}}–{{Oxide}}–{{Semiconductor-Memristor Integrated Circuits}}},
  author = {Amirsoleimani, Amirali and Alibart, Fabien and Yon, Victor and Xu, Jianxiong and Pazhouhandeh, M. Reza and Ecoffey, Serge and Beilliard, Yann and Genov, Roman and Drouin, Dominique},
  date = {2020},
  journaltitle = {Advanced Intelligent Systems},
  volume = {2},
  number = {11},
  pages = {2000115},
  issn = {2640-4567},
  doi = {10.1002/aisy.202000115},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202000115},
  urldate = {2024-02-15},
  abstract = {The low communication bandwidth between memory and processing units in conventional von Neumann machines does not support the requirements of emerging applications that rely extensively on large sets of data. More recent computing paradigms, such as high parallelization and near-memory computing, help alleviate the data communication bottleneck to some extent, but paradigm-shifting concepts are required. In-memory computing has emerged as a prime candidate to eliminate this bottleneck by colocating memory and processing. In this context, resistive switching (RS) memory devices is a key promising choice, due to their unique intrinsic device-level properties, enabling both storing and computing with a small, massively-parallel footprint at low power. Theoretically, this directly translates to a major boost in energy efficiency and computational throughput, but various practical challenges remain. A qualitative and quantitative analysis of several key existing challenges in implementing high-capacity, high-volume RS memories for accelerating the most computationally demanding computation in machine learning (ML) inference, that of vector-matrix multiplication (VMM), is presented. The monolithic integration of RS memories with complementary metal–oxide–semiconductor (CMOS) integrated circuits is presented as the core underlying technology. The key existing design choices in terms of device-level physical implementation, circuit-level design, and system-level considerations is reviewed and an outlook for future directions is provided.},
  langid = {english},
  keywords = {complementary metal–oxide–semiconductor,in-memory computing,inference,memristors,redox-based random access memories,resistive switching memories,ungelesen,vector-matrix multiplications},
  file = {C:\Users\simon\Zotero\storage\SZ37ATSG\In‐Memory Vector‐Matrix Multiplication in Monolithic Complementary.pdf}
}

@article{amirsoleimaniInMemoryVectorMatrixMultiplication2020a,
  title = {In-{{Memory Vector-Matrix Multiplication}} in {{Monolithic Complementary Metal}}–{{Oxide}}–{{Semiconductor-Memristor Integrated Circuits}}: {{Design Choices}}, {{Challenges}}, and {{Perspectives}}},
  shorttitle = {In-{{Memory Vector-Matrix Multiplication}} in {{Monolithic Complementary Metal}}–{{Oxide}}–{{Semiconductor-Memristor Integrated Circuits}}},
  author = {Amirsoleimani, Amirali and Alibart, Fabien and Yon, Victor and Xu, Jianxiong and Pazhouhandeh, M. Reza and Ecoffey, Serge and Beilliard, Yann and Genov, Roman and Drouin, Dominique},
  date = {2020},
  journaltitle = {Advanced Intelligent Systems},
  volume = {2},
  number = {11},
  pages = {2000115},
  issn = {2640-4567},
  doi = {10.1002/aisy.202000115},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202000115},
  urldate = {2024-03-21},
  abstract = {The low communication bandwidth between memory and processing units in conventional von Neumann machines does not support the requirements of emerging applications that rely extensively on large sets of data. More recent computing paradigms, such as high parallelization and near-memory computing, help alleviate the data communication bottleneck to some extent, but paradigm-shifting concepts are required. In-memory computing has emerged as a prime candidate to eliminate this bottleneck by colocating memory and processing. In this context, resistive switching (RS) memory devices is a key promising choice, due to their unique intrinsic device-level properties, enabling both storing and computing with a small, massively-parallel footprint at low power. Theoretically, this directly translates to a major boost in energy efficiency and computational throughput, but various practical challenges remain. A qualitative and quantitative analysis of several key existing challenges in implementing high-capacity, high-volume RS memories for accelerating the most computationally demanding computation in machine learning (ML) inference, that of vector-matrix multiplication (VMM), is presented. The monolithic integration of RS memories with complementary metal–oxide–semiconductor (CMOS) integrated circuits is presented as the core underlying technology. The key existing design choices in terms of device-level physical implementation, circuit-level design, and system-level considerations is reviewed and an outlook for future directions is provided.},
  langid = {english},
  keywords = {complementary metal–oxide–semiconductor,in-memory computing,inference,memristors,redox-based random access memories,resistive switching memories,vector-matrix multiplications},
  file = {C\:\\Users\\simon\\Zotero\\storage\\2M7R2P3J\\Amirsoleimani et al_2020_In-Memory Vector-Matrix Multiplication in Monolithic Complementary.pdf;C\:\\Users\\simon\\Zotero\\storage\\V7M2U7QL\\aisy.html}
}

@online{anon.AIProgramsConsume,
  title = {{{AI}} Programs Consume Large Volumes of Scarce Water},
  author = {{Anon.}},
  url = {https://news.ucr.edu/articles/2023/04/28/ai-programs-consume-large-volumes-scarce-water},
  urldate = {2024-04-28},
  abstract = {UCR study the first time estimates the huge water footprint from running artificial intelligence queries that rely on the cloud computations done in racks of servers that must be kept cool in warehouse-sized data processing centers.},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\E4IJ94KL\ai-programs-consume-large-volumes-scarce-water.html}
}

@online{anon.CloserLookCarbon2023,
  title = {A {{Closer Look}} at {{The Carbon Footprint}} of {{ChatGPT}}},
  author = {{Anon.}},
  date = {2023-11-03T12:32:52+00:00},
  url = {https://piktochart.com/blog/carbon-footprint-of-chatgpt/},
  urldate = {2024-04-28},
  abstract = {A short conversation with ChatGPT produces as much CO2 emissions as boiling a kettle. We take a brief look into ChatGPT’s carbon footprint.},
  langid = {american},
  organization = {Piktochart},
  file = {C:\Users\simon\Zotero\storage\HQ4G9EPQ\carbon-footprint-of-chatgpt.html}
}

@article{anon.Electricity2024Analysis2024,
  title = {Electricity 2024 - {{Analysis}} and Forecast to 2026},
  author = {{Anon.}},
  date = {2024},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\KXK8XQZ2\2024 - Electricity 2024 - Analysis and forecast to 2026.pdf}
}

@article{archieStatisticalAnalysisHeterozygosity1985,
  title = {Statistical {{Analysis}} of {{Heterozygosity Data}}: {{Independent Sample Comparisons}}},
  shorttitle = {Statistical {{Analysis}} of {{Heterozygosity Data}}},
  author = {Archie, James W.},
  date = {1985},
  journaltitle = {Evolution},
  volume = {39},
  number = {3},
  pages = {623--637},
  issn = {1558-5646},
  doi = {10.1111/j.1558-5646.1985.tb00399.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1558-5646.1985.tb00399.x},
  urldate = {2024-04-16},
  abstract = {The distribution of mean heterozygosities under an infinite allele model with constant mutation rate was examined through simulation studies. It was found that, although the variance of the distribution decreases with increasing numbers of loci examined as expected, the shape of the distribution may remain skewed or bimodal. The distribution becomes symmetrical for increasing mean heterozygosity levels and numbers of loci. As a result, parametric statistical tests may not be valid for making comparisons among populations or species. Independent sample t-tests were examined in detail to determine the frequency of rejection of the null hypothesis when pairs of samples are drawn from populations with the same mean heterozygosity. Differing numbers of loci and levels of mean heterozygosity were examined. For mean heterozygosity levels above 7.5\%, t-tests provide the proper rejection rate, with as few as five loci. When mean heterozygosity is as low as 2.5\%, the t-test is conservative even when 40 loci are examined in each population. Independent sample t-tests were then examined for their power to detect true differences between populations as the degree of difference and number of loci vary. Although large differences can be found with high certainty, differences on the order of 5\% heterozygosity may require that large numbers of loci ({$>$}40) be examined in order to be 80\% or more certain of detecting them. In addition, it is emphasized that, for small numbers of loci ({$<$}25), the statistical detection of differences of interesting magnitude requires that relatively rare sampling events occur and that much larger differences be observed among the samples than exist for the population means. Two reasons exist for the lack of sensitivity of the test procedures. First, when mean heterozygosity levels are low, the non-normality of the sample means is perhaps most important. Second, even when mean heterozygosity levels are high or when sample sizes are large enough so sample means are approximately normally distributed, the intrinsically high interlocus variance of heterozygosity estimates makes the tests insensitive to the presence of heterozygosity differences that might be biologically meaningful. Finally, the implications of the results of this study are discussed with regard to observed low levels of correlation between heterozygosity and other explanatory variables.},
  langid = {english},
  file = {C\:\\Users\\simon\\Zotero\\storage\\4D8XXWNJ\\Archie_1985_Statistical Analysis of Heterozygosity Data.pdf;C\:\\Users\\simon\\Zotero\\storage\\GZAQZY7M\\j.1558-5646.1985.tb00399.html}
}

@online{ASICDesignFlow,
  title = {{{ASIC Design Flow}} for {{VLSI Engineering Teams}} [{{GUIDE}}] - {{Xinyx Design}}},
  url = {https://www.xinyxdesign.com/resources/asic-design-flow-for-vlsi-engineering-teams/},
  urldate = {2024-04-03},
  abstract = {The ASIC design flow is a time-tested mature methodology that combines multiple steps to form the backbone of every ASIC design project. All IC engineers have encountered the ASIC design flow in some sequence or another.},
  langid = {american},
  file = {C:\Users\simon\Zotero\storage\QPQ9HMW7\asic-design-flow-for-vlsi-engineering-teams.html}
}

@article{babuReconfigurableFPGAArchitectures2021,
  title = {Reconfigurable {{FPGA Architectures}}: {{A Survey}} and {{Applications}}},
  shorttitle = {Reconfigurable {{FPGA Architectures}}},
  author = {Babu, Praveenkumar and Parthasarathy, Eswaran},
  date = {2021-02-01},
  journaltitle = {Journal of The Institution of Engineers (India): Series B},
  shortjournal = {J. Inst. Eng. India Ser. B},
  volume = {102},
  number = {1},
  pages = {143--156},
  issn = {2250-2114},
  doi = {10.1007/s40031-020-00508-y},
  url = {https://doi.org/10.1007/s40031-020-00508-y},
  urldate = {2024-03-20},
  abstract = {Reconfigurable computing is a potential paradigm which has been effectively performing mostly in the developments of devices likely Field Programmable Gate Arrays (FPGAs). This paper illustrates the reconfigurable architecture of FPGA and its types. Most widely used high-speed computation fabrics utilized in reconfigurable computing are FPGAs. This paper demonstrates the architectures used in reconfigurable computing and shows the various advantages of using reconfigurable computing design over conventional Application-Specific Integrated Circuits for achieving high level of performance for a desired application. The survey deals with the architecture of FPGAs and their types in detail. This paper also explains the highlights and challenges of fine-grained and coarse-grained architectures. FPGAs have supported partial reconfiguration over the few years. This survey also includes the partial reconfiguration techniques and the various applications of reconfigurability.},
  langid = {english},
  keywords = {ASIC,Coarse-grained architecture,Fine-grained architecture,FPGA,Partial reconfiguration,Reconfigurable computing},
  file = {C:\Users\simon\Zotero\storage\ED2JQJ53\Babu_Parthasarathy_2021_Reconfigurable FPGA Architectures.pdf}
}

@online{baiEfficiencySystematicSurvey2024,
  title = {Beyond {{Efficiency}}: {{A Systematic Survey}} of {{Resource-Efficient Large Language Models}}},
  shorttitle = {Beyond {{Efficiency}}},
  author = {Bai, Guangji and Chai, Zheng and Ling, Chen and Wang, Shiyu and Lu, Jiaying and Zhang, Nan and Shi, Tingwei and Yu, Ziyang and Zhu, Mengdan and Zhang, Yifei and Yang, Carl and Cheng, Yue and Zhao, Liang},
  date = {2024-01-03},
  eprint = {2401.00625},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.00625},
  url = {http://arxiv.org/abs/2401.00625},
  urldate = {2024-02-23},
  abstract = {The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between various resources and corresponding optimization techniques. A standardized set of evaluation metrics and datasets is also presented to facilitate consistent and fair comparisons across different models and techniques. By offering a comprehensive overview of the current sota and identifying open research avenues, this survey serves as a foundational reference for researchers and practitioners, aiding them in developing more sustainable and efficient LLMs in a rapidly evolving landscape.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\simon\\Zotero\\storage\\YPEGVJL8\\Bai et al_2024_Beyond Efficiency.pdf;C\:\\Users\\simon\\Zotero\\storage\\XTGM8EVM\\2401.html}
}

@online{baiEfficiencySystematicSurvey2024a,
  title = {Beyond {{Efficiency}}: {{A Systematic Survey}} of {{Resource-Efficient Large Language Models}}},
  shorttitle = {Beyond {{Efficiency}}},
  author = {Bai, Guangji and Chai, Zheng and Ling, Chen and Wang, Shiyu and Lu, Jiaying and Zhang, Nan and Shi, Tingwei and Yu, Ziyang and Zhu, Mengdan and Zhang, Yifei and Yang, Carl and Cheng, Yue and Zhao, Liang},
  date = {2024-01-03},
  eprint = {2401.00625},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2401.00625},
  urldate = {2024-03-07},
  abstract = {The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between various resources and corresponding optimization techniques. A standardized set of evaluation metrics and datasets is also presented to facilitate consistent and fair comparisons across different models and techniques. By offering a comprehensive overview of the current sota and identifying open research avenues, this survey serves as a foundational reference for researchers and practitioners, aiding them in developing more sustainable and efficient LLMs in a rapidly evolving landscape.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\simon\Zotero\storage\WJ9HX4VX\Bai et al. - 2024 - Beyond Efficiency A Systematic Survey of Resource.pdf}
}

@online{baischerLearningHardwareTutorial2021,
  title = {Learning on {{Hardware}}: {{A Tutorial}} on {{Neural Network Accelerators}} and {{Co-Processors}}},
  shorttitle = {Learning on {{Hardware}}},
  author = {Baischer, Lukas and Wess, Matthias and TaheriNejad, Nima},
  date = {2021-04-19},
  eprint = {2104.09252},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.09252},
  url = {http://arxiv.org/abs/2104.09252},
  urldate = {2024-03-18},
  abstract = {Deep neural networks (DNNs) have the advantage that they can take into account a large number of parameters, which enables them to solve complex tasks. In computer vision and speech recognition, they have a better accuracy than common algorithms, and in some tasks, they boast an even higher accuracy than human experts. With the progress of DNNs in recent years, many other fields of application such as diagnosis of diseases and autonomous driving are taking advantage of them. The trend at DNNs is clear: The network size is growing exponentially, which leads to an exponential increase in computational effort and required memory size. For this reason, optimized hardware accelerators are used to increase the performance of the inference of neuronal networks. However, there are various neural network hardware accelerator platforms, such as graphics processing units (GPUs), application specific integrated circuits (ASICs) and field programmable gate arrays (FPGAs). Each of these platforms offer certain advantages and disadvantages. Also, there are various methods for reducing the computational effort of DNNs, which are differently suitable for each hardware accelerator. In this article an overview of existing neural network hardware accelerators and acceleration methods is given. Their strengths and weaknesses are shown and a recommendation of suitable applications is given. In particular, we focus on acceleration of the inference of convolutional neural networks (CNNs) used for image recognition tasks. Given that there exist many different hardware architectures. FPGA-based implementations are well-suited to show the effect of DNN optimization methods on accuracy and throughput. For this reason, the focus of this work is more on FPGA-based implementations.},
  pubstate = {preprint},
  keywords = {Computer Science - Hardware Architecture,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\simon\\Zotero\\storage\\PDJA9RTT\\Baischer et al_2021_Learning on Hardware.pdf;C\:\\Users\\simon\\Zotero\\storage\\KSCY6NQ4\\2104.html}
}

@article{bankFAQDigitalEuro2024,
  title = {{{FAQ}} on a Digital Euro},
  author = {Bank, European Central},
  date = {2024-02-14},
  url = {https://www.ecb.europa.eu/paym/digital_euro/faqs/html/ecb.faq_digital_euro.en.html},
  urldate = {2024-03-13},
  abstract = {The European Central Bank (ECB) is the central bank of the European Union countries which have adopted the euro. Our main task is to maintain price stability in the euro area and so preserve the purchasing power of the single currency.},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\GP6EBKJT\ecb.faq_digital_euro.en.html}
}

@article{barraEquivalenceHopfieldNetworks2012,
  title = {On the Equivalence of {{Hopfield}} Networks and {{Boltzmann Machines}}},
  author = {Barra, Adriano and Bernacchia, Alberto and Santucci, Enrica and Contucci, Pierluigi},
  date = {2012-10-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {34},
  pages = {1--9},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.06.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608012001608},
  urldate = {2024-02-16},
  abstract = {A specific type of neural networks, the Restricted Boltzmann Machines (RBM), are implemented for classification and feature detection in machine learning. They are characterized by separate layers of visible and hidden units, which are able to learn efficiently a generative model of the observed data. We study a “hybrid” version of RBMs, in which hidden units are analog and visible units are binary, and we show that thermodynamics of visible units are equivalent to those of a Hopfield network, in which the N visible units are the neurons and the P hidden units are the learned patterns. We apply the method of stochastic stability to derive the thermodynamics of the model, by considering a formal extension of this technique to the case of multiple sets of stored patterns, which may act as a benchmark for the study of correlated sets. Our results imply that simulating the dynamics of a Hopfield network, requiring the update of N neurons and the storage of N(N−1)/2 synapses, can be accomplished by a hybrid Boltzmann Machine, requiring the update of N+P neurons but the storage of only NP synapses. In addition, the well known glass transition of the Hopfield network has a counterpart in the Boltzmann Machine: it corresponds to an optimum criterion for selecting the relative sizes of the hidden and visible layers, resolving the trade-off between flexibility and generality of the model. The low storage phase of the Hopfield model corresponds to few hidden units and hence a overly constrained RBM, while the spin-glass phase (too many hidden units) corresponds to unconstrained RBM prone to overfitting of the observed data.},
  keywords = {Boltzmann Machines,Hopfield networks,Statistical mechanics,ungelesen},
  file = {C\:\\Users\\simon\\Zotero\\storage\\FBU8CDS7\\On the equivalence of Hopfield networks and Boltzmann Machines.pdf;C\:\\Users\\simon\\Zotero\\storage\\NFHUABGB\\S0893608012001608.html}
}

@article{baskervilleDesignScienceResearch2018,
  title = {Design {{Science Research Contributions}}:  {{Finding}} a {{Balance}} between {{Artifact}} and {{Theory}}},
  shorttitle = {Design {{Science Research Contributions}}},
  author = {Baskerville, Richard and Baiyere, Abayomi and Gregor, Shirley and Hevner, Alan and Rossi, Matti},
  date = {2018-05-31},
  journaltitle = {Journal of the Association for Information Systems},
  volume = {19},
  number = {5},
  issn = {1536-9323},
  url = {https://aisel.aisnet.org/jais/vol19/iss5/3},
  file = {C:\Users\simon\Zotero\storage\VSSGDVAL\3.html}
}

@article{beichlMetropolisAlgorithm2000,
  title = {The {{Metropolis Algorithm}}},
  author = {Beichl, I. and Sullivan, F.},
  date = {2000-01},
  journaltitle = {Computing in Science \& Engineering},
  volume = {2},
  number = {1},
  pages = {65--69},
  issn = {1558-366X},
  doi = {10.1109/5992.814660},
  url = {https://ieeexplore.ieee.org/document/814660},
  urldate = {2024-02-27},
  abstract = {The Metropolis Algorithm has been the most successful and influential of all the members of the computational species that used to be called the "Monte Carlo method". Today, topics related to this algorithm constitute an entire field of computational science supported by a deep theory and having applications ranging from physical simulations to the foundations of computational complexity. Since the rejection method invention (J. von Neumann), it has been developed extensively and applied in a wide variety of settings. The Metropolis Algorithm can be formulated as an instance of the rejection method used for generating steps in a Markov chain.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {Computational complexity,Computational modeling,Distributed computing,Distribution functions,Hospitals,Monte Carlo methods,Physics computing,Probability distribution,Sampling methods},
  file = {C\:\\Users\\simon\\Zotero\\storage\\HWDNSHCW\\Beichl_Sullivan_2000_The Metropolis Algorithm.pdf;C\:\\Users\\simon\\Zotero\\storage\\ZTYBSUIQ\\814660.html}
}

@article{bellettiJanusFPGABasedSystem2009,
  title = {Janus: {{An FPGA-Based System}} for {{High-Performance Scientific Computing}}},
  shorttitle = {Janus},
  author = {Belletti, Francesco and Cotallo, Maria and Cruz, A and Fernandez, Luis Antonio and Gordillo-Guerrero, Antonio and Guidetti, Marco and Maiorano, Andrea and Mantovani, Filippo and Marinari, Enzo and Martin-Mayor, Victor and Munoz-Sudupe, Antonio and Navarro, Denis and Parisi, Giorgio and Perez-Gaviro, Sergio and Rossi, Mauro and Ruiz-Lorenzo, Juan J and Schifano, Sebastiano Fabio and Sciretti, Daniele and Tarancon, Alfonso and Tripiccione, Raffaele and Velasco, J Luis and Yllanes, David and Zanier, Gianpaolo},
  date = {2009-01},
  journaltitle = {Computing in Science \& Engineering},
  volume = {11},
  number = {1},
  pages = {48--58},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2009.11},
  url = {https://ieeexplore.ieee.org/document/4720223},
  urldate = {2024-04-10},
  abstract = {Janus is a modular, massively parallel, and reconfigurable FPGA-based computing system. Each Janus module has one computational core and one host. Janus is tailored to, but not limited to, the needs of a class of hard scientific applications characterized by regular code structure, unconventional data-manipulation requirements, and a few Megabits database. The authors discuss this configurable system's architecture and focus on its use for Monte Carlo simulations of statistical mechanics, as Janus performs impressively on this class of application.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {Application software,Biology computing,Computational modeling,Computer simulation,field-programmable gate array,FPGA,Janus,Lattices,Monte Carlo methods,Monte Carlo simulations,Nobel Prize,Parallel processing,Pervasive computing,Physics computing,scientific computing,Scientific computing},
  file = {C\:\\Users\\simon\\Zotero\\storage\\5LLBIGEX\\Belletti et al_2009_Janus.pdf;C\:\\Users\\simon\\Zotero\\storage\\3HBRQPYB\\4720223.html}
}

@article{bellettiJanusFPGABasedSystem2009a,
  title = {Janus: {{An FPGA-Based System}} for {{High-Performance Scientific Computing}}},
  shorttitle = {Janus},
  author = {Belletti, Francesco and Cotallo, Maria and Cruz, A and Fernandez, Luis Antonio and Gordillo-Guerrero, Antonio and Guidetti, Marco and Maiorano, Andrea and Mantovani, Filippo and Marinari, Enzo and Martin-Mayor, Victor and Munoz-Sudupe, Antonio and Navarro, Denis and Parisi, Giorgio and Perez-Gaviro, Sergio and Rossi, Mauro and Ruiz-Lorenzo, Juan J and Schifano, Sebastiano Fabio and Sciretti, Daniele and Tarancon, Alfonso and Tripiccione, Raffaele and Velasco, J Luis and Yllanes, David and Zanier, Gianpaolo},
  date = {2009-01},
  journaltitle = {Computing in Science \& Engineering},
  volume = {11},
  number = {1},
  pages = {48--58},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2009.11},
  url = {https://ieeexplore.ieee.org/document/4720223},
  urldate = {2024-04-16},
  abstract = {Janus is a modular, massively parallel, and reconfigurable FPGA-based computing system. Each Janus module has one computational core and one host. Janus is tailored to, but not limited to, the needs of a class of hard scientific applications characterized by regular code structure, unconventional data-manipulation requirements, and a few Megabits database. The authors discuss this configurable system's architecture and focus on its use for Monte Carlo simulations of statistical mechanics, as Janus performs impressively on this class of application.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {Application software,Biology computing,Computational modeling,Computer simulation,field-programmable gate array,FPGA,Janus,Lattices,Monte Carlo methods,Monte Carlo simulations,Nobel Prize,Parallel processing,Pervasive computing,Physics computing,scientific computing,Scientific computing},
  file = {C\:\\Users\\simon\\Zotero\\storage\\NPR3VKXN\\Belletti et al_2009_Janus.pdf;C\:\\Users\\simon\\Zotero\\storage\\RWYA4T6Q\\4720223.html}
}

@incollection{berrouOverviewGreenFinance2019,
  title = {An {{Overview}} of {{Green Finance}}},
  booktitle = {The {{Rise}} of {{Green Finance}} in {{Europe}}: {{Opportunities}} and {{Challenges}} for {{Issuers}}, {{Investors}} and {{Marketplaces}}},
  author = {Berrou, Romain and Dessertine, Philippe and Migliorelli, Marco},
  editor = {Migliorelli, Marco and Dessertine, Philippe},
  date = {2019},
  series = {Palgrave {{Studies}} in {{Impact Finance}}},
  pages = {3--29},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-22510-0_1},
  url = {https://doi.org/10.1007/978-3-030-22510-0_1},
  urldate = {2024-03-09},
  abstract = {This chapter aims at giving an overview of the key characteristics of the green finance market as it stands today. To this extent, the chapter first recalls the main roots of the role of ethics in finance. Hence, it deals with the political process culminating with the Paris Agreement and the adoption of the Sustainable Development Goals as well as highlights the role of green finance in such a process. Hence, it provides a detailed picture of the market by describing the types of existing green securities and financial products and showing the recent investment trends. Finally, the chapter summarises the key challenges still ahead for green finance in order to be considered a stable component of the modern financial landscape.},
  isbn = {978-3-030-22510-0},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\HCVB75GU\Berrou et al_2019_An Overview of Green Finance.pdf}
}

@inproceedings{bjarnasonModelSoftwarePrototyping2021,
  title = {A {{Model}} of {{Software Prototyping}} Based on a {{Systematic Map}}},
  booktitle = {Proceedings of the 15th {{ACM}} / {{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}} ({{ESEM}})},
  author = {Bjarnason, Elizabeth and Lang, Franz and Mjöberg, Alexander},
  date = {2021-10-11},
  series = {{{ESEM}} '21},
  pages = {1--11},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3475716.3475772},
  url = {https://dl.acm.org/doi/10.1145/3475716.3475772},
  urldate = {2024-04-01},
  abstract = {Background: Prototyping is an established practice for user interface design and for requirements engineering within agile software development, even so there is a lack of theory on prototyping. Aims: The main research objective is to provide a means to categorise prototyping instances, in order to enable comparison and reflection of prototyping practices. Method: We have performed a systematic mapping study of methodological aspects of prototyping consisting of thirty-three primary studies upon which we designed a model of prototyping that was validated through a focus group at a case company. Results: Our model consists of four aspects of prototyping, namely purpose, prototype scope, prototype use, and exploration strategy. This model supported the focus group participants in discussing prototyping practices by considering concrete prototyping instances in terms of the concepts provided by our model. Conclusions: The model can be used to categorise prototyping instances and can support practitioners in reflecting on their prototyping practices. Our study provides a starting point for further research on prototyping and into how the practice can be applied more cost-effectively to elicit, validate, and communicate requirements.},
  isbn = {978-1-4503-8665-4},
  keywords = {Agile,requirements engineering,systematic mapping study},
  file = {C:\Users\simon\Zotero\storage\ZVMPB8EP\Bjarnason et al_2021_A Model of Software Prototyping based on a Systematic Map.pdf}
}

@inproceedings{bjarnasonModelSoftwarePrototyping2021a,
  title = {A {{Model}} of {{Software Prototyping}} Based on a {{Systematic Map}}},
  booktitle = {Proceedings of the 15th {{ACM}} / {{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}} ({{ESEM}})},
  author = {Bjarnason, Elizabeth and Lang, Franz and Mjöberg, Alexander},
  date = {2021-10-11},
  series = {{{ESEM}} '21},
  pages = {1--11},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3475716.3475772},
  url = {https://dl.acm.org/doi/10.1145/3475716.3475772},
  urldate = {2024-04-01},
  abstract = {Background: Prototyping is an established practice for user interface design and for requirements engineering within agile software development, even so there is a lack of theory on prototyping. Aims: The main research objective is to provide a means to categorise prototyping instances, in order to enable comparison and reflection of prototyping practices. Method: We have performed a systematic mapping study of methodological aspects of prototyping consisting of thirty-three primary studies upon which we designed a model of prototyping that was validated through a focus group at a case company. Results: Our model consists of four aspects of prototyping, namely purpose, prototype scope, prototype use, and exploration strategy. This model supported the focus group participants in discussing prototyping practices by considering concrete prototyping instances in terms of the concepts provided by our model. Conclusions: The model can be used to categorise prototyping instances and can support practitioners in reflecting on their prototyping practices. Our study provides a starting point for further research on prototyping and into how the practice can be applied more cost-effectively to elicit, validate, and communicate requirements.},
  isbn = {978-1-4503-8665-4},
  keywords = {Agile,requirements engineering,systematic mapping study},
  file = {C:\Users\simon\Zotero\storage\ZNE8Q4K2\Bjarnason et al_2021_A Model of Software Prototyping based on a Systematic Map.pdf}
}

@article{bohBuildingDigitalResilience2023,
  title = {Building {{Digital Resilience Against Major Shocks}}},
  author = {Boh, Wai and Constantinides, Panos and Padmanabhan, Balaji and Viswanathan, Siva},
  date = {2023-02-13},
  journaltitle = {MIS Quarterly},
  shortjournal = {MIS Quarterly},
  volume = {47},
  pages = {343--361},
  abstract = {Major shocks such as the COVID-19 pandemic create unique and exceptional challenges for different entities, including individuals, groups, and organizations. In this special issue editorial, we introduce the concept of digital resilience, which refers to the capabilities developed through the use of digital technologies to absorb major shocks, adapt to disruptions caused by the shocks, and transform to a new stable state, where entities are more prepared to deal with major shocks. The individual papers in this special issue offer compelling examples of how digital resilience is exhibited and how the process of digital resilience can unfold in response to specific major shocks. Drawing upon and extending these papers, we present an integrated framework of how digital technology can help build resilience capabilities, which is missing in past research but needed to mitigate and manage future major shocks, including financial recessions and climate change. We conclude with four important themes for future IS research.},
  file = {C:\Users\simon\Zotero\storage\AQ28KTPG\Boh et al_2023_Building Digital Resilience Against Major Shocks.pdf}
}

@article{bohmNoiseinjectedAnalogIsing2022,
  title = {Noise-Injected Analog {{Ising}} Machines Enable Ultrafast Statistical Sampling and Machine Learning},
  author = {Böhm, Fabian and Alonso-Urquijo, Diego and Verschaffelt, Guy and Van der Sande, Guy},
  date = {2022-10-04},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {5847},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-33441-3},
  url = {https://www.nature.com/articles/s41467-022-33441-3},
  urldate = {2024-02-15},
  abstract = {Ising machines are a promising non-von-Neumann computational concept for neural network training and combinatorial optimization. However, while various neural networks can be implemented with Ising machines, their inability to perform fast statistical sampling makes them inefficient for training neural networks compared to digital computers. Here, we introduce a universal concept to achieve ultrafast statistical sampling with analog Ising machines by injecting noise. With an opto-electronic Ising machine, we experimentally demonstrate that this can be used for accurate sampling of Boltzmann distributions and for unsupervised training of neural networks, with equal accuracy as software-based training. Through simulations, we find that Ising machines can perform statistical sampling orders-of-magnitudes faster than software-based methods. This enables the use of Ising machines beyond combinatorial optimization and makes them into efficient tools for machine learning and other applications.},
  issue = {1},
  langid = {english},
  keywords = {Complex networks,Computer science,Information theory and computation,Optoelectronic devices and components,ungelesen},
  file = {C:\Users\simon\Zotero\storage\CUQQWVHL\Noise-injected analog Ising machines enable ultrafast statistical sampling and machine learning.pdf}
}

@article{bohmNoiseinjectedAnalogIsing2022a,
  title = {Noise-Injected Analog {{Ising}} Machines Enable Ultrafast Statistical Sampling and Machine Learning},
  author = {Böhm, Fabian and Alonso-Urquijo, Diego and Verschaffelt, Guy and Van der Sande, Guy},
  date = {2022-10-04},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {5847},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-33441-3},
  url = {https://www.nature.com/articles/s41467-022-33441-3},
  urldate = {2024-02-15},
  abstract = {Ising machines are a promising non-von-Neumann computational concept for neural network training and combinatorial optimization. However, while various neural networks can be implemented with Ising machines, their inability to perform fast statistical sampling makes them inefficient for training neural networks compared to digital computers. Here, we introduce a universal concept to achieve ultrafast statistical sampling with analog Ising machines by injecting noise. With an opto-electronic Ising machine, we experimentally demonstrate that this can be used for accurate sampling of Boltzmann distributions and for unsupervised training of neural networks, with equal accuracy as software-based training. Through simulations, we find that Ising machines can perform statistical sampling orders-of-magnitudes faster than software-based methods. This enables the use of Ising machines beyond combinatorial optimization and makes them into efficient tools for machine learning and other applications.},
  issue = {1},
  langid = {english},
  keywords = {Complex networks,Computer science,Information theory and computation,Optoelectronic devices and components,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\JC236XT8\Noise-injected analog Ising machines enable ultrafast statistical sampling and machine learning.pdf}
}

@article{boutrosFPGAArchitecturePrinciples2021,
  title = {{{FPGA Architecture}}: {{Principles}} and {{Progression}}},
  shorttitle = {{{FPGA Architecture}}},
  author = {Boutros, Andrew and Betz, Vaughn},
  date = {2021},
  journaltitle = {IEEE Circuits and Systems Magazine},
  volume = {21},
  number = {2},
  pages = {4--29},
  issn = {1558-0830},
  doi = {10.1109/MCAS.2021.3071607},
  url = {https://ieeexplore.ieee.org/abstract/document/9439568},
  urldate = {2024-03-20},
  abstract = {Since their inception more than thirty years ago, field-programmable gate arrays (FPGAs) have been widely used to implement a myriad of applications from different domains. As a result of their low-level hardware reconfigurability, FPGAs have much faster design cycles and lower development costs compared to custom-designed chips. The design of an FPGA architecture involves many different design choices starting from the high-level architectural parameters down to the transistor-level implementation details, with the goal of making a highly programmable device while minimizing the area and performance cost of reconfigurability. As the needs of applications and the capabilities of process technology are constantly evolving, FPGA architecture must also adapt. In this article, we review the evolution of the different key components of modern commercial FPGA architectures and shed the light on their main design principles and implementation challenges.},
  eventtitle = {{{IEEE Circuits}} and {{Systems Magazine}}},
  keywords = {Circuits and systems,Field programmable gate arrays,Hardware,Performance evaluation},
  file = {C\:\\Users\\simon\\Zotero\\storage\\R4KDQUQP\\Boutros_Betz_2021_FPGA Architecture.pdf;C\:\\Users\\simon\\Zotero\\storage\\ILKB34AE\\9439568.html}
}

@online{caiHarnessingIntrinsicNoise2019,
  title = {Harnessing {{Intrinsic Noise}} in {{Memristor Hopfield Neural Networks}} for {{Combinatorial Optimization}}},
  author = {Cai, Fuxi and Kumar, Suhas and Van Vaerenbergh, Thomas and Liu, Rui and Li, Can and Yu, Shimeng and Xia, Qiangfei and Yang, J. Joshua and Beausoleil, Raymond and Lu, Wei and Strachan, John Paul},
  date = {2019-04-03},
  eprint = {1903.11194},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1903.11194},
  url = {http://arxiv.org/abs/1903.11194},
  urldate = {2024-02-15},
  abstract = {We describe a hybrid analog-digital computing approach to solve important combinatorial optimization problems that leverages memristors (two-terminal nonvolatile memories). While previous memristor accelerators have had to minimize analog noise effects, we show that our optimization solver harnesses such noise as a computing resource. Here we describe a memristor-Hopfield Neural Network (mem-HNN) with massively parallel operations performed in a dense crossbar array. We provide experimental demonstrations solving NP-hard max-cut problems directly in analog crossbar arrays, and supplement this with experimentally-grounded simulations to explore scalability with problem size, providing the success probabilities, time and energy to solution, and interactions with intrinsic analog noise. Compared to fully digital approaches, and present-day quantum and optical accelerators, we forecast the mem-HNN to have over four orders of magnitude higher solution throughput per power consumption. This suggests substantially improved performance and scalability compared to current quantum annealing approaches, while operating at room temperature and taking advantage of existing CMOS technology augmented with emerging analog non-volatile memristors.},
  pubstate = {preprint},
  keywords = {Computer Science - Emerging Technologies,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\FLZE6J2J\Cai et al. - 2019 - Harnessing Intrinsic Noise in Memristor Hopfield N.pdf}
}

@online{caiHarnessingIntrinsicNoise2019a,
  title = {Harnessing {{Intrinsic Noise}} in {{Memristor Hopfield Neural Networks}} for {{Combinatorial Optimization}}},
  author = {Cai, Fuxi and Kumar, Suhas and Van Vaerenbergh, Thomas and Liu, Rui and Li, Can and Yu, Shimeng and Xia, Qiangfei and Yang, J. Joshua and Beausoleil, Raymond and Lu, Wei and Strachan, John Paul},
  date = {2019-04-03},
  eprint = {1903.11194},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1903.11194},
  url = {http://arxiv.org/abs/1903.11194},
  urldate = {2024-03-21},
  abstract = {We describe a hybrid analog-digital computing approach to solve important combinatorial optimization problems that leverages memristors (two-terminal nonvolatile memories). While previous memristor accelerators have had to minimize analog noise effects, we show that our optimization solver harnesses such noise as a computing resource. Here we describe a memristor-Hopfield Neural Network (mem-HNN) with massively parallel operations performed in a dense crossbar array. We provide experimental demonstrations solving NP-hard max-cut problems directly in analog crossbar arrays, and supplement this with experimentally-grounded simulations to explore scalability with problem size, providing the success probabilities, time and energy to solution, and interactions with intrinsic analog noise. Compared to fully digital approaches, and present-day quantum and optical accelerators, we forecast the mem-HNN to have over four orders of magnitude higher solution throughput per power consumption. This suggests substantially improved performance and scalability compared to current quantum annealing approaches, while operating at room temperature and taking advantage of existing CMOS technology augmented with emerging analog non-volatile memristors.},
  pubstate = {preprint},
  keywords = {Computer Science - Emerging Technologies},
  file = {C\:\\Users\\simon\\Zotero\\storage\\5854KZ2P\\Cai et al_2019_Harnessing Intrinsic Noise in Memristor Hopfield Neural Networks for.pdf;C\:\\Users\\simon\\Zotero\\storage\\GGP8JPV5\\1903.html}
}

@article{caiPowerefficientCombinatorialOptimization2020,
  title = {Power-Efficient Combinatorial Optimization Using Intrinsic Noise in Memristor {{Hopfield}} Neural Networks},
  author = {Cai, Fuxi and Kumar, Suhas and Van Vaerenbergh, Thomas and Sheng, Xia and Liu, Rui and Li, Can and Liu, Zhan and Foltin, Martin and Yu, Shimeng and Xia, Qiangfei and Yang, J. Joshua and Beausoleil, Raymond and Lu, Wei D. and Strachan, John Paul},
  date = {2020-07},
  journaltitle = {Nature Electronics},
  shortjournal = {Nat Electron},
  volume = {3},
  number = {7},
  pages = {409--418},
  publisher = {Nature Publishing Group},
  issn = {2520-1131},
  doi = {10.1038/s41928-020-0436-6},
  url = {https://www.nature.com/articles/s41928-020-0436-6},
  urldate = {2024-03-21},
  abstract = {To tackle important combinatorial optimization problems, a variety of annealing-inspired computing accelerators, based on several different technology platforms, have been proposed, including quantum-, optical- and electronics-based approaches. However, to be of use in industrial applications, further improvements in speed and energy efficiency are necessary. Here, we report a memristor-based annealing system that uses an energy-efficient neuromorphic architecture based on a Hopfield neural network. Our analogue–digital computing approach creates an optimization solver in which massively parallel operations are performed in a dense crossbar array that can inject the needed computational noise through the analogue array and device errors, amplified or dampened by using a novel feedback algorithm. We experimentally show that the approach can solve non-deterministic polynomial-time (NP)-hard max-cut problems by harnessing the intrinsic hardware noise. We also use experimentally grounded simulations to explore scalability with problem size, which suggest that our memristor-based approach can offer a solution throughput over four orders of magnitude higher per power consumption relative to current quantum, optical and fully digital approaches.},
  langid = {english},
  keywords = {Computational science,Electrical and electronic engineering,Electronic devices,Electronic properties and materials},
  file = {C:\Users\simon\Zotero\storage\F4SBPVXU\Cai et al_2020_Power-efficient combinatorial optimization using intrinsic noise in memristor.pdf}
}

@article{changDirectObservationDualFilament2017,
  title = {Direct {{Observation}} of {{Dual-Filament Switching Behaviors}} in {{Ta2O5-Based Memristors}}},
  author = {Chang, Chia-Fu and Chen, Jui-Yuan and Huang, Chun-Wei and Chiu, Chung-Hua and Lin, Ting-Yi and Yeh, Ping-Hung and Wu, Wen-Wei},
  date = {2017},
  journaltitle = {Small},
  volume = {13},
  number = {15},
  pages = {1603116},
  issn = {1613-6829},
  doi = {10.1002/smll.201603116},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smll.201603116},
  urldate = {2024-03-22},
  abstract = {The Forming phenomenon is observed via in situ transmission electron microscopy in the Ag/Ta2O5/Pt system. The device is switched to a low-resistance state as the dual filament is connected to the electrodes. The results of energy dispersive spectrometer and electron energy loss spectroscopy analyses demonstrate that the filament is composed by a stack of oxygen vacancies and Ag metal.},
  keywords = {dual filaments,hydroxide,in situ TEM,memristors,RRAM,Ta2O5},
  file = {C:\Users\simon\Zotero\storage\UJ4BKUM2\smll.html}
}

@inproceedings{charithaTypeIIDiabetesPrediction2022,
  title = {Type-{{II Diabetes Prediction Using Machine Learning Algorithms}}},
  booktitle = {2022 {{International Conference}} on {{Computer Communication}} and {{Informatics}} ({{ICCCI}})},
  author = {Charitha, C. and Devi Chaitrasree, Amuluru and Varma, Penmetsa Chidananda and Lakshmi, C.},
  date = {2022-01},
  pages = {1--5},
  issn = {2329-7190},
  doi = {10.1109/ICCCI54379.2022.9740844},
  url = {https://ieeexplore.ieee.org/document/9740844},
  urldate = {2024-04-12},
  abstract = {Non-Insulin Dependent Diabetes Mellitus or Type2 Diabetes is one of the critical diseases and many people are suffering from it. Every year, approximately 2 to 5 million people are losing their lives as Diabetics. If Diabetes is predicted earlier, it can be controlled and also, deadly risks such as diabetes cardiac stroke, nephropathy and other disorders associated with it can be prevented. Therefore, early prediction of diabetes helps in maintaining good health. With the recent development in machine learning (ML), it is being applied to various aspects of the medical health. The Pima Indian Diabetes data set (PID), which was used in this paper, was acquired from the UCI repository. In this study, after undergoing a thorough data pre-processing and Feature engineering with feature importance models like Random Forest Importance and RFE, we used many Machine Learning models such as KNN, Logistic Regression, SVM, Random Forest, LightGBM and XGBoost for train-test splits like 60–40, 70–30 and 80–20 to predict Type-II diabetes mellitus. Among all models, the highest accuracy is obtained as 91.47\% from lightGBM model for 80–20 train test split.},
  eventtitle = {2022 {{International Conference}} on {{Computer Communication}} and {{Informatics}} ({{ICCCI}})},
  keywords = {Computational modeling,Data models,Data Preprocessing,Diabetes,Diabetes Mellitus,Feature Engineering,Informatics,LightGBM,Machine Learning,Machine learning algorithms,Predictive models,Support vector machines},
  file = {C\:\\Users\\simon\\Zotero\\storage\\9Y2U5EVV\\Charitha et al_2022_Type-II Diabetes Prediction Using Machine Learning Algorithms.pdf;C\:\\Users\\simon\\Zotero\\storage\\N4948UF4\\9740844.html}
}

@inproceedings{charithaTypeIIDiabetesPrediction2022a,
  title = {Type-{{II Diabetes Prediction Using Machine Learning Algorithms}}},
  booktitle = {2022 {{International Conference}} on {{Computer Communication}} and {{Informatics}} ({{ICCCI}})},
  author = {Charitha, C. and Devi Chaitrasree, Amuluru and Varma, Penmetsa Chidananda and Lakshmi, C.},
  date = {2022-01},
  pages = {1--5},
  issn = {2329-7190},
  doi = {10.1109/ICCCI54379.2022.9740844},
  url = {https://ieeexplore.ieee.org/document/9740844},
  urldate = {2024-04-12},
  abstract = {Non-Insulin Dependent Diabetes Mellitus or Type2 Diabetes is one of the critical diseases and many people are suffering from it. Every year, approximately 2 to 5 million people are losing their lives as Diabetics. If Diabetes is predicted earlier, it can be controlled and also, deadly risks such as diabetes cardiac stroke, nephropathy and other disorders associated with it can be prevented. Therefore, early prediction of diabetes helps in maintaining good health. With the recent development in machine learning (ML), it is being applied to various aspects of the medical health. The Pima Indian Diabetes data set (PID), which was used in this paper, was acquired from the UCI repository. In this study, after undergoing a thorough data pre-processing and Feature engineering with feature importance models like Random Forest Importance and RFE, we used many Machine Learning models such as KNN, Logistic Regression, SVM, Random Forest, LightGBM and XGBoost for train-test splits like 60–40, 70–30 and 80–20 to predict Type-II diabetes mellitus. Among all models, the highest accuracy is obtained as 91.47\% from lightGBM model for 80–20 train test split.},
  eventtitle = {2022 {{International Conference}} on {{Computer Communication}} and {{Informatics}} ({{ICCCI}})},
  keywords = {Computational modeling,Data models,Data Preprocessing,Diabetes,Diabetes Mellitus,Feature Engineering,Informatics,LightGBM,Machine Learning,Machine learning algorithms,Predictive models,Support vector machines},
  file = {C\:\\Users\\simon\\Zotero\\storage\\67PJ6Y2N\\Charitha et al_2022_Type-II Diabetes Prediction Using Machine Learning Algorithms.pdf;C\:\\Users\\simon\\Zotero\\storage\\U4U3FZKE\\9740844.html}
}

@inproceedings{chenApplicationVoltageComparator2021,
  title = {Application of {{Voltage Comparator}} and Its {{Multisim Simulation}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Power}}, {{Intelligent Computing}} and {{Systems}} ({{ICPICS}})},
  author = {Chen, Ying and Zhang, Miao and Shen, Xuejing},
  date = {2021-07},
  pages = {28--30},
  doi = {10.1109/ICPICS52425.2021.9524134},
  url = {https://ieeexplore.ieee.org/abstract/document/9524134},
  urldate = {2024-03-25},
  abstract = {The voltage comparator which is composed by the integrated operational amplifier is a kind of common analog signal processor, which is widely used. Voltage comparator circuit of automobile charging system, which is composed of voltage comparator, is a kind of common electrical applications. Through the Multisim simulation software, we can deepen our understanding and grasp of the theoretical knowledge of the voltage comparator.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Power}}, {{Intelligent Computing}} and {{Systems}} ({{ICPICS}})},
  keywords = {Automobiles,Computational modeling,Conferences,Monitoring,Multisim,Operational amplifiers,Software,voltage comparator,voltage comparator circuit},
  file = {C\:\\Users\\simon\\Zotero\\storage\\TE6TC24A\\Chen et al_2021_Application of Voltage Comparator and its Multisim Simulation.pdf;C\:\\Users\\simon\\Zotero\\storage\\65GKLUYC\\9524134.html}
}

@inproceedings{chienReducingCarbonImpact2023,
  title = {Reducing the {{Carbon Impact}} of {{Generative AI Inference}} (Today and in 2035)},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Sustainable Computer Systems}}},
  author = {Chien, Andrew A and Lin, Liuzixuan and Nguyen, Hai and Rao, Varsha and Sharma, Tristan and Wijayawardana, Rajini},
  date = {2023-07-09},
  pages = {1--7},
  publisher = {ACM},
  location = {Boston MA USA},
  doi = {10.1145/3604930.3605705},
  url = {https://dl.acm.org/doi/10.1145/3604930.3605705},
  urldate = {2024-04-28},
  eventtitle = {{{HotCarbon}} '23: 2nd {{Workshop}} on {{Sustainable Computer Systems}}},
  isbn = {9798400702426},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\6ZRUD5J5\Chien et al_2023_Reducing the Carbon Impact of Generative AI Inference (today and in 2035).pdf}
}

@article{cichyDeepNeuralNetworks2019,
  title = {Deep {{Neural Networks}} as {{Scientific Models}}},
  author = {Cichy, Radoslaw M. and Kaiser, Daniel},
  date = {2019-04-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {4},
  eprint = {30795896},
  eprinttype = {pmid},
  pages = {305--317},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2019.01.009},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(19)30034-8},
  urldate = {2024-02-23},
  langid = {english},
  keywords = {deep learning,explanation,exploration,gelesen,neural network,prediction,scientific model,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\4CWZNGI3\Cichy_Kaiser_2019_Deep Neural Networks as Scientific Models.pdf}
}

@article{clinchGrandChallenges2022,
  title = {Grand {{Challenges}}},
  author = {Clinch, Sarah and Intille, Stephen},
  date = {2022-07-01},
  journaltitle = {IEEE Pervasive Computing},
  volume = {21},
  number = {03},
  pages = {7--8},
  publisher = {IEEE Computer Society},
  issn = {1536-1268},
  doi = {10.1109/MPRV.2022.3198813},
  url = {https://www.computer.org/csdl/magazine/pc/2022/03/09903273/1GZo7D5rB8A},
  urldate = {2024-02-15},
  abstract = {The articles in this special section focus on new applications for pervasive computing.},
  langid = {english},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\VZMMDAK3\Grand Challenges.pdf}
}

@online{daiOutsourcingClimateChange2021,
  type = {SSRN Scholarly Paper},
  title = {Outsourcing {{Climate Change}}},
  author = {Dai, Rui and Duan, Rui and Liang, Hao and Ng, Lilian},
  date = {2021-01-07},
  number = {3765485},
  location = {Rochester, NY},
  doi = {10.2139/ssrn.3765485},
  url = {https://papers.ssrn.com/abstract=3765485},
  urldate = {2024-03-10},
  abstract = {This paper examines how firms combat climate change and the motivations behind their strategies. Using firm-level carbon emissions and import volume data, we find pervasive evidence of firms outsourcing their emissions to foreign suppliers rather than investing in abatement—a strategy not fully explained by production offshoring, regulatory arbitrage, and supply chain shocks.  Instead,  our findings reveal that agency problems play a significant role in facilitating corporate carbon outsourcing.  While the outsourcing strategy improves short-term profitability, it adversely affects firm value and increases the cost of equity capital, suggesting that investors demand compensation for their exposure to such transition risks.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Carbon Premium,Green Technologies,Imports,Outsourcing Emissions,Reputational Risk,Stakeholders},
  file = {C:\Users\simon\Zotero\storage\N5B7L2HQ\Dai et al_2021_Outsourcing Climate Change.pdf}
}

@article{dallyEvolutionGraphicsProcessing2021,
  title = {Evolution of the {{Graphics Processing Unit}} ({{GPU}})},
  author = {Dally, William J. and Keckler, Stephen W. and Kirk, David B.},
  date = {2021-11},
  journaltitle = {IEEE Micro},
  volume = {41},
  number = {6},
  pages = {42--51},
  issn = {1937-4143},
  doi = {10.1109/MM.2021.3113475},
  url = {https://ieeexplore.ieee.org/abstract/document/9623445},
  urldate = {2024-03-20},
  abstract = {Graphics processing units (GPUs) power today’s fastest supercomputers, are the dominant platform for deep learning, and provide the intelligence for devices ranging from self-driving cars to robots and smart cameras. They also generate compelling photorealistic images at real-time frame rates. GPUs have evolved by adding features to support new use cases. NVIDIA’s GeForce 256, the first GPU, was a dedicated processor for real-time graphics, an application that demands large amounts of floating-point arithmetic for vertex and fragment shading computations and high memory bandwidth. As real-time graphics advanced, GPUs became programmable. The combination of programmability and floating-point performance made GPUs attractive for running scientific applications. Scientists found ways to use early programmable GPUs by casting their calculations as vertex and fragment shaders. GPUs evolved to meet the needs of scientific users by adding hardware for simpler programming, double-precision floating-point arithmetic, and resilience.},
  eventtitle = {{{IEEE Micro}}},
  keywords = {Graphics processing units},
  file = {C:\Users\simon\Zotero\storage\W3XBQE66\9623445.html}
}

@online{darioamodeiAICompute,
  title = {{{AI}} and Compute},
  author = {{Dario Amodei} and {Danny Hernandez}},
  url = {https://openai.com/research/ai-and-compute},
  urldate = {2024-02-15},
  abstract = {We’re releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore’s Law had a 2-year doubling period)[\^{}footnote-correction]. Since 2012, this metric has grown by more than 300,000x (a 2-year doubling period would yield only a 7x increase). Improvements in compute have been a key component of AI progress, so as long as this trend continues, it’s worth preparing for the implications of systems far outside today’s capabilities.},
  langid = {american},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\LW5E6CXF\ai-and-compute.html}
}

@article{demirkiranElectroPhotonicSystemAccelerating2023,
  title = {An {{Electro-Photonic System}} for {{Accelerating Deep Neural Networks}}},
  author = {Demirkiran, Cansu and Eris, Furkan and Wang, Gongyu and Elmhurst, Jonathan and Moore, Nick and Harris, Nicholas C. and Basumallik, Ayon and Reddi, Vijay Janapa and Joshi, Ajay and Bunandar, Darius},
  date = {2023-10-31},
  journaltitle = {ACM Journal on Emerging Technologies in Computing Systems},
  shortjournal = {J. Emerg. Technol. Comput. Syst.},
  volume = {19},
  number = {4},
  eprint = {2109.01126},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--31},
  issn = {1550-4832, 1550-4840},
  doi = {10.1145/3606949},
  url = {http://arxiv.org/abs/2109.01126},
  urldate = {2024-04-25},
  abstract = {The number of parameters in deep neural networks (DNNs) is scaling at about 5\$\textbackslash times\$ the rate of Moore's Law. To sustain this growth, photonic computing is a promising avenue, as it enables higher throughput in dominant general matrix-matrix multiplication (GEMM) operations in DNNs than their electrical counterpart. However, purely photonic systems face several challenges including lack of photonic memory and accumulation of noise. In this paper, we present an electro-photonic accelerator, ADEPT, which leverages a photonic computing unit for performing GEMM operations, a vectorized digital electronic ASIC for performing non-GEMM operations, and SRAM arrays for storing DNN parameters and activations. In contrast to prior works in photonic DNN accelerators, we adopt a system-level perspective and show that the gains while large are tempered relative to prior expectations. Our goal is to encourage architects to explore photonic technology in a more pragmatic way considering the system as a whole to understand its general applicability in accelerating today's DNNs. Our evaluation shows that ADEPT can provide, on average, 5.73\$\textbackslash times\$ higher throughput per Watt compared to the traditional systolic arrays (SAs) in a full-system, and at least 6.8\$\textbackslash times\$ and \$2.5\textbackslash times\$ better throughput per Watt, compared to state-of-the-art electronic and photonic accelerators, respectively.},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Hardware Architecture},
  file = {C\:\\Users\\simon\\Zotero\\storage\\8NA6QKHM\\Demirkiran et al_2023_An Electro-Photonic System for Accelerating Deep Neural Networks.pdf;C\:\\Users\\simon\\Zotero\\storage\\D847PLKB\\2109.html}
}

@online{DiscreteContinuousModels,
  title = {Discrete and {{Continuous Models}} and {{Applied Computational Science}}},
  url = {http://journals.rudn.ru/miph},
  urldate = {2024-04-08},
  abstract = {Discrete and Continuous Models and Applied Computational Science},
  langid = {english},
  organization = {Discrete and Continuous Models and Applied Computational Science},
  file = {C:\Users\simon\Zotero\storage\GQEE2Q99\miph.html}
}

@incollection{dramschChapterOne702020,
  title = {Chapter {{One}} - 70 Years of Machine Learning in Geoscience in Review},
  booktitle = {Advances in {{Geophysics}}},
  author = {Dramsch, Jesper Sören},
  editor = {Moseley, Ben and Krischer, Lion},
  date = {2020-01-01},
  series = {Machine {{Learning}} in {{Geosciences}}},
  volume = {61},
  pages = {1--55},
  publisher = {Elsevier},
  doi = {10.1016/bs.agph.2020.08.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0065268720300054},
  urldate = {2024-02-28},
  abstract = {This review gives an overview of the development of machine learning in geoscience. A thorough analysis of the codevelopments of machine learning applications throughout the last 70 years relates the recent enthusiasm for machine learning to developments in geoscience. I explore the shift of kriging toward a mainstream machine learning method and the historic application of neural networks in geoscience, following the general trend of machine learning enthusiasm through the decades. Furthermore, this chapter explores the shift from mathematical fundamentals and knowledge in software development toward skills in model validation, applied statistics, and integrated subject matter expertise. The review is interspersed with code examples to complement the theoretical foundations and illustrate model validation and machine learning explainability for science. The scope of this review includes various shallow machine learning methods, e.g., decision trees, random forests, support-vector machines, and Gaussian processes, as well as, deep neural networks, including feed-forward neural networks, convolutional neural networks, recurrent neural networks, and generative adversarial networks. Regarding geoscience, the review has a bias toward geophysics but aims to strike a balance with geochemistry, geostatistics, and geology, however, excludes remote sensing, as this would exceed the scope. In general, I aim to provide context for the recent enthusiasm surrounding deep learning with respect to research, hardware, and software developments that enable successful application of shallow and deep machine learning in all disciplines of Earth science.},
  keywords = {Deep learning,Earth science,Geology,Geophysics,Geoscience,Kriging,Machine learning,Neural networks,Review},
  file = {C\:\\Users\\simon\\Zotero\\storage\\L5UC2QWA\\Dramsch_2020_Chapter One - 70 years of machine learning in geoscience in review.pdf;C\:\\Users\\simon\\Zotero\\storage\\DSKCIMKK\\S0065268720300054.html}
}

@incollection{dreschDesignScienceResearch2015,
  title = {Design {{Science Research}}},
  booktitle = {Design {{Science Research}}: {{A Method}} for {{Science}} and {{Technology Advancement}}},
  author = {Dresch, Aline and Lacerda, Daniel Pacheco and Antunes, José Antônio Valle},
  editor = {Dresch, Aline and Lacerda, Daniel Pacheco and Antunes Jr, José Antônio Valle},
  date = {2015},
  pages = {67--102},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-07374-3_4},
  url = {https://doi.org/10.1007/978-3-319-07374-3_4},
  urldate = {2024-03-28},
  abstract = {This chapter presents the main concepts of design science research, which is a method that is conducted under the paradigm of design science to operationalize research. In addition to these concepts, the foundations for the application of design science research as a research method and the methods formalized by several authors for its operationalization are presented. A comparison of design science research with two alternate methods is performed. To prevent an exhaustive comparison in this book, we compare design science research with methods that are commonly used for qualitative research in Brazil: case study and action research.},
  isbn = {978-3-319-07374-3},
  langid = {english},
  keywords = {Design Cycle,Design Science,Evaluation Step,Focus Group,Fourth Step},
  file = {C:\Users\simon\Zotero\storage\XDZ47VG8\Dresch et al_2015_Design Science Research.pdf}
}

@online{duImplicitGenerationGeneralization2020,
  title = {Implicit {{Generation}} and {{Generalization}} in {{Energy-Based Models}}},
  author = {Du, Yilun and Mordatch, Igor},
  date = {2020-06-29},
  eprint = {1903.08689},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1903.08689},
  url = {http://arxiv.org/abs/1903.08689},
  urldate = {2024-02-23},
  abstract = {Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning,ungelesen,zitiert},
  file = {C\:\\Users\\simon\\Zotero\\storage\\RDJVNPK3\\Du_Mordatch_2020_Implicit Generation and Generalization in Energy-Based Models.pdf;C\:\\Users\\simon\\Zotero\\storage\\IRNZEZX7\\1903.html}
}

@online{duModelBasedPlanning2021,
  title = {Model {{Based Planning}} with {{Energy Based Models}}},
  author = {Du, Yilun and Lin, Toru and Mordatch, Igor},
  date = {2021-03-08},
  eprint = {1909.06878},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1909.06878},
  url = {http://arxiv.org/abs/1909.06878},
  urldate = {2024-02-19},
  abstract = {Model-based planning holds great promise for improving both sample efficiency and generalization in reinforcement learning (RL). We show that energy-based models (EBMs) are a promising class of models to use for model-based planning. EBMs naturally support inference of intermediate states given start and goal state distributions. We provide an online algorithm to train EBMs while interacting with the environment, and show that EBMs allow for significantly better online learning than corresponding feed-forward networks. We further show that EBMs support maximum entropy state inference and are able to generate diverse state space plans. We show that inference purely in state space - without planning actions - allows for better generalization to previously unseen obstacles in the environment and prevents the planner from exploiting the dynamics model by applying uncharacteristic action sequences. Finally, we show that online EBM training naturally leads to intentionally planned state exploration which performs significantly better than random exploration.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning,ungelesen,zitiert},
  file = {C\:\\Users\\simon\\Zotero\\storage\\VJYEAH2U\\Du et al_2021_Model Based Planning with Energy Based Models.pdf;C\:\\Users\\simon\\Zotero\\storage\\TVYW9SDL\\1909.html}
}

@article{durstewitzDeepNeuralNetworks2019,
  title = {Deep Neural Networks in Psychiatry},
  author = {Durstewitz, Daniel and Koppe, Georgia and Meyer-Lindenberg, Andreas},
  date = {2019-11},
  journaltitle = {Molecular Psychiatry},
  shortjournal = {Mol Psychiatry},
  volume = {24},
  number = {11},
  pages = {1583--1598},
  publisher = {Nature Publishing Group},
  issn = {1476-5578},
  doi = {10.1038/s41380-019-0365-9},
  url = {https://www.nature.com/articles/s41380-019-0365-9},
  urldate = {2024-02-23},
  abstract = {Machine and deep learning methods, today’s core of artificial intelligence, have been applied with increasing success and impact in many commercial and research settings. They are powerful tools for large scale data analysis, prediction and classification, especially in very data-rich environments (“big data”), and have started to find their way into medical applications. Here we will first give an overview of machine learning methods, with a focus on deep and recurrent neural networks, their relation to statistics, and the core principles behind them. We will then discuss and review directions along which (deep) neural networks can be, or already have been, applied in the context of psychiatry, and will try to delineate their future potential in this area. We will also comment on an emerging area that so far has been much less well explored: by embedding semantically interpretable computational models of brain dynamics or behavior into a statistical machine learning context, insights into dysfunction beyond mere prediction and classification may be gained. Especially this marriage of computational models with statistical inference may offer insights into neural and behavioral mechanisms that could open completely novel avenues for psychiatric treatment.},
  issue = {11},
  langid = {english},
  keywords = {gelesen,Neuroscience,Psychiatric disorders,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\RHCV9KWS\Durstewitz et al_2019_Deep neural networks in psychiatry.pdf}
}

@book{ebertSystematischesRequirementsEngineering2008,
  title = {Systematisches {{Requirements Engineering}} Und {{Management}} - {{Anforderungen}} Ermitteln, Spezifizieren, Analysieren Und Verwalten (2. {{Aufl}}.).},
  author = {Ebert, Christof},
  date = {2008-01-01},
  isbn = {978-3-89864-546-1},
  file = {C:\Users\simon\Zotero\storage\6CUJXR9S\Ebert_2008_Systematisches Requirements Engineering und Management - Anforderungen.pdf}
}

@online{EmissionsobergrenzenUndZertifikate,
  title = {Emissionsobergrenzen und ‑zertifikate - Europäische Kommission},
  url = {https://climate.ec.europa.eu/eu-action/eu-emissions-trading-system-eu-ets/emissions-cap-and-allowances_de},
  urldate = {2024-03-10},
  abstract = {StrategieDas Gesamtvolumen der Treibhausgasemissionen, die von unter das Emissionshandelssystem der EU (EU-EHS) fallenden Kraftwerken, Fabriken und...},
  langid = {ngerman},
  file = {C:\Users\simon\Zotero\storage\6QYRJ8ME\emissions-cap-and-allowances_de.html}
}

@online{EnvironmentallyResponsibleETF,
  title = {Environmentally {{Responsible ETF List}}},
  url = {https://etfdb.com/esg-investing/environmental-issues/},
  urldate = {2024-03-13},
  abstract = {Click to see more information on Environmentally Responsible ETFs including historical performance, dividends, holdings, expense ratios, ESG scores, technicals and more.},
  organization = {ETF Database},
  file = {C:\Users\simon\Zotero\storage\YRV6YH4F\environmental-issues.html}
}

@online{EnvironmentallyResponsibleETFa,
  title = {Environmentally {{Responsible ETF List}}},
  url = {https://etfdb.com/esg-investing/environmental-issues/},
  urldate = {2024-03-13},
  abstract = {Click to see more information on Environmentally Responsible ETFs including historical performance, dividends, holdings, expense ratios, ESG scores, technicals and more.},
  organization = {ETF Database},
  file = {C:\Users\simon\Zotero\storage\Q3YTEKPN\environmental-issues.html}
}

@online{EUCarbonPermits,
  title = {{{EU Carbon Permits}} - {{Price}} - {{Chart}} - {{Historical Data}} - {{News}}},
  url = {https://tradingeconomics.com/commodity/carbon},
  urldate = {2024-03-10},
  file = {C:\Users\simon\Zotero\storage\6Q6LIZPW\carbon.html}
}

@online{EuropaeischerGruenerDeal2023,
  title = {Ein europäischer Grüner Deal},
  date = {2023-12-20},
  url = {https://www.consilium.europa.eu/de/policies/green-deal/},
  urldate = {2024-03-10},
  abstract = {Was ist der europäische Grüne Deal und welche Initiativen gehören dazu? Der Grüne Deal ist die Strategie, mit der die EU ihr Ziel erreichen will, bis~2050 klimaneutral zu werden.},
  langid = {ngerman},
  file = {C:\Users\simon\Zotero\storage\DM9GJCUA\green-deal.html}
}

@book{fahlmanMassivelyParallelArchitectures1983,
  title = {Massively {{Parallel Architectures}} for {{AI}}: {{NETL}}, {{Thistle}}, and {{Boltzmann Machines}}.},
  shorttitle = {Massively {{Parallel Architectures}} for {{AI}}},
  author = {Fahlman, Scott and Hinton, Geoffrey and Sejnowski, Terrence},
  date = {1983-01-01},
  journaltitle = {[No source information available]},
  pages = {113},
  abstract = {ABSTRACT It is becoming,increasingly apparent that some,aspects of intelligent behavior require enormous ,computational power,and,that some,sort of massively parallel computing architecture is the most plausible way to deliver sueh power. Parallelism, rather than raw speed of the computing,elements. seems,to be the way that the brain gets such jobs done. But even if the need for massive parallelism is admitted, there is still the question of what kind of parallel architecture best fits the needs of various A1 tasks. In this paper we will attempt to isolate a number(\$\#\$\#\$\#CommaToBean intelligent system must perform. We will describe several families of massively parallel computing architectures, and we will see which of these computational tasks can be handled by each of these families. In particular, we will describe a new architecture, which we call the Boltzmann machine, whose abilities appear to include a number,of tasks that are inefficient},
  pagetotal = {109},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\5QXANLI6\1983 - Massively Parallel Architectures for Al NETL, Thistle, and Boltzmann Machines.pdf}
}

@inproceedings{fischerIntroductionRestrictedBoltzmann2012,
  title = {An {{Introduction}} to {{Restricted Boltzmann Machines}}},
  booktitle = {Progress in {{Pattern Recognition}}, {{Image Analysis}}, {{Computer Vision}}, and {{Applications}}},
  author = {Fischer, Asja and Igel, Christian},
  editor = {Alvarez, Luis and Mejail, Marta and Gomez, Luis and Jacobo, Julio},
  date = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {14--36},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-33275-3_2},
  abstract = {Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided.},
  isbn = {978-3-642-33275-3},
  langid = {english},
  keywords = {gelesen,Gibbs Sampling,Markov Chain,Markov Chain Monte Carlo,Markov Chain Monte Carlo Method,Restrict Boltzmann Machine,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\6E9KS4GX\Fischer_Igel_2012_An Introduction to Restricted Boltzmann Machines.pdf}
}

@article{gawlikowskiSurveyUncertaintyDeep2023,
  title = {A Survey of Uncertainty in Deep Neural Networks},
  author = {Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and Shahzad, Muhammad and Yang, Wen and Bamler, Richard and Zhu, Xiao Xiang},
  date = {2023-10-01},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {56},
  number = {1},
  pages = {1513--1589},
  issn = {1573-7462},
  doi = {10.1007/s10462-023-10562-9},
  url = {https://doi.org/10.1007/s10462-023-10562-9},
  urldate = {2024-02-23},
  abstract = {Over the last decade, neural networks have reached almost every field of science and become a crucial part of various real world applications. Due to the increasing spread, confidence in neural network predictions has become more and more important. However, basic neural networks do not deliver certainty estimates or suffer from over- or under-confidence, i.e. are badly calibrated. To overcome this, many researchers have been working on understanding and quantifying uncertainty in a neural network’s prediction. As a result, different types and sources of uncertainty have been identified and various approaches to measure and quantify uncertainty in neural networks have been proposed. This work gives a comprehensive overview of uncertainty estimation in neural networks, reviews recent advances in the field, highlights current challenges, and identifies potential research opportunities. It is intended to give anyone interested in uncertainty estimation in neural networks a broad overview and introduction, without presupposing prior knowledge in this field. For that, a comprehensive introduction to the most crucial sources of uncertainty is given and their separation into reducible model uncertainty and irreducible data uncertainty is presented. The modeling of these uncertainties based on deterministic neural networks, Bayesian neural networks (BNNs), ensemble of neural networks, and test-time data augmentation approaches is introduced and different branches of these fields as well as the latest developments are discussed. For a practical application, we discuss different measures of uncertainty, approaches for calibrating neural networks, and give an overview of existing baselines and available implementations. Different examples from the wide spectrum of challenges in the fields of medical image analysis, robotics, and earth observation give an idea of the needs and challenges regarding uncertainties in the practical applications of neural networks. Additionally, the practical limitations of uncertainty quantification methods in neural networks for mission- and safety-critical real world applications are discussed and an outlook on the next steps towards a broader usage of such methods is given.},
  langid = {english},
  keywords = {Bayesian deep neural networks,Calibration,Ensembles,gelesen,Test-time augmentation,Uncertainty,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\39QQTBNQ\Gawlikowski et al_2023_A survey of uncertainty in deep neural networks.pdf}
}

@article{georgeDigitalSustainabilityEntrepreneurship2021,
  title = {Digital {{Sustainability}} and {{Entrepreneurship}}: {{How Digital Innovations Are Helping Tackle Climate Change}} and {{Sustainable Development}}},
  shorttitle = {Digital {{Sustainability}} and {{Entrepreneurship}}},
  author = {George, Gerard and Merrill, Ryan K. and Schillebeeckx, Simon J. D.},
  date = {2021-09-01},
  journaltitle = {Entrepreneurship Theory and Practice},
  volume = {45},
  number = {5},
  pages = {999--1027},
  publisher = {SAGE Publications Inc},
  issn = {1042-2587},
  doi = {10.1177/1042258719899425},
  url = {https://doi.org/10.1177/1042258719899425},
  urldate = {2024-03-09},
  abstract = {We explore how digital technologies are helping address grand challenges to tackle climate change and promote sustainable development. With digital technologies, entrepreneurial organizations have adopted innovative approaches to tackle seemingly intractable societal challenges. We refer to these broadly as digital sustainability activities. By focusing on the digital toolbox employed by pioneering organizations, we propose a research agenda that generates novel questions for entrepreneurship, business models, and ecosystems as well as new ways of thinking about trust and institutional logics. We believe that digital sustainability can spur empirical advances in entrepreneurship, innovation, and strategy with potential for positive impact on society.},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\VU94C4B5\George et al_2021_Digital Sustainability and Entrepreneurship.pdf}
}

@article{georgeEnvironmentalImpactAI2023,
  title = {The {{Environmental Impact}} of {{AI}}: {{A Case Study}} of {{Water Consumption}} by {{Chat GPT}}},
  shorttitle = {The {{Environmental Impact}} of {{AI}}},
  author = {George, A. Shaji and George, A. S. Hovan and Martin, A. S. Gabrio},
  date = {2023-04-20},
  journaltitle = {Partners Universal International Innovation Journal},
  shortjournal = {Partners Universal International Innovation Journal},
  volume = {1},
  number = {2},
  pages = {97--104},
  issn = {2583-9675},
  doi = {10.5281/zenodo.7855594},
  url = {https://puiij.com/index.php/research/article/view/39},
  urldate = {2024-04-28},
  abstract = {As AI is becoming more a part of our lives, people are starting to worry about the negative consequences it might have on the environment. One of the major issues is its high water consumption. The water[21] consumption of AI models, including Chat GPT, is a major concern and must be managed effectively to reduce environmental harm. This document examines the amount[15] of water that is utilized by Chat GPT and other AI models and investigates the impact that it may have on the environment, as well as possible solutions to control their water usage. The study further considers the plausibility and usefulness of these approaches. The findings imply that although water usage of AI systems is significantly lower compared to other industries, it is still a matter of concern. AI models can have a significant water footprint, but this can be reduced by taking certain measures such as improving energy efficiency, utilizing renewable energy sources, optimizing algorithms and implementing strategies to conserve water. Despite the potential of these solutions, there are still issues to be addressed, such as the expense associated with implementation, and further research is required for optimum utilization. In conclusion, this document emphasizes the relevance of recognizing the water footprint caused by AI models, giving important details regarding potential solutions to minimize their environmental impact.},
  issue = {2},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\GLMQT5FN\George et al_2023_The Environmental Impact of AI.pdf}
}

@incollection{gerthKuenstlicheIntelligenzZwischen2023,
  title = {Künstliche Intelligenz zwischen Utopie und Realität: Aktuelle und zukünftige Entwicklungen von KI am Beispiel von Human-Machine-Interaction, Blockchain, Green Tech und Mobilität},
  shorttitle = {Künstliche Intelligenz zwischen Utopie und Realität},
  booktitle = {Entrepreneurship der Zukunft: Voraussetzung, Implementierung und Anwendung von Künstlicher Intelligenz im Rahmen datenbasierter Geschäftsmodelle},
  author = {Gerth, Sebastian and Heim, Lars},
  editor = {Heim, Lars and Gerth, Sebastian},
  date = {2023},
  pages = {421--458},
  publisher = {Springer Fachmedien},
  location = {Wiesbaden},
  doi = {10.1007/978-3-658-42060-4_17},
  url = {https://doi.org/10.1007/978-3-658-42060-4_17},
  urldate = {2024-02-15},
  abstract = {Der vorliegende Beitrag wagt auf der Basis vergangener und aktueller Entwicklungen von Künstlicher Intelligenz (KI) einen Ausblick auf zukünftige Leistungen von KI in spezifischen Gebieten. Es wird argumentiert, dass KI ein eigenständiger Megatrend mit zahlreichen Auswirkungen auf die Gesellschaft und v.~a. auch auf die Wirtschaft in unterschiedlichen Bereichen und Ebenen mit zahlreichen Anknüpfungspunkten diverser Geschäftsbereiche und -modelle ist. Es wird gezeigt, dass die wesentliche Stärke von KI einerseits die exakte Analyse und das zur Erreichung von vordefinierten Zielen optimale Inbeziehungsetzen spezifischer Parameter ist. Andererseits vermag KI bestimmte Prognosen zu fundieren. Aus diesem Grund wird der Chatbot ChatGPT des Entwicklers OpenAI zur Zukunft von KI befragt – eine KI trifft also Vorhersagen über KI. Dieser vielversprechende Ansatz wird dennoch kritisch reflektiert und der Beitrag mit einer Beleuchtung von starker und schwacher KI im Unternehmenskontext sowie einer prüfenden Deskription der bevorstehenden KI-Evolution abgeschlossen.},
  isbn = {978-3-658-42060-4},
  langid = {ngerman},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\F8N7CZ5G\Künstliche Intelligenz zwischen Utopie und Realitä.pdf}
}

@online{GitHubHtqinQuantSR,
  title = {{{GitHub}} - Htqin/{{QuantSR}}: {{This}} Project Is the Official Implementation of Our Accepted {{NeurIPS}} 2023 (Spotlight) Paper {{QuantSR}}: {{Accurate Low-bit Quantization}} for {{Efficient Image Super-Resolution}}.},
  shorttitle = {{{GitHub}} - Htqin/{{QuantSR}}},
  url = {https://github.com/htqin/QuantSR},
  urldate = {2024-04-19},
  abstract = {This project is the official implementation of our accepted NeurIPS 2023 (spotlight) paper QuantSR: Accurate Low-bit Quantization for Efficient Image Super-Resolution. - htqin/QuantSR},
  langid = {english},
  organization = {GitHub},
  file = {C:\Users\simon\Zotero\storage\ZJTER4C6\main.html}
}

@article{gmComprehensiveSurveyAnalysis2020,
  title = {A Comprehensive Survey and Analysis of Generative Models in Machine Learning},
  author = {Gm, Harshvardhan and Gourisaria, Mahendra Kumar and Pandey, Manjusha and Rautaray, Siddharth Swarup},
  date = {2020-11-01},
  journaltitle = {Computer Science Review},
  shortjournal = {Computer Science Review},
  volume = {38},
  pages = {100285},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2020.100285},
  url = {https://www.sciencedirect.com/science/article/pii/S1574013720303853},
  urldate = {2024-03-08},
  abstract = {Generative models have been in existence for many decades. In the field of machine learning, we come across many scenarios when directly learning a target is intractable through discriminative models, and in such cases the joint distribution of the target and the training data is approximated and generated. These generative models help us better represent or model a set of data by generating data in the form of Markov chains or simply employing a generative iterative process to do the same. With the recent innovation of Generative Adversarial Networks (GANs), it is now possible to make use of AI to generate pieces of art, music, etc. with a high extent of realism. In this paper, we review and analyse critically all the generative models, namely Gaussian Mixture Models (GMM), Hidden Markov Models (HMM), Latent Dirichlet Allocation (LDA), Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBN), Deep Boltzmann Machines (DBM), and GANs. We study their algorithms and implement each of the models to provide the reader some insights on which generative model to pick from while dealing with a problem. We also provide some noteworthy contributions done in the past to these models from the literature.},
  keywords = {Bayesian inference,Deep learning,Generative models,Machine learning,Neural networks},
  file = {C:\Users\simon\Zotero\storage\RASD8WLC\S1574013720303853.html}
}

@article{gomberDigitalFinanceFinTech2017,
  title = {Digital {{Finance}} and {{FinTech}}: Current Research and Future Research Directions},
  shorttitle = {Digital {{Finance}} and {{FinTech}}},
  author = {Gomber, Peter and Koch, Jascha-Alexander and Siering, Michael},
  date = {2017-07-01},
  journaltitle = {Journal of Business Economics},
  shortjournal = {J Bus Econ},
  volume = {87},
  number = {5},
  pages = {537--580},
  issn = {1861-8928},
  doi = {10.1007/s11573-017-0852-x},
  url = {https://doi.org/10.1007/s11573-017-0852-x},
  urldate = {2024-03-09},
  abstract = {Since decades, the financial industry has experienced a continuous evolution in service delivery due to digitalization. This evolution is characterized by expanded connectivity and enhanced speed of information processing both at the customer interface and in back-office processes. Recently, there has been a shift in the focus of digitalization from improving the delivery of traditional tasks to introducing fundamentally new business opportunities and models for financial service companies. Digital Finance encompasses a magnitude of new financial products, financial businesses, finance-related software, and novel forms of customer communication and interaction—delivered by FinTech companies and innovative financial service providers. Against this backdrop, the research on finance and information systems has started to analyze these changes and the impact of digital progress on the financial sector. Therefore, this article reviews the current state of research in Digital Finance that deals with these novel and innovative business functions. Moreover, it gives an outlook on potential future research directions. As a conceptual basis for reviewing this field, the Digital Finance Cube, which embraces three key dimensions of Digital Finance and FinTech, i.e., the respective business functions, the technologies and technological concepts applied as well as the institutions concerned, is introduced. This conceptualization supports researchers and practitioners when orientating in the field of Digital Finance, allows for the arrangement of academic research relatively to each other, and enables for the revelation of the gaps in research.},
  langid = {english},
  keywords = {Digital Finance,e-Finance,FinTech,Future research opportunities,G10,G21,G22,G23,G24,G28,Literature review,State of the art},
  file = {C:\Users\simon\Zotero\storage\FB29HGAG\Gomber et al_2017_Digital Finance and FinTech.pdf}
}

@article{gregorPositioningPresentingDesign2013,
  title = {Positioning and {{Presenting Design Science Research}} for {{Maximum Impact}}},
  author = {Gregor, Shirley and Hevner, Alan R.},
  date = {2013},
  journaltitle = {MIS Quarterly},
  volume = {37},
  number = {2},
  eprint = {43825912},
  eprinttype = {jstor},
  pages = {337--355},
  publisher = {Management Information Systems Research Center, University of Minnesota},
  issn = {0276-7783},
  url = {https://www.jstor.org/stable/43825912},
  urldate = {2024-03-28},
  abstract = {Design science research (DSR) has staked its rightful ground as an important and legitimate Information Systems (IS) research paradigm. We contend that DSR has yet to attain its full potential impact on the development and use of information systems due to gaps in the understanding and application of DSR concepts and methods. This essay aims to help researchers (1) appreciate the levels of artifact abstractions that may be DSR contributions, (2) identify appropriate ways of consuming and producing knowledge when they are preparing journal articles or other scholarly works, (3) understand and position the knowledge contributions of their research projects, and (4) structure a DSR article so that it emphasizes significant contributions to the knowledge base. Our focal contribution is the DSR knowledge contribution framework with two dimensions based on the existing state of knowledge in both the problem and solution domains for the research opportunity under study. In addition, we propose a DSR communication schema with similarities to more conventional publication patterns, but which substitutes the description of the DSR artifact in place of a traditional results section. We evaluate the DSR contribution framework and the DSR communication schema via examinations of DSR exemplar publications.},
  file = {C:\Users\simon\Zotero\storage\477R6UKA\Gregor_Hevner_2013_Positioning and Presenting Design Science Research for Maximum Impact.pdf}
}

@inproceedings{gushanskiyMethodModelingQuantum2020,
  title = {Method of {{Modeling Quantum Computations Using}} a {{Hardware Accelerator}}},
  booktitle = {2020 {{International Russian Automation Conference}} ({{RusAutoCon}})},
  author = {Gushanskiy, Sergey and Potapov, Viktor and Pukhovskiy, Valery},
  date = {2020-09},
  pages = {83--87},
  doi = {10.1109/RusAutoCon49822.2020.9208110},
  url = {https://ieeexplore.ieee.org/abstract/document/9208110},
  urldate = {2024-03-18},
  abstract = {During the study, software techniques for optimizing the quantum computer model were considered. The main goal of techniques that increase the efficiency of modeling is to reduce the amount of data that must be processed when modeling quantum gates. The most suitable algorithm for implementing a hardware accelerator was considered and modeled in the experimental part. Algorithms of graph theory or decision diagrams are not very suitable for this task, since their implementation will require the implementation of tensor matrix products, which is more convenient to do on a software simulator. It is also worth noting the increasingly growing interest in modeling quantum computing on special hardware, such as video cards or supercomputers. Therefore, the technique presented in the work is relevant, because it combines software solutions in the field of optimization over the past few years and recently studied methods for modeling quantum computing.},
  eventtitle = {2020 {{International Russian Automation Conference}} ({{RusAutoCon}})},
  keywords = {Computational modeling,emulator of quantum algorithms,entanglement,Hardware,Mathematical model,quantum algorithm,Quantum computing,Quantum mechanics,quantum parallelism,Random access memory,Registers,software},
  file = {C:\Users\simon\Zotero\storage\DDSNIG6E\9208110.html}
}

@inproceedings{gustafssonEnergyBasedModelsDeep2020,
  title = {Energy-{{Based Models}} for {{Deep Probabilistic Regression}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Gustafsson, Fredrik K. and Danelljan, Martin and Bhat, Goutam and Schön, Thomas B.},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {325--343},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-58565-5_20},
  abstract = {While deep learning-based classification is generally tackled using standardized approaches, a wide variety of techniques are employed for regression. In computer vision, one particularly popular such technique is that of confidence-based regression, which entails predicting a confidence value for each input-target pair (x,~y). While this approach has demonstrated impressive results, it requires important task-dependent design choices, and the predicted confidences lack a natural probabilistic meaning. We address these issues by proposing a general and conceptually simple regression method with a clear probabilistic interpretation. In our proposed approach, we create an energy-based model of the conditional target density p(y|x), using a deep neural network to predict the un-normalized density from (x,~y). This model of p(y|x) is trained by directly minimizing the associated negative log-likelihood, approximated using Monte Carlo sampling. We perform comprehensive experiments on four computer vision regression tasks. Our approach outperforms direct regression, as well as other probabilistic and confidence-based methods. Notably, our model achieves a \$\$2.2\textbackslash\%\$\$2.2\%AP improvement over Faster-RCNN for object detection on the COCO dataset, and sets a new state-of-the-art on visual tracking when applied for bounding box estimation. In contrast to confidence-based methods, our approach is also shown to be directly applicable to more general tasks such as age and head-pose estimation. Code is available at https://github.com/fregu856/ebms\_regression.},
  isbn = {978-3-030-58565-5},
  langid = {english},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\NAN4GFF3\Gustafsson et al_2020_Energy-Based Models for Deep Probabilistic Regression.pdf}
}

@online{helmenstineHowManyAtoms2022,
  title = {How {{Many Atoms Are}} in the {{World}}?},
  author = {Helmenstine, Anne},
  date = {2022-05-10T19:00:47+00:00},
  url = {https://sciencenotes.org/how-many-atoms-are-in-the-world/},
  urldate = {2024-02-21},
  abstract = {Learn how many atoms are in the world and how to perform the calculation. Compare this number to the number of way to order playing cards.},
  langid = {american},
  organization = {Science Notes and Projects},
  keywords = {gelesen,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\I5MHB32W\how-many-atoms-are-in-the-world.html}
}

@online{hernandezMeasuringAlgorithmicEfficiency2020,
  title = {Measuring the {{Algorithmic Efficiency}} of {{Neural Networks}}},
  author = {Hernandez, Danny and Brown, Tom B.},
  date = {2020-05-08},
  eprint = {2005.04305},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2005.04305},
  url = {http://arxiv.org/abs/2005.04305},
  urldate = {2024-02-15},
  abstract = {Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning,ungelesen},
  file = {C:\Users\simon\Zotero\storage\S692LSEN\Measuring the Algorithmic Efficiency of Neural Networks.pdf}
}

@article{hevnerDesignScienceInformation2004,
  title = {Design {{Science}} in {{Information Systems Research}}},
  author = {Hevner, Alan and R, Alan and March, Salvatore and T, Salvatore and {Park} and Park, Jinsoo and {Ram} and {Sudha}},
  date = {2004-03-01},
  journaltitle = {Management Information Systems Quarterly},
  shortjournal = {Management Information Systems Quarterly},
  volume = {28},
  pages = {75},
  abstract = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.},
  file = {C:\Users\simon\Zotero\storage\835IYJ8Z\Hevner et al_2004_Design Science in Information Systems Research.pdf}
}

@article{hevnerDesignScienceInformation2004a,
  title = {Design {{Science}} in {{Information Systems Research}}},
  author = {Hevner, Alan R. and March, Salvatore T. and Park, Jinsoo and Ram, Sudha},
  date = {2004},
  journaltitle = {MIS Quarterly},
  volume = {28},
  number = {1},
  eprint = {25148625},
  eprinttype = {jstor},
  pages = {75--105},
  publisher = {Management Information Systems Research Center, University of Minnesota},
  issn = {0276-7783},
  doi = {10.2307/25148625},
  url = {https://www.jstor.org/stable/25148625},
  urldate = {2024-03-28},
  abstract = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.},
  file = {C:\Users\simon\Zotero\storage\6PLSKXEF\Hevner et al_2004_Design Science in Information Systems Research.pdf}
}

@article{hevnerEnvisioningEntrepreneurshipDigital2022,
  title = {Envisioning Entrepreneurship and Digital Innovation through a Design Science Research Lens: {{A}} Matrix Approach},
  shorttitle = {Envisioning Entrepreneurship and Digital Innovation through a Design Science Research Lens},
  author = {Hevner, Alan and Gregor, Shirley},
  date = {2022-04-01},
  journaltitle = {Information \& Management},
  shortjournal = {Information \& Management},
  volume = {59},
  number = {3},
  pages = {103350},
  issn = {0378-7206},
  doi = {10.1016/j.im.2020.103350},
  url = {https://www.sciencedirect.com/science/article/pii/S0378720620302883},
  urldate = {2024-03-28},
  abstract = {Design Science Research (DSR) in the information systems (IS) field is, at its essence, about Digital Innovation (DI). Innovative sociotechnical design artifacts involve digital information technologies (IT) being used in ways that result in profound disruptions to traditional ways of doing business and to widespread societal changes. The pervasiveness of DI means that the individuals involved in bringing it about have diverse backgrounds, including application specialists, software engineers, data scientists, business managers, economists, venture capitalists, various user groups, and entrepreneurial leaders. This range of backgrounds means that DI, much more than traditional innovation, leads to varied perspectives on the methods and tools to be used in the development of effective and evolvable complex systems incorporating digital innovations. In this paper we present a new matrix approach to DI based on DSR, entrepreneurship, and innovation theories. Clear strategic guidance allows these multiple stakeholders to make sense of the diverse landscape and to understand when and how different entrepreneurial strategies for innovation can best be applied. We define the combined DSR and DI matrix approach in terms of four strategies: invention; advancement; exaptation; and exploitation and their associated DI practices. The research contribution is a novel DSR-DI matrix process model. This model extends entrepreneurship theory as it enriches effectuation thinking with more detailed process guidance for ambidextrous entrepreneurship and it enriches DSR models for DI by showing more explicitly the different pathways corresponding to different quadrants in the knowledge-innovation matrix.},
  keywords = {Design science research,Digital innovation,Entrepreneurship,Entrepreneurship strategies,Information technology,Knowledge,Sociotechnical systems},
  file = {C:\Users\simon\Zotero\storage\BAX9M69F\S0378720620302883.html}
}

@book{hintemannDataCenters20212022,
  title = {Data Centers 2021: {{Data}} Center Boom in {{Germany}} Continues - {{Cloud}} Computing Drives the Growth of the Data Center Industry and Its Energy Consumption},
  shorttitle = {Data Centers 2021},
  author = {Hintemann, Ralph and Hinterholzer, Simon},
  date = {2022-08-05},
  doi = {10.13140/RG.2.2.31826.43207},
  abstract = {The energy consumption of data centers continues to increase. At 17 billion kWh, data centers consumed 6.5 \% more electricity in 2021 than in 2020. The main reason for the growth in energy consumption is the expansion of cloud data centers in Germany and the associated increase in the number of large data centers. However, traditional data centers operated by companies themselves also continue to have a high share of data center capacities in Germany.},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\8S6LNT75\Data centers 2021 Data center boom in Germany continues.pdf}
}

@incollection{hintonBoltzmannMachines2014,
  title = {Boltzmann {{Machines}}},
  booktitle = {Encyclopedia of {{Machine Learning}} and {{Data Mining}}},
  author = {Hinton, Geoffrey},
  editor = {Sammut, Claude and Webb, Geoffrey I.},
  date = {2014},
  pages = {1--7},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-1-4899-7502-7_31-1},
  url = {https://link.springer.com/10.1007/978-1-4899-7502-7_31-1},
  urldate = {2024-03-16},
  isbn = {978-1-4899-7502-7},
  langid = {english},
  keywords = {gelesen,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\4JUU99AL\Hinton - 2014 - Boltzmann Machines.pdf}
}

@incollection{hintonPracticalGuideTraining2012,
  title = {A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {Hinton, Geoffrey E.},
  editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  date = {2012},
  volume = {7700},
  pages = {599--619},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-35289-8_32},
  url = {http://link.springer.com/10.1007/978-3-642-35289-8_32},
  urldate = {2024-02-15},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\HGALEWG5\Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf}
}

@incollection{hintonPracticalGuideTraining2012a,
  title = {A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {Hinton, Geoffrey E.},
  editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  date = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {599--619},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-35289-8_32},
  url = {https://doi.org/10.1007/978-3-642-35289-8_32},
  urldate = {2024-02-15},
  abstract = {Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.},
  isbn = {978-3-642-35289-8},
  langid = {english},
  keywords = {gelesen,Hide Unit,Learning Rate,Reconstruction Error,Restrict Boltzmann Machine,Training Case,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\VZ5ZDZTH\Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf}
}

@online{hizzaniMemristorbasedHardwareAlgorithms2023,
  title = {Memristor-Based Hardware and Algorithms for Higher-Order {{Hopfield}} Optimization Solver Outperforming Quadratic {{Ising}} Machines},
  author = {Hizzani, Mohammad and Heittmann, Arne and Hutchinson, George and Dobrynin, Dmitrii and Van Vaerenbergh, Thomas and Bhattacharya, Tinish and Renaudineau, Adrien and Strukov, Dmitri and Strachan, John Paul},
  date = {2023-11-02},
  eprint = {2311.01171},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.01171},
  url = {http://arxiv.org/abs/2311.01171},
  urldate = {2024-02-15},
  abstract = {Ising solvers offer a promising physics-based approach to tackle the challenging class of combinatorial optimization problems. However, typical solvers operate in a quadratic energy space, having only pair-wise coupling elements which already dominate area and energy. We show that such quadratization can cause severe problems: increased dimensionality, a rugged search landscape, and misalignment with the original objective function. Here, we design and quantify a higher-order Hopfield optimization solver, with 28nm CMOS technology and memristive couplings for lower area and energy computations. We combine algorithmic and circuit analysis to show quantitative advantages over quadratic Ising Machines (IM)s, yielding 48x and 72x reduction in time-to-solution (TTS) and energy-to-solution (ETS) respectively for Boolean satisfiability problems of 150 variables, with favorable scaling.},
  pubstate = {preprint},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Hardware Architecture,ungelesen},
  file = {C:\Users\simon\Zotero\storage\UWNBRWXG\Hizzani et al. - 2023 - Memristor-based hardware and algorithms for higher.pdf}
}

@online{hizzaniMemristorbasedHardwareAlgorithms2023a,
  title = {Memristor-Based Hardware and Algorithms for Higher-Order {{Hopfield}} Optimization Solver Outperforming Quadratic {{Ising}} Machines},
  author = {Hizzani, Mohammad and Heittmann, Arne and Hutchinson, George and Dobrynin, Dmitrii and Van Vaerenbergh, Thomas and Bhattacharya, Tinish and Renaudineau, Adrien and Strukov, Dmitri and Strachan, John Paul},
  date = {2023-11-02},
  eprint = {2311.01171},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.01171},
  url = {http://arxiv.org/abs/2311.01171},
  urldate = {2024-03-21},
  abstract = {Ising solvers offer a promising physics-based approach to tackle the challenging class of combinatorial optimization problems. However, typical solvers operate in a quadratic energy space, having only pair-wise coupling elements which already dominate area and energy. We show that such quadratization can cause severe problems: increased dimensionality, a rugged search landscape, and misalignment with the original objective function. Here, we design and quantify a higher-order Hopfield optimization solver, with 28nm CMOS technology and memristive couplings for lower area and energy computations. We combine algorithmic and circuit analysis to show quantitative advantages over quadratic Ising Machines (IM)s, yielding 48x and 72x reduction in time-to-solution (TTS) and energy-to-solution (ETS) respectively for Boolean satisfiability problems of 150 variables, with favorable scaling.},
  pubstate = {preprint},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Hardware Architecture},
  file = {C\:\\Users\\simon\\Zotero\\storage\\M3NQPYH7\\Hizzani et al_2023_Memristor-based hardware and algorithms for higher-order Hopfield optimization.pdf;C\:\\Users\\simon\\Zotero\\storage\\GPICDJD6\\2311.html}
}

@book{holzweissigWissenschaftlichesArbeiten2017,
  title = {Wissenschaftliches Arbeiten},
  author = {Holzweißig, Kai},
  date = {2017-06-23T15:39:00},
  publisher = {Leanpub},
  url = {https://leanpub.next/wawinfo},
  urldate = {2024-04-26},
  abstract = {Eine Anleitung für das wissenschaftliche Arbeiten für dual Studierende der Wirtschaftsinformatik.},
  langid = {german},
  file = {C:\Users\simon\Zotero\storage\4ZZEG4TX\wawinfo.html}
}

@article{hopfieldNeuralNetworksPhysical1982,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities.},
  author = {Hopfield, J J},
  date = {1982-04},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {79},
  number = {8},
  pages = {2554--2558},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.79.8.2554},
  url = {https://www.pnas.org/doi/10.1073/pnas.79.8.2554},
  urldate = {2024-02-19},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\BF4BJIPP\Hopfield_1982_Neural networks and physical systems with emergent collective computational.pdf}
}

@article{hsuEffectsGenerativeDiscriminative2010,
  title = {Effects of {{Generative}} and {{Discriminative Learning}} on {{Use}} of {{Category Variability}}},
  author = {Hsu, Anne and Griffiths, Thomas},
  date = {2010-02-01},
  journaltitle = {Proceedings of 32nd Annual Conference of the Cognitive Science Society},
  shortjournal = {Proceedings of 32nd Annual Conference of the Cognitive Science Society},
  abstract = {Rational models of category learning can take two different approaches to representing the relationship between objects and categories. The generative approach solves the categorization problem by building a probabilistic model of each category and using Bayes’ rule to infer category labels. In contrast, the discriminative approach directly learns a mapping between inputs and category labels. With this distinction in mind, we revisit a previously studied categorization experiment that showed people are biased towards categorizing objects into a category with higher variability. Modelling results predict that generative learners should be more greatly affected by category variability than discriminative learners. We show that humans can be prompted to adopt either a generative or discriminative approach to learning the same input, resulting in the predicted effect on use of category variability.}
}

@incollection{huberIntroductionMetropolisRosenbluth1997,
  title = {Introduction to {{Metropolis}}, {{Rosenbluth}}, {{Rosenbluth}}, {{Teller}}, and {{Teller}} (1953) {{Equations}} of {{State Calculations}} by {{Fast Computing Machines}}. {{J}}. {{Chem}}. {{Phys}}.,21, 1087–1092. and {{Geman}} and {{Geman}} (1984) {{Stochastic Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}. {{IEEE Trans}}. {{Pattern Anal}}. {{Machine Intelligence}},6, 721–741.},
  booktitle = {Breakthroughs in {{Statistics}}},
  author = {Huber, Peter J.},
  editor = {Kotz, Samuel and Johnson, Norman L.},
  date = {1997},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {123--139},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4612-0667-5_6},
  url = {https://doi.org/10.1007/978-1-4612-0667-5_6},
  urldate = {2024-02-27},
  abstract = {The breakthrough to be discussed here occurred in two steps, separated in time by over three decades. The first, conceptually decisive step, but lacking direct relevance to mainstream statistics, was the invention of Markov Chain Monte Carlo methods by Metropolis et al. (1953). The second step, decisive for applications in statistics, occurred when Geman and Geman (1984) wrestled the seminal idea of Metropolis et al. from its statistical mechanics surroundings, modified it, and applied it to Bayesian modeling and the computation of posterior distributions in otherwise intractable situations.},
  isbn = {978-1-4612-0667-5},
  langid = {english},
  keywords = {Configuration Space,Markov Chain Monte Carlo Method,Markov Random Fields,Radial Distribution Function,Virial Coefficient}
}

@article{huembeliPhysicsEnergybasedModels2022,
  title = {The Physics of Energy-Based Models},
  author = {Huembeli, Patrick and Arrazola, Juan Miguel and Killoran, Nathan and Mohseni, Masoud and Wittek, Peter},
  date = {2022-01-06},
  journaltitle = {Quantum Machine Intelligence},
  shortjournal = {Quantum Mach. Intell.},
  volume = {4},
  number = {1},
  pages = {1},
  issn = {2524-4914},
  doi = {10.1007/s42484-021-00057-7},
  url = {https://doi.org/10.1007/s42484-021-00057-7},
  urldate = {2024-02-19},
  abstract = {Energy-based models (EBMs) are experiencing a resurgence of interest in both the physics community and the machine learning community. This article provides an intuitive introduction to EBMs, without requiring any background in machine learning, connecting elementary concepts from physics with basic concepts and tools in generative models, and finally giving a perspective where current research in the field is heading. This article, in its original form, was written as an online lecture note in HTML and Javascript and contains interactive graphics. We recommend the reader to also visit the interactive version.},
  langid = {english},
  keywords = {Energy-based models,gelesen,Gibbs sampling,Machine learning,Spin glasses,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\5SE9BGQ4\The physics of energy-based models.pdf}
}

@inproceedings{huSurveyConvolutionalNeural2022,
  title = {A {{Survey}} on {{Convolutional Neural Network Accelerators}}: {{GPU}}, {{FPGA}} and {{ASIC}}},
  shorttitle = {A {{Survey}} on {{Convolutional Neural Network Accelerators}}},
  booktitle = {2022 14th {{International Conference}} on {{Computer Research}} and {{Development}} ({{ICCRD}})},
  author = {Hu, Yunxiang and Liu, Yuhao and Liu, Zhuovuan},
  date = {2022-01},
  pages = {100--107},
  doi = {10.1109/ICCRD54409.2022.9730377},
  url = {https://ieeexplore.ieee.org/abstract/document/9730377},
  urldate = {2024-03-19},
  abstract = {In recent years, artificial intelligence (AI) has been under rapid development, applied in various areas. Among a vast number of neural network (NN) models, the convolutional neural network (CNN) has a mainstream status in application such as image and sound recognition and machine decision. The convolution operation is the most complex and requires acceleration. A practical method is to optimize the architecture of the deep learning processor (DLP). The traditional CPU architecture lacks parallelism and memory bandwidth and is not suitable for CNN operations. Current researches are focused on graphic processing unit (GPU), field programmable gate array (FPGA) and application specific integrated circuit (ASIC). GPU is the maturest and the most widely applied, however it is not flexible and has high cost and energy consumption. Even though FPGA possesses high flexibility and low energy consumption, it is inferior in performance. ASIC, due to targeted design, is advanced in performance and energy consumption. However, it is highly inflexible. This article reviews the research outcomes of the three classic types of processors applied to CNN, and put forward the future research trend. In particular, this paper analyzes and compares the experimental performance of several processors of different types, and then summarizes the respective advantageous application fields. Hence, the novelty of this article is in the summary of practical DLPs, which is expected to provide helps for the AI researchers, and guide the selection of CNN-supporting hardware in industrial application.},
  eventtitle = {2022 14th {{International Conference}} on {{Computer Research}} and {{Development}} ({{ICCRD}})},
  keywords = {Application specific integrated circuits,ASIC,Computer architecture,convolutional neural network,Deep learning,deep learning accelerator,Energy consumption,FPGA,GPU,Graphics processing units,Market research,Training},
  file = {C\:\\Users\\simon\\Zotero\\storage\\AZ2KN6IP\\Hu et al_2022_A Survey on Convolutional Neural Network Accelerators.pdf;C\:\\Users\\simon\\Zotero\\storage\\V7IPZZFL\\9730377.html}
}

@online{InternationalerCO2MarktEuropaeische,
  title = {Internationaler CO2-Markt - Europäische Kommission},
  url = {https://climate.ec.europa.eu/eu-action/eu-emissions-trading-system-eu-ets/international-carbon-market_de},
  urldate = {2024-03-10},
  abstract = {StrategieInternationale CO2-Märkte können bei der kosteneffizienten Verringerung der weltweiten Treibhausgasemissionen eine wesentliche Rolle spielen...},
  langid = {ngerman},
  file = {C:\Users\simon\Zotero\storage\M7E6LAZ5\international-carbon-market_de.html}
}

@article{isingBeitragZurTheorie1925,
  title = {Beitrag zur Theorie des Ferromagnetismus},
  author = {Ising, Ernst},
  date = {1925-02-01},
  journaltitle = {Zeitschrift für Physik},
  shortjournal = {Z. Physik},
  volume = {31},
  number = {1},
  pages = {253--258},
  issn = {0044-3328},
  doi = {10.1007/BF02980577},
  url = {https://doi.org/10.1007/BF02980577},
  urldate = {2024-03-21},
  langid = {ngerman},
  file = {C:\Users\simon\Zotero\storage\6PRA279U\Ising_1925_Beitrag zur Theorie des Ferromagnetismus.pdf}
}

@incollection{izadkhahNPNPCompleteNPHard2022,
  title = {P, {{NP}}, {{NP-Complete}}, and~{{NP-Hard Problems}}},
  booktitle = {Problems on {{Algorithms}}: {{A Comprehensive Exercise Book}} for {{Students}} in {{Software Engineering}}},
  author = {Izadkhah, Habib},
  editor = {Izadkhah, Habib},
  date = {2022},
  pages = {497--511},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-17043-0_15},
  url = {https://doi.org/10.1007/978-3-031-17043-0_15},
  urldate = {2024-03-21},
  abstract = {The theory of computational complexity examines the difficulty of solving problems by computer (more precisely, algorithmically). In this theory, the complexity of problem definitions is classified into two sets; P which denotes “Polynomial” time and NP which indicates “Non-deterministic Polynomial” time. There are also NP-Hard and NP-Complete sets, which we use to express more complex problems. This chapter provides 87 exercises for addressing the theory of computational complexity.},
  isbn = {978-3-031-17043-0},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\Q7PFEYG3\Izadkhah_2022_P, NP, NP-Complete, and NP-Hard Problems.pdf}
}

@online{jacksonAIBoomWill2024,
  title = {{{AI Boom Will Cause Data Centre Electricity Demand}} to {{Double}}},
  author = {Jackson, Amber},
  date = {2024-01-24T14:00:28Z},
  url = {https://datacentremagazine.com/data-centres/ai-boom-will-cause-data-centre-electricity-demand-to-double},
  urldate = {2024-04-26},
  abstract = {IEA Expects Global Electricity Demand, Driven by AI Growth, to Double by 2026 - Citing the Importance of Sustainability and Net Zero Investments},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\7K5KCCTL\ai-boom-will-cause-data-centre-electricity-demand-to-double.html}
}

@article{javaidReviewBlockchainTechnology2022,
  title = {A Review of {{Blockchain Technology}} Applications for Financial Services},
  author = {Javaid, Mohd and Haleem, Abid and Singh, Ravi Pratap and Suman, Rajiv and Khan, Shahbaz},
  date = {2022-07-01},
  journaltitle = {BenchCouncil Transactions on Benchmarks, Standards and Evaluations},
  shortjournal = {BenchCouncil Transactions on Benchmarks, Standards and Evaluations},
  volume = {2},
  number = {3},
  pages = {100073},
  issn = {2772-4859},
  doi = {10.1016/j.tbench.2022.100073},
  url = {https://www.sciencedirect.com/science/article/pii/S2772485922000606},
  urldate = {2024-03-10},
  abstract = {Financial service providers find blockchain technology useful to enhance authenticity, security, and risk management. Several institutions are adopting blockchain in trade and finance systems to build smart contracts between participants, improve efficiency and transparency, and open up newer revenue opportunities. Blockchain’s unique recording capabilities make the existing clearing and settlement process redundant. Banks and other financial entities are adopting blockchain-enabled IDs to identify people. Better results come from organisations’ capacity to foresee emerging trends in financial blockchain applications and develop blockchain functionality. The transfer of asset ownership and addressing the maintenance of a precise financial ledger. Measurement, communication, and analysis of financial information are three significant areas to be focussed on by accounting professionals. Blockchain clarifies asset ownership and the existence of obligations for accountants, and it has the potential to improve productivity. This paper identifies and studies relevant articles related to blockchain for finance. This paper focuses on Blockchain technology and its importance for financial services. Further takes up various tools, strategies, and featured services in Blockchain-based financial services. Finally, the paper identifies and evaluates the significant applications of Blockchain technology in financial services. Credit reports significantly impact the financial lives of customers. Recent data breaches demonstrate the superior security of blockchain-based credit reporting over conventional server-based reporting. Blockchain-based systems enable the faster, more cost-effective, and more customised issuance of digital securities. With its adoption, the market for investors can be expanded, costs for issuers can be reduced, and counterparty risk can be reduced due to the ability to customise digital financial instruments to the demands of investors. It uses mutualised standards, protocols, and shared procedures to give network users a single common source of truth. Participants in the business network can now more easily collaborate, manage data, and agree with this technology’s application.},
  keywords = {Banking,Blockchain,Customer,Finance,Transaction},
  file = {C:\Users\simon\Zotero\storage\E7JKABT2\S2772485922000606.html}
}

@article{kellnerSoftwareProcessSimulation1999,
  title = {Software Process Simulation Modeling: {{Why}}? {{What}}? {{How}}?},
  shorttitle = {Software Process Simulation Modeling},
  author = {Kellner, Marc I and Madachy, Raymond J and Raffo, David M},
  date = {1999-04-15},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  volume = {46},
  number = {2},
  pages = {91--105},
  issn = {0164-1212},
  doi = {10.1016/S0164-1212(99)00003-5},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121299000035},
  urldate = {2024-04-02},
  abstract = {Software process simulation modeling is increasingly being used to address a variety of issues from the strategic management of software development, to supporting process improvements, to software project management training. The scope of software process simulation applications ranges from narrow focused portions of the life cycle to longer term product evolutionary models with broad organizational impacts. This article provides an overview of work being conducted in this field. It identifies the questions and issues that simulation can be used to address (`why'), the scope and variables that can be usefully simulated (`what'), and the modeling approaches and techniques that can be most productively employed (`how'). It includes a summary of the papers in this special issue of the Journal of Systems and Software, which were presented at the First International Silver Falls Workshop on Software Process Simulation Modeling (ProSim'98). It also provides a framework that helps characterize work in this field, and applies this new characterization scheme to many of the articles in this special issue. This paper concludes by offering some guidance in selecting a simulation modeling approach for practical application, and recommending some issues warranting additional research.},
  file = {C:\Users\simon\Zotero\storage\CEGDAX75\S0164121299000035.html}
}

@article{kikstraIPCCSixthAssessment2022,
  title = {The {{IPCC Sixth Assessment Report WGIII}} Climate Assessment of Mitigation Pathways: From Emissions to Global Temperatures},
  shorttitle = {The {{IPCC Sixth Assessment Report WGIII}} Climate Assessment of Mitigation Pathways},
  author = {Kikstra, Jarmo S. and Nicholls, Zebedee R. J. and Smith, Christopher J. and Lewis, Jared and Lamboll, Robin D. and Byers, Edward and Sandstad, Marit and Meinshausen, Malte and Gidden, Matthew J. and Rogelj, Joeri and Kriegler, Elmar and Peters, Glen P. and Fuglestvedt, Jan S. and Skeie, Ragnhild B. and Samset, Bjørn H. and Wienpahl, Laura and family=Vuuren, given=Detlef P., prefix=van, useprefix=true and family=Wijst, given=Kaj-Ivar, prefix=van der, useprefix=true and Al Khourdajie, Alaa and Forster, Piers M. and Reisinger, Andy and Schaeffer, Roberto and Riahi, Keywan},
  date = {2022-12-20},
  journaltitle = {Geoscientific Model Development},
  volume = {15},
  number = {24},
  pages = {9075--9109},
  publisher = {Copernicus GmbH},
  issn = {1991-959X},
  doi = {10.5194/gmd-15-9075-2022},
  url = {https://gmd.copernicus.org/articles/15/9075/2022/},
  urldate = {2024-03-10},
  abstract = {While the Intergovernmental Panel on Climate Change (IPCC) physical science reports usually assess a handful of future scenarios, the Working Group III contribution on climate mitigation to the IPCC's Sixth Assessment Report (AR6 WGIII) assesses hundreds to thousands of future emissions scenarios. A key task in WGIII is to assess the global mean temperature outcomes of these scenarios in a consistent manner, given the challenge that the emissions scenarios from different integrated assessment models (IAMs) come with different sectoral and gas-to-gas coverage and cannot all be assessed consistently by complex Earth system models. In this work, we describe the “climate-assessment” workflow and its methods, including infilling of missing emissions and emissions harmonisation as applied to 1202 mitigation scenarios in AR6 WGIII. We evaluate the global mean temperature projections and effective radiative forcing (ERF) characteristics of climate emulators FaIRv1.6.2 and MAGICCv7.5.3 and use the CICERO simple climate model (CICERO-SCM) for sensitivity analysis. We discuss the implied overshoot severity of the mitigation pathways using overshoot degree years and look at emissions and temperature characteristics of scenarios compatible with one possible interpretation of the Paris Agreement. We find that the lowest class of emissions scenarios that limit global warming to “1.5 ∘C (with a probability of greater than 50 \%) with no or limited overshoot” includes 97 scenarios for MAGICCv7.5.3 and 203 for FaIRv1.6.2. For the MAGICCv7.5.3 results, “limited overshoot” typically implies exceedance of median temperature projections of up to about 0.1 ∘C for up to a few decades before returning to below 1.5 ∘C by or before the year 2100. For more than half of the scenarios in this category that comply with three criteria for being “Paris-compatible”, including net-zero or net-negative greenhouse gas (GHG) emissions, median temperatures decline by about 0.3–0.4 ∘C after peaking at 1.5–1.6 ∘C in 2035–2055. We compare the methods applied in AR6 with the methods used for SR1.5 and discuss their implications. This article also introduces a “climate-assessment” Python package which allows for fully reproducing the IPCC AR6 WGIII temperature assessment. This work provides a community tool for assessing the temperature outcomes of emissions pathways and provides a basis for further work such as extending the workflow to include downscaling of climate characteristics to a regional level and calculating impacts.},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\MH8EWX2X\Kikstra et al_2022_The IPCC Sixth Assessment Report WGIII climate assessment of mitigation pathways.pdf}
}

@inproceedings{larochelleClassificationUsingDiscriminative2008,
  title = {Classification Using Discriminative Restricted {{Boltzmann}} Machines},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  author = {Larochelle, Hugo and Bengio, Yoshua},
  date = {2008-07-05},
  series = {{{ICML}} '08},
  pages = {536--543},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1390156.1390224},
  url = {https://dl.acm.org/doi/10.1145/1390156.1390224},
  urldate = {2024-02-22},
  abstract = {Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.},
  isbn = {978-1-60558-205-4},
  keywords = {gelesen,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\EPN7UF5H\Larochelle_Bengio_2008_Classification using discriminative restricted Boltzmann machines.pdf}
}

@article{larochelleLearningAlgorithmsClassification2012,
  title = {Learning Algorithms for the Classification Restricted {{Boltzmann}} Machine},
  author = {Larochelle, Hugo and Mandel, Michael and Pascanu, Razvan and Bengio, Y.},
  date = {2012-03-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {The Journal of Machine Learning Research},
  volume = {13},
  pages = {643--669},
  abstract = {Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely semi-supervised and multitask learning.},
  keywords = {gelesen,ungelesen},
  file = {C:\Users\simon\Zotero\storage\BE788SPH\Larochelle et al_2012_Learning algorithms for the classification restricted Boltzmann machine.pdf}
}

@article{leeDigitalizationAchieveTechnology2022,
  title = {Digitalization to {{Achieve Technology Innovation}} in {{Climate Technology Transfer}}},
  author = {Lee, Woo-Jin and Mwebaza, Rose},
  date = {2022-01},
  journaltitle = {Sustainability},
  volume = {14},
  number = {1},
  pages = {63},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2071-1050},
  doi = {10.3390/su14010063},
  url = {https://www.mdpi.com/2071-1050/14/1/63},
  urldate = {2024-03-09},
  abstract = {Technology Innovation has the potential to play a strategic role in improving the effectiveness and efficiency of national efforts to address climate change. The United Nations (UN) Climate Technology Centre and Network (CTCN) is mandated to support developing countries’ climate change responses through innovative technologies to achieve the goals of the Paris Agreement. In order to enhance the role of the CTCN as an innovation matchmaker, it is important to explore and leverage the implementation potential of new digital technologies and their transformational impact. Thus, in this research, to engage digitalization as an innovative tool with the environment, we first explored digitalization during the climate technology transfer processes by comprehensively reviewing CTCN Technical Assistance (Digitalization Technical Assistance, D-TA) activities in three climate sectors of risk prediction, policy decision making, and resource optimization. Then, by applying analytical methodologies of in-depth interviews with major digital-climate stakeholders and a staged model for technology innovation, we propose future strategies for enhancing the role of CTCN as an innovation matchmaker in the three digitalization cases of digital collection, digital analysis, and digital diffusion.},
  issue = {1},
  langid = {english},
  keywords = {digitalization,matchmaker,technology innovation,technology transfer},
  file = {C:\Users\simon\Zotero\storage\CNEQZUHN\Lee_Mwebaza_2022_Digitalization to Achieve Technology Innovation in Climate Technology Transfer.pdf}
}

@article{lehnertMostResourceEfficient2023,
  title = {Most {{Resource Efficient Matrix Vector Multiplication}} on {{FPGAs}}},
  author = {Lehnert, Alexander and Holzinger, Philipp and Pfenning, Simon and Müller, Ralf and Reichenbach, Marc},
  date = {2023},
  journaltitle = {IEEE Access},
  volume = {11},
  pages = {3881--3898},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3234622},
  url = {https://ieeexplore.ieee.org/document/10007836?denied=},
  urldate = {2024-03-16},
  abstract = {Fast and resource-efficient inference in artificial neural networks (ANNs) is of utmost importance and drives many new developments in the area of new hardware architectures, e.g., by means of systolic arrays or algorithmic optimization such as pruning. In this paper, we present a novel method for lowering the computation effort for ANN inference utilizing ideas from information theory. Weight matrices are sliced into submatrices of logarithmic aspect ratios. These slices are then factorized. This reduces the number of required computations without compromising on fully parallel processing. We create a new hardware architecture for this dedicated purpose. We also provide a tool to map these sliced and factorized matrices efficiently to reconfigurable hardware. By comparing to the state of the art FPGA implementations, we can prove our claim by lowering hardware resources measured in look-up-tables (LUTs) by a factor of three to six. Our method does not rely on any particular property of the weight matrices of the ANN. It works for the general task of multiplying an input vector with a constant matrix and is also suitable for digital signal processing beyond ANNs.},
  eventtitle = {{{IEEE Access}}},
  keywords = {computational efficiency,Computational efficiency,computer architecture,Computer architecture,Constant matrix multiplication,Encoding,Field programmable gate arrays,Matrix decomposition,neural networks,Neural networks,reconfigurable architectures,Reconfigurable architectures,Signal processing algorithms,Sparse matrices},
  file = {C\:\\Users\\simon\\Zotero\\storage\\YJ8M8N3A\\Lehnert et al_2023_Most Resource Efficient Matrix Vector Multiplication on FPGAs.pdf;C\:\\Users\\simon\\Zotero\\storage\\YXM7AXQA\\10007836.html}
}

@article{lehnertMostResourceEfficient2023a,
  title = {Most {{Resource Efficient Matrix Vector Multiplication}} on {{FPGAs}}},
  author = {Lehnert, Alexander and Holzinger, Philipp and Pfenning, Simon and Müller, Ralf and Reichenbach, Marc},
  date = {2023},
  journaltitle = {IEEE Access},
  volume = {11},
  pages = {3881--3898},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3234622},
  url = {https://ieeexplore.ieee.org/document/10007836?denied=},
  urldate = {2024-03-16},
  abstract = {Fast and resource-efficient inference in artificial neural networks (ANNs) is of utmost importance and drives many new developments in the area of new hardware architectures, e.g., by means of systolic arrays or algorithmic optimization such as pruning. In this paper, we present a novel method for lowering the computation effort for ANN inference utilizing ideas from information theory. Weight matrices are sliced into submatrices of logarithmic aspect ratios. These slices are then factorized. This reduces the number of required computations without compromising on fully parallel processing. We create a new hardware architecture for this dedicated purpose. We also provide a tool to map these sliced and factorized matrices efficiently to reconfigurable hardware. By comparing to the state of the art FPGA implementations, we can prove our claim by lowering hardware resources measured in look-up-tables (LUTs) by a factor of three to six. Our method does not rely on any particular property of the weight matrices of the ANN. It works for the general task of multiplying an input vector with a constant matrix and is also suitable for digital signal processing beyond ANNs.},
  eventtitle = {{{IEEE Access}}},
  keywords = {computational efficiency,Computational efficiency,computer architecture,Computer architecture,Constant matrix multiplication,Encoding,Field programmable gate arrays,Matrix decomposition,neural networks,Neural networks,reconfigurable architectures,Reconfigurable architectures,Signal processing algorithms,Sparse matrices},
  file = {C\:\\Users\\simon\\Zotero\\storage\\5AEW7SML\\Lehnert et al_2023_Most Resource Efficient Matrix Vector Multiplication on FPGAs.pdf;C\:\\Users\\simon\\Zotero\\storage\\V7IMZZHP\\10007836.html}
}

@article{liTemperatureBasedRestricted2016,
  title = {Temperature Based {{Restricted Boltzmann Machines}}},
  author = {Li, Guoqi and Deng, Lei and Xu, Yi and Wen, Changyun and Wang, Wei and Pei, Jing and Shi, Luping},
  date = {2016-01-13},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {6},
  number = {1},
  pages = {19133},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/srep19133},
  url = {https://www.nature.com/articles/srep19133},
  urldate = {2024-02-27},
  abstract = {Restricted Boltzmann machines (RBMs), which apply graphical models to learning probability distribution over a set of inputs, have attracted much attention recently since being proposed as building blocks of multi-layer learning systems called deep belief networks (DBNs). Note that temperature is a key factor of the Boltzmann distribution that RBMs originate from. However, none of existing schemes have considered the impact of temperature in the graphical model of DBNs. In this work, we propose temperature based restricted Boltzmann machines (TRBMs) which reveals that temperature is an essential parameter controlling the selectivity of the firing neurons in the hidden layers. We theoretically prove that the effect of temperature can be adjusted by setting the parameter of the sharpness of the logistic function in the proposed TRBMs. The performance of RBMs can be improved by adjusting the temperature parameter of TRBMs. This work provides a comprehensive insights into the deep belief networks and deep learning architectures from a physical point of view.},
  issue = {1},
  langid = {english},
  keywords = {Applied physics,Computational science},
  file = {C:\Users\simon\Zotero\storage\2EB9SJC3\Li et al_2016_Temperature based Restricted Boltzmann Machines.pdf}
}

@article{lucasIsingFormulationsMany2014,
  title = {Ising Formulations of Many {{NP}} Problems},
  author = {Lucas, Andrew},
  date = {2014-02-12},
  journaltitle = {Frontiers in Physics},
  shortjournal = {Front. Phys.},
  volume = {2},
  publisher = {Frontiers},
  issn = {2296-424X},
  doi = {10.3389/fphy.2014.00005},
  url = {https://www.frontiersin.org/articles/10.3389/fphy.2014.00005},
  urldate = {2024-03-22},
  abstract = {We provide Ising formulations for many NP-complete and NP-hard problems, including all of Karp's 21 NP-complete problems. This collects and extends mappings to the Ising model from partitioning, covering and satisfiability. In each case, the required number of spins is at most cubic in the size of the problem. This work may be useful in designing adiabatic quantum optimization algorithms.},
  langid = {english},
  keywords = {adiabatic quantum computation,Algorithms,complexity theory,NP,spin glasses},
  file = {C:\Users\simon\Zotero\storage\DPHDRI6B\Lucas_2014_Ising formulations of many NP problems.pdf}
}

@online{luccioniPowerHungryProcessing2023,
  title = {Power {{Hungry Processing}}: {{Watts Driving}} the {{Cost}} of {{AI Deployment}}?},
  shorttitle = {Power {{Hungry Processing}}},
  author = {Luccioni, Alexandra Sasha and Jernite, Yacine and Strubell, Emma},
  date = {2023-11-28},
  eprint = {2311.16863},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.16863},
  url = {http://arxiv.org/abs/2311.16863},
  urldate = {2024-02-15},
  abstract = {Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of "generality" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\66VJIJMH\Power Hungry Processing Watts Driving the Cost of AI Deployment.pdf}
}

@inproceedings{luqiRapidSoftwarePrototyping1992,
  title = {Rapid Software Prototyping},
  booktitle = {Proceedings of the {{Twenty-Fifth Hawaii International Conference}} on {{System Sciences}}},
  author = {Luqi, L. and Steigerwald, R.},
  date = {1992-01},
  volume = {ii},
  pages = {470-479 vol.2},
  doi = {10.1109/HICSS.1992.183261},
  url = {https://ieeexplore.ieee.org/abstract/document/183261},
  urldate = {2024-04-02},
  abstract = {Rapid software prototyping is an iterative software development methodology aimed at improving the analysis, design, and development of proposed systems. The paper describes rapid prototyping at the system and software levels and reviews the characteristics of computer-aided prototyping. The authors then describe the state-of-the-art in rapid prototyping and discuss technologies that improve the future outlook for prototyping, such as prototyping languages, software reuse, and designer interfaces. To add some cohesion to the concepts, they describe the characteristics of a computer-aided rapid prototyping system. Finally, they provide summaries of the outstanding papers that comprise the rapid prototyping mini-track.{$<>$}},
  eventtitle = {Proceedings of the {{Twenty-Fifth Hawaii International Conference}} on {{System Sciences}}},
  keywords = {Aircraft,Delay,Hardware,Humans,Prototypes,Software design,Software prototyping,Software systems,Timing,Virtual prototyping},
  file = {C\:\\Users\\simon\\Zotero\\storage\\FZIBWQSI\\Luqi_Steigerwald_1992_Rapid software prototyping.pdf;C\:\\Users\\simon\\Zotero\\storage\\M7ANERCS\\183261.html}
}

@article{machupalliReviewASICAccelerators2022,
  title = {Review of {{ASIC}} Accelerators for Deep Neural Network},
  author = {Machupalli, Raju and Hossain, Masum and Mandal, Mrinal},
  date = {2022-03-01},
  journaltitle = {Microprocessors and Microsystems},
  shortjournal = {Microprocessors and Microsystems},
  volume = {89},
  pages = {104441},
  issn = {0141-9331},
  doi = {10.1016/j.micpro.2022.104441},
  url = {https://www.sciencedirect.com/science/article/pii/S0141933122000163},
  urldate = {2024-03-20},
  abstract = {Deep neural networks (DNNs) have become an essential tool in artificial intelligence, with a wide range of applications such as computer vision, medical diagnosis, security, robotics, and autonomous vehicle. The DNNs deliver the state-of-the-art performance in many applications. The complexity of the DNN models generally increases with application complexity and deployment of complex DNN models requires high computational power. General-purpose processors are unable to process complex DNNs within the required throughput, latency, and power budget. Therefore, domain-specific hardware accelerators are required to provide high computational resources with superior energy efficiency and throughput within a small chip area. In this paper, existing DNN hardware accelerators are reviewed and classified based on the optimization techniques used in their implementations. Each optimization technique generally improves one or more specific performance parameter(s). For example, the hardware optimized for sparse DNNs may provide poor performance for dense DNNs in terms of power and throughput. Therefore, understanding the tradeoff between different hardware accelerators helps to identify the best accelerator model for application deployment. We identify three major areas, ALU, dataflow, and sparsity, in hardware architectures having the potential to improve the overall performance of an accelerator. Existing hardware accelerators for inference are broadly classified into these three categories. As there is no standard model or performance metrics to evaluate the efficiency of the new DNN hardwares in the literature, the classification model can help to identify appropriate performance parameters and benchmark accelerators.},
  keywords = {ASIC,Deep neural network,Domain specific accelerator,Hardware accelerator,Neural processor},
  file = {C:\Users\simon\Zotero\storage\R38Q4TV4\S0141933122000163.html}
}

@book{mackayInformationTheoryInference2003,
  title = {Information {{Theory}}, {{Inference}} and {{Learning Algorithms}}},
  author = {MacKay, David J. C.},
  date = {2003-09-25},
  eprint = {AKuMj4PN_EMC},
  eprinttype = {googlebooks},
  publisher = {Cambridge University Press},
  abstract = {Information theory and inference, often taught separately, are here united in one entertaining textbook. These topics lie at the heart of many exciting areas of contemporary science and engineering - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics, and cryptography. This textbook introduces theory in tandem with applications. Information theory is taught alongside practical communication systems, such as arithmetic coding for data compression and sparse-graph codes for error-correction. A toolbox of inference techniques, including message-passing algorithms, Monte Carlo methods, and variational approximations, are developed alongside applications of these tools to clustering, convolutional codes, independent component analysis, and neural networks. The final part of the book describes the state of the art in error-correcting codes, including low-density parity-check codes, turbo codes, and digital fountain codes -- the twenty-first century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, David MacKay's groundbreaking book is ideal for self-learning and for undergraduate or graduate courses. Interludes on crosswords, evolution, and sex provide entertainment along the way. In sum, this is a textbook on information, communication, and coding for a new generation of students, and an unparalleled entry point into these subjects for professionals in areas as diverse as computational biology, financial engineering, and machine learning.},
  isbn = {978-0-521-64298-9},
  langid = {english},
  pagetotal = {694},
  keywords = {Computers / Artificial Intelligence / Computer Vision & Pattern Recognition,Computers / Computer Science,Computers / Data Science / Data Modeling & Design,Computers / Data Science / Neural Networks,Computers / Information Theory,Mathematics / Algebra / General,Philosophy / Logic,Science / Physics / General,Technology & Engineering / Electronics / General}
}

@online{maEra1bitLLMs2024,
  title = {The {{Era}} of 1-Bit {{LLMs}}: {{All Large Language Models}} Are in 1.58 {{Bits}}},
  shorttitle = {The {{Era}} of 1-Bit {{LLMs}}},
  author = {Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
  date = {2024-02-27},
  eprint = {2402.17764},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2402.17764},
  urldate = {2024-04-19},
  abstract = {Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary \{-1, 0, 1\}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\simon\\Zotero\\storage\\LJB4KAS9\\Ma et al_2024_The Era of 1-bit LLMs.pdf;C\:\\Users\\simon\\Zotero\\storage\\UN4J7KRB\\2402.html}
}

@article{mahmoodiVersatileStochasticDot2019,
  title = {Versatile Stochastic Dot Product Circuits Based on Nonvolatile Memories for High Performance Neurocomputing and Neurooptimization},
  author = {Mahmoodi, M. R. and Prezioso, M. and Strukov, D. B.},
  date = {2019-11-08},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {10},
  number = {1},
  pages = {5113},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13103-7},
  url = {https://www.nature.com/articles/s41467-019-13103-7},
  urldate = {2024-03-26},
  abstract = {The key operation in stochastic neural networks, which have become the state-of-the-art approach for solving problems in machine learning, information theory, and statistics, is a stochastic dot-product. While there have been many demonstrations of dot-product circuits and, separately, of stochastic neurons, the efficient hardware implementation combining both functionalities is still missing. Here we report compact, fast, energy-efficient, and scalable stochastic dot-product circuits based on either passively integrated metal-oxide memristors or embedded floating-gate memories. The circuit’s high performance is due to mixed-signal implementation, while the efficient stochastic operation is achieved by utilizing circuit’s noise, intrinsic and/or extrinsic to the memory cell array. The dynamic scaling of weights, enabled by analog memory devices, allows for efficient realization of different annealing approaches to improve functionality. The proposed approach is experimentally verified for two representative applications, namely by implementing neural network for solving a four-node graph-partitioning problem, and a Boltzmann machine with 10-input and~8-hidden~neurons.},
  langid = {english},
  keywords = {Electrical and electronic engineering,Electronic devices,Network models},
  file = {C:\Users\simon\Zotero\storage\VA8WRCVN\Mahmoodi et al_2019_Versatile stochastic dot product circuits based on nonvolatile memories for.pdf}
}

@article{mallComprehensiveReviewDeep2023,
  title = {A Comprehensive Review of Deep Neural Networks for Medical Image Processing: {{Recent}} Developments and Future Opportunities},
  shorttitle = {A Comprehensive Review of Deep Neural Networks for Medical Image Processing},
  author = {Mall, Pawan Kumar and Singh, Pradeep Kumar and Srivastav, Swapnita and Narayan, Vipul and Paprzycki, Marcin and Jaworska, Tatiana and Ganzha, Maria},
  date = {2023-12-01},
  journaltitle = {Healthcare Analytics},
  shortjournal = {Healthcare Analytics},
  volume = {4},
  pages = {100216},
  issn = {2772-4425},
  doi = {10.1016/j.health.2023.100216},
  url = {https://www.sciencedirect.com/science/article/pii/S2772442523000837},
  urldate = {2024-02-23},
  abstract = {Artificial Intelligence (AI) solutions have been widely used in healthcare, and recent developments in deep neural networks have contributed to significant advances in medical image processing. Much ongoing research is aimed at helping medical practitioners by providing automated systems to analyze images and diagnose acute diseases, such as brain tumors, bone cancer, breast cancer, bone fracture, and many others. This comprehensive review delivers an overview of recent advances in medical imaging using deep neural networks. In addition to the comprehensive literature review, a summary of openly available data sources and future research directions are outlined.},
  keywords = {Artificial intelligence,Deep neural networks,gelesen,Machine learning,Medical imaging diagnostic analytics,Predictive analytics,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\ZH4THS93\S2772442523000837.html}
}

@article{marinoDeepNeuralNetworks2023,
  title = {Deep Neural Networks Compression: {{A}} Comparative Survey and Choice Recommendations},
  shorttitle = {Deep Neural Networks Compression},
  author = {Marinó, Giosué Cataldo and Petrini, Alessandro and Malchiodi, Dario and Frasca, Marco},
  date = {2023-02-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {520},
  pages = {152--170},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.11.072},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231222014643},
  urldate = {2024-02-23},
  abstract = {The state-of-the-art performance for several real-world problems is currently reached by deep and, in particular, convolutional neural networks (CNN). Such learning models exploit recent results in the field of deep learning, leading to highly performing, yet very large neural networks with typically millions to billions of parameters. As a result, such models are often redundant and excessively oversized, with a detrimental effect on the environment in terms of unnecessary energy consumption and a limitation to their deployment on low-resource devices. The necessity for compression techniques able to reduce the number of model parameters and their resource demand is thereby increasingly felt by the research community. In this paper we propose the first extensive comparison, to the best of our knowledge, of the main lossy and structure-preserving approaches to compress pre-trained CNNs, applicable in principle to any existing model. Our study is intended to provide a first and preliminary guidance to choose the most suitable compression technique when there is the need to reduce the occupancy of pre-trained models. Both convolutional and fully-connected layers are included in the analysis. Our experiments involved two pre-trained state-of-the-art CNNs (proposed to solve classification or regression problems) and five benchmarks, and gave rise to important insights about the applicability and performance of such techniques w.r.t.the type of layer to be compressed and the category of problem tackled.},
  keywords = {CNN compression,Connection pruning,Huffman coding,Succinct Deep Neural Networks,Weight quantization,Weight sharing},
  file = {C:\Users\simon\Zotero\storage\TBJBQ945\S0925231222014643.html}
}

@article{metropolisEquationStateCalculations1953,
  title = {Equation of {{State Calculations}} by {{Fast Computing Machines}}},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  date = {1953-06-01},
  journaltitle = {The Journal of Chemical Physics},
  shortjournal = {The Journal of Chemical Physics},
  volume = {21},
  number = {6},
  pages = {1087--1092},
  issn = {0021-9606},
  doi = {10.1063/1.1699114},
  url = {https://doi.org/10.1063/1.1699114},
  urldate = {2024-02-27},
  abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.}
}

@article{mihramSimulationMethodology1976,
  title = {Simulation Methodology},
  author = {Mihram, G. Arthur},
  date = {1976-02-01},
  journaltitle = {Theory and Decision},
  shortjournal = {Theor Decis},
  volume = {7},
  number = {1},
  pages = {67--94},
  issn = {1573-7187},
  doi = {10.1007/BF00141103},
  url = {https://doi.org/10.1007/BF00141103},
  urldate = {2024-04-02},
  abstract = {Operational researchers, management scientists, and industrial engineers have been asked by Russell Ackoff to become ‘systems scientists’, yet he stated that ‘Systems Science is not a science’. (TIMS Interfaces, 2 (4), 41). A. C. Fabergé (Science184, 1330) notes that the original intent of operational researchers was that they be scientists, ‘trained to observe’. Hugh J. Miser (Operations Research22, 903), views ‘operations research as a science’, noting that its progress indeed is of a cyclic nature.},
  langid = {english},
  keywords = {Industrial Engineer,Management Scientist,Operation Research,Social Scientist,System Scientist},
  file = {C:\Users\simon\Zotero\storage\JLWP34KN\Mihram_1976_Simulation methodology.pdf}
}

@article{mocanuTopologicalInsightRestricted2016,
  title = {A Topological Insight into Restricted {{Boltzmann}} Machines},
  author = {Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
  date = {2016-09-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {104},
  number = {2},
  pages = {243--270},
  issn = {1573-0565},
  doi = {10.1007/s10994-016-5570-z},
  url = {https://doi.org/10.1007/s10994-016-5570-z},
  urldate = {2024-02-22},
  abstract = {Restricted Boltzmann Machines (RBMs) and models derived from them have been successfully used as basic building blocks in deep artificial neural networks for automatic features extraction, unsupervised weights initialization, but also as density estimators. Thus, their generative and discriminative capabilities, but also their computational time are instrumental to a wide range of applications. Our main contribution is to look at RBMs from a topological perspective, bringing insights from network science. Firstly, here we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which naturally have a small-world topology. Secondly, we demonstrate both on synthetic and real-world datasets that by constraining RBMs and GRBMs to a scale-free topology (while still considering local neighborhoods and data distribution), we reduce the number of weights that need to be computed by a few orders of magnitude, at virtually no loss in generative performance. Thirdly, we show that, for a fixed number of weights, our proposed sparse models (which by design have a higher number of hidden neurons) achieve better generative capabilities than standard fully connected RBMs and GRBMs (which by design have a smaller number of hidden neurons), at no additional computational costs.},
  langid = {english},
  keywords = {Complex networks,Deep learning,Scale-free networks,Small-world networks,Sparse restricted Boltzmann machines},
  file = {C:\Users\simon\Zotero\storage\524Q4ZX2\Mocanu et al_2016_A topological insight into restricted Boltzmann machines.pdf}
}

@online{mohseniIsingMachinesHardware2022,
  title = {Ising Machines as Hardware Solvers of Combinatorial Optimization Problems},
  author = {Mohseni, Naeimeh and McMahon, Peter L. and Byrnes, Tim},
  date = {2022-04-01},
  eprint = {2204.00276},
  eprinttype = {arxiv},
  eprintclass = {physics, physics:quant-ph},
  doi = {10.48550/arXiv.2204.00276},
  url = {http://arxiv.org/abs/2204.00276},
  urldate = {2024-02-15},
  abstract = {Ising machines are hardware solvers which aim to find the absolute or approximate ground states of the Ising model. The Ising model is of fundamental computational interest because it is possible to formulate any problem in the complexity class NP as an Ising problem with only polynomial overhead. A scalable Ising machine that outperforms existing standard digital computers could have a huge impact for practical applications for a wide variety of optimization problems. In this review, we survey the current status of various approaches to constructing Ising machines and explain their underlying operational principles. The types of Ising machines considered here include classical thermal annealers based on technologies such as spintronics, optics, memristors, and digital hardware accelerators; dynamical-systems solvers implemented with optics and electronics; and superconducting-circuit quantum annealers. We compare and contrast their performance using standard metrics such as the ground-state success probability and time-to-solution, give their scaling relations with problem size, and discuss their strengths and weaknesses.},
  pubstate = {preprint},
  keywords = {Physics - Applied Physics,Quantum Physics,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\VWIC5SI6\Ising machines Hardware solvers for combinatorial.pdf}
}

@article{mohseniIsingMachinesHardware2022a,
  title = {Ising Machines as Hardware Solvers of Combinatorial Optimization Problems},
  author = {Mohseni, Naeimeh and McMahon, Peter L. and Byrnes, Tim},
  date = {2022-06},
  journaltitle = {Nature Reviews Physics},
  shortjournal = {Nat Rev Phys},
  volume = {4},
  number = {6},
  pages = {363--379},
  publisher = {Nature Publishing Group},
  issn = {2522-5820},
  doi = {10.1038/s42254-022-00440-8},
  url = {https://www.nature.com/articles/s42254-022-00440-8},
  urldate = {2024-03-22},
  abstract = {Ising machines are hardware solvers that aim to find the absolute or approximate ground states of the Ising model. The Ising model is of fundamental computational interest because any problem in the complexity class NP can be formulated as an Ising problem with only polynomial overhead, and thus a scalable Ising machine that outperforms existing standard digital computers could have a huge impact for practical applications. We survey the status of various approaches to constructing Ising machines and explain their underlying operational principles. The types of Ising machines considered here include classical thermal annealers based on technologies such as spintronics, optics, memristors and digital hardware accelerators; dynamical systems solvers implemented with optics and electronics; and superconducting-circuit quantum annealers. We compare and contrast their performance using standard metrics such as the ground-state success probability and time-to-solution, give their scaling relations with problem size, and discuss their strengths and weaknesses.},
  langid = {english},
  keywords = {Computational science,Electronics,Information theory and computation,photonics and device physics},
  file = {C:\Users\simon\Zotero\storage\JZRT247M\Mohseni et al_2022_Ising machines as hardware solvers of combinatorial optimization problems.pdf}
}

@article{mololothBlockchainMachineLearning2023,
  title = {Blockchain and {{Machine Learning}} for {{Future Smart Grids}}: {{A Review}}},
  shorttitle = {Blockchain and {{Machine Learning}} for {{Future Smart Grids}}},
  author = {Mololoth, Vidya Krishnan and Saguna, Saguna and Åhlund, Christer},
  date = {2023-01},
  journaltitle = {Energies},
  volume = {16},
  number = {1},
  pages = {528},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1996-1073},
  doi = {10.3390/en16010528},
  url = {https://www.mdpi.com/1996-1073/16/1/528},
  urldate = {2024-03-12},
  abstract = {Developments such as the increasing electrical energy demand, growth of renewable energy sources, cyber–physical security threats, increased penetration of electric vehicles (EVs), and unpredictable behavior of prosumers and EV users pose a range of challenges to the electric power system. To address these challenges, a decentralized system using blockchain technology and machine learning techniques for secure communication, distributed energy management and decentralized energy trading between prosumers is required. Blockchain enables secure distributed trust platforms, addresses optimization and reliability challenges, and allows P2P distributed energy exchange as well as flexibility services between customers. On the other hand, machine learning techniques enable intelligent smart grid operations by using prediction models and big data analysis. Motivated from these facts, in this review, we examine the potential of combining blockchain technology and machine learning techniques in the development of smart grid and investigate the benefits achieved by using both techniques for the future smart grid scenario. Further, we discuss research challenges and future research directions of applying blockchain and machine learning techniques for smart grids both individually as well as combining them together. The identified areas that require significant research are demand management in power grids, improving the security of grids with better consensus mechanisms, electric vehicle charging systems, scheduling of the entire grid system, designing secure microgrids, and the interconnection of different blockchain networks.},
  issue = {1},
  langid = {english},
  keywords = {blockchain,demand response management,electric vehicles,energy trading,machine learning,security,smart grids},
  file = {C:\Users\simon\Zotero\storage\SF54GLAS\Mololoth et al_2023_Blockchain and Machine Learning for Future Smart Grids.pdf}
}

@article{monevaSustainabilityReportingView2023,
  title = {Sustainability Reporting in View of the {{European}} Sustainable Finance Taxonomy: {{Is}} the Financial Sector Ready to Disclose Circular Economy?},
  shorttitle = {Sustainability Reporting in View of the {{European}} Sustainable Finance Taxonomy},
  author = {Moneva, José M. and Scarpellini, Sabina and Aranda-Usón, Alfonso and Alvarez Etxeberria, Igor},
  date = {2023},
  journaltitle = {Corporate Social Responsibility and Environmental Management},
  volume = {30},
  number = {3},
  pages = {1336--1347},
  issn = {1535-3966},
  doi = {10.1002/csr.2423},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/csr.2423},
  urldate = {2024-03-13},
  abstract = {The European sustainable finance taxonomy requires financial and nonfinancial companies to provide investors with information about the environmental performance of their assets and economic activities. For financial institutions, making socially and environmentally responsible investments visible through a common label that guarantees specific standards for the entire European Union is crucial. Against this background, this study analyzes the evolution of sustainability reporting practices and their assurance in a sample of European financial institutions. To this end, we used a double qualitative methodological approach, based on (a) an external analysis of nonfinancial information and its relationship with the main economic-financial variables of the sample companies, and (b) a case study of a bank carried out through semi-structured interviews. This study provides an external measurement analysis of nonfinancial information in entities from different countries, which can contribute to broadening the scope and level of sustainability and circular economy accountability.},
  langid = {english},
  keywords = {assurance,circular economy,CSR,financial institutions,sustainability accounting,sustainability reporting,sustainable finance},
  file = {C\:\\Users\\simon\\Zotero\\storage\\C9EG5RFV\\Moneva et al_2023_Sustainability reporting in view of the European sustainable finance taxonomy.pdf;C\:\\Users\\simon\\Zotero\\storage\\6JXXMVCA\\csr.html}
}

@online{montufarRestrictedBoltzmannMachines2018,
  title = {Restricted {{Boltzmann Machines}}: {{Introduction}} and {{Review}}},
  shorttitle = {Restricted {{Boltzmann Machines}}},
  author = {Montufar, Guido},
  date = {2018-06-19},
  eprint = {1806.07066},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.1806.07066},
  url = {http://arxiv.org/abs/1806.07066},
  urldate = {2024-02-15},
  abstract = {The restricted Boltzmann machine is a network of stochastic units with undirected interactions between pairs of visible and hidden units. This model was popularized as a building block of deep learning architectures and has continued to play an important role in applied and theoretical machine learning. Restricted Boltzmann machines carry a rich structure, with connections to geometry, applied algebra, probability, statistics, machine learning, and other areas. The analysis of these models is attractive in its own right and also as a platform to combine and generalize mathematical tools for graphical models with hidden variables. This article gives an introduction to the mathematical analysis of restricted Boltzmann machines, reviews recent results on the geometry of the sets of probability distributions representable by these models, and suggests a few directions for further investigation.},
  pubstate = {preprint},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Machine Learning,ungelesen},
  file = {C:\Users\simon\Zotero\storage\3DLWASIX\Montufar - 2018 - Restricted Boltzmann Machines Introduction and Re.pdf}
}

@online{MostResourceEfficient,
  title = {Most {{Resource Efficient Matrix Vector Multiplication}} on {{FPGAs}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/10007836?denied=},
  urldate = {2024-03-16}
}

@online{MostResourceEfficienta,
  title = {Most {{Resource Efficient Matrix Vector Multiplication}} on {{FPGAs}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/10007836?denied=},
  urldate = {2024-03-16}
}

@book{nazmbojnordiMemristiveBoltzmannMachine2016,
  title = {Memristive {{Boltzmann}} Machine: {{A}} Hardware Accelerator for Combinatorial Optimization and Deep Learning},
  shorttitle = {Memristive {{Boltzmann}} Machine},
  author = {Nazm Bojnordi, Mahdi and Ipek, Engin},
  date = {2016-03-01},
  pages = {13},
  doi = {10.1109/HPCA.2016.7446049},
  pagetotal = {1},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\A9DK674U\Memristive Boltzmann machine A hardware accelerator for combinatorial optimization and deep learning.pdf}
}

@article{nelsonSoftwarePrototyping2016,
  title = {Software Prototyping},
  author = {Nelson, Scott D. and Fiol, Guilherme Del and Hanseler, Haley and Crouch, Barbara Insley and Cummins, Mollie R.},
  date = {2016-01},
  journaltitle = {Applied Clinical Informatics},
  shortjournal = {Appl Clin Inform},
  volume = {07},
  number = {1},
  pages = {22--32},
  publisher = {Schattauer GmbH},
  issn = {1869-0327},
  doi = {10.4338/ACI-2015-07-CR-0091},
  url = {http://www.thieme-connect.de/DOI/DOI?10.4338/ACI-2015-07-CR-0091},
  urldate = {2024-04-02},
  abstract = {Health information exchange (HIE) between Poison Control Centers (PCCs) and Emergency Departments (EDs) could improve care of poisoned patients. However, PCC information systems are not designed to facilitate HIE with EDs; therefore, we are developing specialized software to support HIE within the normal workflow of the PCC using user-centered design and rapid prototyping.  To describe the design of an HIE dashboard and the refinement of user requirements through rapid prototyping.  Using previously elicited user requirements, we designed low-fidelity sketches of designs on paper with iterative refinement. Next, we designed an interactive high-fidelity prototype and conducted scenario-based usability tests with end users. Users were asked to think aloud while accomplishing tasks related to a case vignette. After testing, the users provided feedback and evaluated the prototype using the System Usability Scale (SUS).  Survey results from three users provided useful feedback that was then incorporated into the design. After achieving a stable design, we used the prototype itself as the specification for development of the actual software. Benefits of prototyping included having 1) subject-matter experts heavily involved with the design; 2) flexibility to make rapid changes, 3) the ability to minimize software development efforts early in the design stage; 4) rapid finalization of requirements; 5) early visualization of designs; 6) and a powerful vehicle for communication of the design to the programmers. Challenges included 1) time and effort to develop the prototypes and case scenarios; 2) no simulation of system performance; 3) not having all proposed functionality available in the final product; and 4) missing needed data elements in the PCC information system.},
  langid = {english},
  keywords = {computer interface,human engineering/methods,medical informatics/methods,Software design,user},
  file = {C:\Users\simon\Zotero\storage\ITJYJMKG\Nelson et al_2016_Software prototyping.pdf}
}

@inproceedings{NIPS2000_1f1baa5b,
  title = {Recognizing Hand-Written Digits Using Hierarchical Products of Experts},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mayraz, Guy and Hinton, Geoffrey E},
  editor = {Leen, T. and Dietterich, T. and Tresp, V.},
  date = {2000},
  volume = {13},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\J6D4HNWV\Mayraz und Hinton - 2000 - Recognizing hand-written digits using hierarchical.pdf}
}

@article{oesterleKonsortialforschung2010,
  title = {Konsortialforschung},
  author = {Österle, Hubert and Otto, Boris},
  date = {2010-10-01},
  journaltitle = {WIRTSCHAFTSINFORMATIK},
  shortjournal = {WIRTSCHAFTSINFORMATIK},
  volume = {52},
  number = {5},
  pages = {273--285},
  issn = {1861-8936},
  doi = {10.1007/s11576-010-0238-y},
  url = {https://doi.org/10.1007/s11576-010-0238-y},
  urldate = {2024-03-29},
  abstract = {Gestaltungsorientierte Forschung in der Wirtschaftsinformatik strebt Ergebnisse an, welche den Anforderungen wissenschaftlicher Strenge und praktischer Relevanz gleichermaßen genügen. Jedoch stehen Forscher heutzutage vor der Herausforderung, überhaupt Zugang zur Wissensbasis in der Praxis zu erhalten und dieses Wissen zu erfassen. Vor diesem Hintergrund schlägt dieser Aufsatz eine Methode für Konsortialforschung vor, welche die multilaterale Zusammenarbeit zwischen Forschern und Praktikern im Forschungsprozess ermöglichen soll. Der Entwurf der Methode basiert auf einem selbstbewertenden Gestaltungsprozess, welcher sich über einen Zeitraum von über zwanzig Jahren erstreckte. Der Aufsatz trägt in zweifacher Weise zur wissenschaftlichen Diskussion bei. Zum einen adressiert er die wissenschaftliche Grundlage gestaltungsorientierter Forschung, denn er liefert Forschern eine Handlungsanleitung für die Zusammenarbeit mit Praktikern bei der Gestaltung von Artefakten. Zum anderen stellt die Methode selbst ein Artefakt dar, also das Ergebnis eines gestaltungsorientierten Forschungsprozesses.},
  langid = {ngerman},
  keywords = {Consortium research,Design science research,Forschungsmethode,gestaltungsorientierte Forschung,Konsortialforschung,Research method},
  file = {C:\Users\simon\Zotero\storage\STSEPTIV\Österle_Otto_2010_Konsortialforschung.pdf}
}

@article{oesterleMemorandumZurGestaltungsorientierten2010,
  title = {Memorandum Zur Gestaltungsorientierten {{Wirtschaftsinformatik}}},
  author = {Oesterle, Hubert and Becker, Jörg and Hess, Thomas and Karagiannis, Dimitris and Krcmar, Helmut and Loos, Peter and Mertens, Peter and Oberweis, Andreas and Sinz, Elmar},
  date = {2010-09-01},
  journaltitle = {http://www.alexandria.unisg.ch/Publikationen/71074},
  shortjournal = {http://www.alexandria.unisg.ch/Publikationen/71074},
  volume = {62},
  doi = {10.1007/BF03372838},
  abstract = {Wie kann die Wirtschaftsinformatik am besten aus ihrer Forschung Nutzen für Wirtschaft und Gesellschaft erzeugen? Mit dieser Frage eröffnen die Autoren im vorliegenden Memorandum die Diskussion um die Forschungsparadigmen, die für die Disziplin adäquat sind. Die Autoren treten für die Gestaltungsorientierung der Wirtschaftsinformatik ein, machen sich aber auch explizit für einen Methodenpluralismus stark, begrüßen Diskussionen, Kritik oder Zustimmung und heißen Kommentare jeder Art willkommen, die dazu dienen, die Forschung in der Wirtschaftsinformatik voran zu bringen.},
  file = {C:\Users\simon\Zotero\storage\2LQHKEVN\Oesterle et al_2010_Memorandum zur gestaltungsorientierten Wirtschaftsinformatik.pdf}
}

@article{ortega-zamoranoFPGAHardwareAcceleration2016,
  title = {{{FPGA Hardware Acceleration}} of {{Monte Carlo Simulations}} for the {{Ising Model}}},
  author = {Ortega-Zamorano, Francisco and Montemurro, Marcelo A. and Cannas, Sergio A. and Jerez, José M. and Franco, Leonardo},
  date = {2016-09-01},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  volume = {27},
  number = {9},
  eprint = {1602.03016},
  eprinttype = {arxiv},
  eprintclass = {physics},
  pages = {2618--2627},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2015.2505725},
  url = {http://arxiv.org/abs/1602.03016},
  urldate = {2024-04-10},
  abstract = {A two-dimensional Ising model with nearest-neighbors ferromagnetic interactions is implemented in a Field Programmable Gate Array (FPGA) board.Extensive Monte Carlo simulations were carried out using an efficient hardware representation of individual spins and a combined global-local LFSR random number generator. Consistent results regarding the descriptive properties of magnetic systems, like energy, magnetization and susceptibility are obtained while a speed-up factor of approximately 6 times is achieved in comparison to previous FPGA-based published works and almost \$10\^{}4\$ times in comparison to a standard CPU simulation. A detailed description of the logic design used is given together with a careful analysis of the quality of the random number generator used. The obtained results confirm the potential of FPGAs for analyzing the statistical mechanics of magnetic systems.},
  keywords = {Computer Science - Hardware Architecture,Physics - Computational Physics},
  file = {C\:\\Users\\simon\\Zotero\\storage\\VEEUL2RF\\Ortega-Zamorano et al_2016_FPGA Hardware Acceleration of Monte Carlo Simulations for the Ising Model.pdf;C\:\\Users\\simon\\Zotero\\storage\\TPMM7LBD\\1602.html}
}

@article{ortega-zamoranoFPGAHardwareAcceleration2016a,
  title = {{{FPGA Hardware Acceleration}} of {{Monte Carlo Simulations}} for the {{Ising Model}}},
  author = {Ortega-Zamorano, Francisco and Montemurro, Marcelo A. and Cannas, Sergio A. and Jerez, José M. and Franco, Leonardo},
  date = {2016-09-01},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  volume = {27},
  number = {9},
  eprint = {1602.03016},
  eprinttype = {arxiv},
  eprintclass = {physics},
  pages = {2618--2627},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2015.2505725},
  url = {http://arxiv.org/abs/1602.03016},
  urldate = {2024-04-16},
  abstract = {A two-dimensional Ising model with nearest-neighbors ferromagnetic interactions is implemented in a Field Programmable Gate Array (FPGA) board.Extensive Monte Carlo simulations were carried out using an efficient hardware representation of individual spins and a combined global-local LFSR random number generator. Consistent results regarding the descriptive properties of magnetic systems, like energy, magnetization and susceptibility are obtained while a speed-up factor of approximately 6 times is achieved in comparison to previous FPGA-based published works and almost \$10\^{}4\$ times in comparison to a standard CPU simulation. A detailed description of the logic design used is given together with a careful analysis of the quality of the random number generator used. The obtained results confirm the potential of FPGAs for analyzing the statistical mechanics of magnetic systems.},
  keywords = {Computer Science - Hardware Architecture,Physics - Computational Physics},
  file = {C\:\\Users\\simon\\Zotero\\storage\\Y2AV8S34\\Ortega-Zamorano et al_2016_FPGA Hardware Acceleration of Monte Carlo Simulations for the Ising Model.pdf;C\:\\Users\\simon\\Zotero\\storage\\HX7M65YQ\\1602.html}
}

@book{pandianIntelligentComputingOptimization,
  title = {Intelligent {{Computing}} and {{Optimization}}},
  author = {Pandian, Vasant and Vladimir Panchenko},
  url = {https://link.springer.com/book/9783031508868},
  urldate = {2024-02-04},
  abstract = {This book covers the 7th edition of International Conference on Intelligent Computing and Optimization took place at Baitong Hotel \& Resort},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\WPJ3SWAN\9783031508868.html}
}

@online{ParisAgreementUNFCCC,
  title = {The {{Paris Agreement}} | {{UNFCCC}}},
  url = {https://unfccc.int/process-and-meetings/the-paris-agreement},
  urldate = {2024-03-10},
  file = {C:\Users\simon\Zotero\storage\BNSASL6V\the-paris-agreement.html}
}

@online{patronOptimalRelaxationRate2024,
  title = {On the Optimal Relaxation Rate for the {{Metropolis}} Algorithm in One Dimension},
  author = {Patrón, A. and Chepelianskii, A. D. and Prados, A. and Trizac, E.},
  date = {2024-02-17},
  eprint = {2402.11267},
  eprinttype = {arxiv},
  eprintclass = {cond-mat, physics:math-ph},
  doi = {10.48550/arXiv.2402.11267},
  url = {http://arxiv.org/abs/2402.11267},
  urldate = {2024-02-27},
  abstract = {We study the relaxation of the Metropolis Monte Carlo algorithm corresponding to a single particle trapped in a one-dimensional confining potential, with even jump distributions that ensure that the dynamics verifies detailed balance. Previous work suggested that, for smooth jump distributions, the fastest relaxation rate is obtained as a result of the competition between diffusive and rejection-dominated dynamics. In this work, we show that a new regime comes into play for two-peaked jump distributions, where the relaxation dynamics is neither dominated by diffusion nor rejection: the eigenmodes adopt an oscillatory form, reminiscent of charge density waves (CDW) -- thus we term this new regime the CDW regime. Using a combination of numerical and analytical techniques, the parameter regions corresponding to diffusion, rejection, and CDW are characterised, as well as the transition lines between them -- i.e. a phase diagram is built. The optimal relaxation rate is located at the triple point of phase coexistence, where the transition lines (diffusive-rejection, diffusive-CDW, and CDW-rejection) intersect. Our theoretical framework is checked versus the numerical diagonalisation of the master equation. We also briefly discuss more sophisticated attempts at optimising the relaxation rate to equilibrium.},
  pubstate = {preprint},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematical Physics},
  file = {C\:\\Users\\simon\\Zotero\\storage\\LVWVYVHT\\Patrón et al_2024_On the optimal relaxation rate for the Metropolis algorithm in one dimension.pdf;C\:\\Users\\simon\\Zotero\\storage\\AX9IPXGS\\2402.html}
}

@article{peccerilloSurveyHardwareAccelerators2022,
  title = {A Survey on Hardware Accelerators: {{Taxonomy}}, Trends, Challenges, and Perspectives},
  shorttitle = {A Survey on Hardware Accelerators},
  author = {Peccerillo, Biagio and Mannino, Mirco and Mondelli, Andrea and Bartolini, Sandro},
  date = {2022-08-01},
  journaltitle = {Journal of Systems Architecture},
  shortjournal = {Journal of Systems Architecture},
  volume = {129},
  pages = {102561},
  issn = {1383-7621},
  doi = {10.1016/j.sysarc.2022.102561},
  url = {https://www.sciencedirect.com/science/article/pii/S1383762122001138},
  urldate = {2024-03-19},
  abstract = {In recent years, the limits of the multicore approach emerged in the so-called “dark silicon” issue and diminishing returns of an ever-increasing core count. Hardware manufacturers, out of necessity, switched their focus to accelerators, a new paradigm that pursues specialization and heterogeneity over generality and homogeneity. They are special-purpose hardware structures separated from the CPU with aspects that exhibit a high degree of variability. We define a taxonomy based on fourteen of these aspects, grouped in four macro-categories: general aspects, host coupling, architecture, and software aspects. According to it, we categorize around 100 accelerators of the last decade from both industry and academia, and critically analyze emerging trends. We complete our discussion with throughput and efficiency figures. Then, we discuss some prominent open challenges that accelerators are facing, analyzing state-of-the-art solutions, and suggesting prospective research directions for the future.},
  keywords = {Accelerators,CGRA,Classification,Data-parallel,Domain-Specific Architectures,Future research directions,Machine Learning,Open challenges,PIM,Survey,Taxonomy},
  file = {C\:\\Users\\simon\\Zotero\\storage\\K7M9E2SZ\\Peccerillo et al. - 2022 - A survey on hardware accelerators Taxonomy, trend.pdf;C\:\\Users\\simon\\Zotero\\storage\\CDQXTCS2\\S1383762122001138.html}
}

@online{pedrettiXTIMEInmemoryEngine2024,
  title = {X-{{TIME}}: {{An}} in-Memory Engine for Accelerating Machine Learning on Tabular Data with {{CAMs}}},
  shorttitle = {X-{{TIME}}},
  author = {Pedretti, Giacomo and Moon, John and Bruel, Pedro and Serebryakov, Sergey and Roth, Ron M. and Buonanno, Luca and Gajjar, Archit and Ziegler, Tobias and Xu, Cong and Foltin, Martin and Faraboschi, Paolo and Ignowski, Jim and Graves, Catherine E.},
  date = {2024-02-02},
  eprint = {2304.01285},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.01285},
  url = {http://arxiv.org/abs/2304.01285},
  urldate = {2024-02-15},
  abstract = {Structured, or tabular, data is the most common format in data science. While deep learning models have proven formidable in learning from unstructured data such as images or speech, they are less accurate than simpler approaches when learning from tabular data. In contrast, modern tree-based Machine Learning (ML) models shine in extracting relevant information from structured data. An essential requirement in data science is to reduce model inference latency in cases where, for example, models are used in a closed loop with simulation to accelerate scientific discovery. However, the hardware acceleration community has mostly focused on deep neural networks and largely ignored other forms of machine learning. Previous work has described the use of an analog content addressable memory (CAM) component for efficiently mapping random forests. In this work, we focus on an overall analog-digital architecture implementing a novel increased precision analog CAM and a programmable network on chip allowing the inference of state-of-the-art tree-based ML models, such as XGBoost and CatBoost. Results evaluated in a single chip at 16nm technology show 119x lower latency at 9740x higher throughput compared with a state-of-the-art GPU, with a 19W peak power consumption.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,ungelesen},
  file = {C:\Users\simon\Zotero\storage\AG7EV9DE\Pedretti et al. - 2024 - X-TIME An in-memory engine for accelerating machi.pdf}
}

@article{peffersDesignScienceResearch2007,
  title = {A Design Science Research Methodology for Information Systems Research},
  author = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus and Chatterjee, S.},
  date = {2007-01-01},
  journaltitle = {Journal of Management Information Systems},
  shortjournal = {Journal of Management Information Systems},
  volume = {24},
  pages = {45--77},
  abstract = {The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.},
  file = {C:\Users\simon\Zotero\storage\QG9KPJQA\Peffers et al_2007_A design science research methodology for information systems research.pdf}
}

@article{peffersDesignScienceResearch2007a,
  title = {A {{Design Science Research Methodology}} for {{Information Systems Research}}},
  author = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus A. and Chatterjee, Samir},
  date = {2007-12},
  journaltitle = {Journal of Management Information Systems},
  shortjournal = {Journal of Management Information Systems},
  volume = {24},
  number = {3},
  pages = {45--77},
  issn = {0742-1222, 1557-928X},
  doi = {10.2753/MIS0742-1222240302},
  url = {https://www.tandfonline.com/doi/full/10.2753/MIS0742-1222240302},
  urldate = {2024-03-29},
  abstract = {The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\S4KX6IGZ\Peffers et al. - 2007 - A Design Science Research Methodology for Informat.pdf}
}

@article{peffersDesignScienceResearch2007b,
  title = {A Design Science Research Methodology for Information Systems Research},
  author = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus and Chatterjee, S.},
  date = {2007-01-01},
  journaltitle = {Journal of Management Information Systems},
  shortjournal = {Journal of Management Information Systems},
  volume = {24},
  pages = {45--77},
  abstract = {The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.},
  file = {C:\Users\simon\Zotero\storage\2C9PM2D8\Peffers et al_2007_A design science research methodology for information systems research.pdf}
}

@online{pointonCarbonFootprintChatGPT2023,
  title = {The Carbon Footprint of {{ChatGPT}}},
  author = {Pointon, Chris},
  date = {2023-04-19T15:13:42},
  url = {https://medium.com/@chrispointon/the-carbon-footprint-of-chatgpt-e1bc14e4cc2a},
  urldate = {2024-04-28},
  abstract = {An estimate of the carbon emissions from OpenAI’s ChatGPT chatbot service},
  langid = {english},
  organization = {Medium},
  file = {C:\Users\simon\Zotero\storage\IIL6IYHL\the-carbon-footprint-of-chatgpt-e1bc14e4cc2a.html}
}

@online{PowertopArchWiki,
  title = {Powertop - {{ArchWiki}}},
  url = {https://wiki.archlinux.org/title/powertop},
  urldate = {2024-04-30},
  file = {C:\Users\simon\Zotero\storage\FMAE4YVB\powertop.html}
}

@article{raiEditorCommentsCOVID192020,
  title = {Editor’s {{Comments}}:  {{The COVID-19 Pandemic}}:  {{Building Resilience}} with {{IS Research}}},
  shorttitle = {Editor’s {{Comments}}},
  author = {Rai, Arun},
  date = {2020-06-01},
  journaltitle = {Management Information Systems Quarterly},
  volume = {44},
  number = {2},
  pages = {iii-vii},
  issn = {ISSN 0276-7783/ISSN 2162-9730},
  url = {https://aisel.aisnet.org/misq/vol44/iss2/2},
  file = {C:\Users\simon\Zotero\storage\XTMNXLI9\2.html}
}

@online{ramsauerHopfieldNetworksAll2021,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2021-04-28},
  eprint = {2008.02217},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2008.02217},
  url = {http://arxiv.org/abs/2008.02217},
  urldate = {2024-02-28},
  abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\simon\\Zotero\\storage\\TZQL3XBP\\Ramsauer et al_2021_Hopfield Networks is All You Need.pdf;C\:\\Users\\simon\\Zotero\\storage\\U2TRQQSW\\2008.html}
}

@inproceedings{ranzatoEfficientLearningSparse2006,
  title = {Efficient {{Learning}} of {{Sparse Representations}} with an {{Energy-Based Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {family=Ranzato, given=Marc', prefix=aurelio, useprefix=false and Poultney, Christopher and Chopra, Sumit and Cun, Yann},
  date = {2006},
  volume = {19},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper/2006/hash/87f4d79e36d68c3031ccf6c55e9bbd39-Abstract.html},
  urldate = {2024-03-07},
  abstract = {We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces "stroke detectors" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.},
  file = {C:\Users\simon\Zotero\storage\NBUMP2CI\Ranzato et al_2006_Efficient Learning of Sparse Representations with an Energy-Based Model.pdf}
}

@online{raoUltimateGuideASIC,
  title = {The {{Ultimate Guide}} to {{ASIC Design}}: {{From Concept}} to {{Production}}},
  shorttitle = {The {{Ultimate Guide}} to {{ASIC Design}}},
  author = {Rao, Ravi},
  url = {https://www.wevolver.com/article/the-ultimate-guide-to-asic-design-from-concept-to-production, https://www.wevolver.com/article/the-ultimate-guide-to-asic-design-from-concept-to-production},
  urldate = {2024-04-03},
  abstract = {ASICs, or Application-Specific Integrated Circuits, are specialized chips designed to perform specific tasks with high efficiency and precision, offering a powerful solution for a wide range of industries and applications.},
  file = {C:\Users\simon\Zotero\storage\TPEHUVA5\the-ultimate-guide-to-asic-design-from-concept-to-production.html}
}

@article{raschkaMachineLearningPython2020,
  title = {Machine {{Learning}} in {{Python}}: {{Main Developments}} and {{Technology Trends}} in {{Data Science}}, {{Machine Learning}}, and {{Artificial Intelligence}}},
  shorttitle = {Machine {{Learning}} in {{Python}}},
  author = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
  date = {2020-04},
  journaltitle = {Information},
  volume = {11},
  number = {4},
  pages = {193},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2078-2489},
  doi = {10.3390/info11040193},
  url = {https://www.mdpi.com/2078-2489/11/4/193},
  urldate = {2024-04-08},
  abstract = {Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.},
  issue = {4},
  langid = {english},
  keywords = {data science,deep learning,GPU computing,machine learning,neural networks,Python},
  file = {C:\Users\simon\Zotero\storage\A8TV6MPE\Raschka et al_2020_Machine Learning in Python.pdf}
}

@online{RestrictedBoltzmannMachine,
  title = {Restricted {{Boltzmann Machine}} Features for Digit Classification},
  url = {https://scikit-learn/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html},
  urldate = {2024-04-10},
  abstract = {For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model ( ...},
  langid = {english},
  organization = {scikit-learn},
  file = {C:\Users\simon\Zotero\storage\KZ4Q4B4G\plot_rbm_logistic_classification.html}
}

@online{ReviewComparativeAnalysis,
  title = {Review and Comparative Analysis of Machine Learning Libraries for Machine Learning},
  url = {https://cyberleninka.ru/article/n/review-and-comparative-analysis-of-machine-learning-libraries-for-machine-learning},
  urldate = {2024-04-08},
  file = {C:\Users\simon\Zotero\storage\A8QR6288\review-and-comparative-analysis-of-machine-learning-libraries-for-machine-learning.html}
}

@online{robertMetropolisHastingsAlgorithm2016,
  title = {The {{Metropolis-Hastings}} Algorithm},
  author = {Robert, Christian P.},
  date = {2016-01-27},
  eprint = {1504.01896},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1504.01896},
  url = {http://arxiv.org/abs/1504.01896},
  urldate = {2024-02-27},
  abstract = {This short note is a self-contained and basic introduction to the Metropolis-Hastings algorithm, this ubiquitous tool used for producing dependent simulations from an arbitrary distribution. The document illustrates the principles of the methodology on simple examples with R codes and provides references to the recent extensions of the method.},
  pubstate = {preprint},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\simon\\Zotero\\storage\\DPSLD9DJ\\Robert_2016_The Metropolis-Hastings algorithm.pdf;C\:\\Users\\simon\\Zotero\\storage\\KKAV6T6V\\1504.html}
}

@article{rosenthalOptimalProposalDistributions2009,
  title = {Optimal {{Proposal Distributions}} and {{Adaptive MCMC}}},
  author = {Rosenthal, S.},
  date = {2009-09-29},
  journaltitle = {Handbook of Markov Chain Monte Carlo},
  shortjournal = {Handbook of Markov Chain Monte Carlo},
  abstract = {We review recent work concerning optimal proposal scalings for Metropolis-Hastings MCMC algorithms, and adaptive MCMC algorithms for trying to improve the algorithm on the y.}
}

@online{rouhaniMicroscalingDataFormats2023,
  title = {Microscaling {{Data Formats}} for {{Deep Learning}}},
  author = {Rouhani, Bita Darvish and Zhao, Ritchie and More, Ankit and Hall, Mathew and Khodamoradi, Alireza and Deng, Summer and Choudhary, Dhruv and Cornea, Marius and Dellinger, Eric and Denolf, Kristof and Dusan, Stosic and Elango, Venmugil and Golub, Maximilian and Heinecke, Alexander and James-Roxby, Phil and Jani, Dharmesh and Kolhe, Gaurav and Langhammer, Martin and Li, Ada and Melnick, Levi and Mesmakhosroshahi, Maral and Rodriguez, Andres and Schulte, Michael and Shafipour, Rasoul and Shao, Lei and Siu, Michael and Dubey, Pradeep and Micikevicius, Paulius and Naumov, Maxim and Verrilli, Colin and Wittig, Ralph and Burger, Doug and Chung, Eric},
  date = {2023-10-19},
  eprint = {2310.10537},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.10537},
  urldate = {2024-04-19},
  abstract = {Narrow bit-width data formats are key to reducing the computational and storage costs of modern deep learning applications. This paper evaluates Microscaling (MX) data formats that combine a per-block scaling factor with narrow floating-point and integer types for individual elements. MX formats balance the competing needs of hardware efficiency, model accuracy, and user friction. Empirical results on over two dozen benchmarks demonstrate practicality of MX data formats as a drop-in replacement for baseline FP32 for AI inference and training with low user friction. We also show the first instance of training generative language models at sub-8-bit weights, activations, and gradients with minimal accuracy loss and no modifications to the training recipe.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\simon\\Zotero\\storage\\VUB6R5RW\\Rouhani et al_2023_Microscaling Data Formats for Deep Learning.pdf;C\:\\Users\\simon\\Zotero\\storage\\WRD4BKAW\\2310.html}
}

@online{rouhaniSharedMicroexponentsLittle2023,
  title = {With {{Shared Microexponents}}, {{A Little Shifting Goes}} a {{Long Way}}},
  author = {Rouhani, Bita and Zhao, Ritchie and Elango, Venmugil and Shafipour, Rasoul and Hall, Mathew and Mesmakhosroshahi, Maral and More, Ankit and Melnick, Levi and Golub, Maximilian and Varatkar, Girish and Shao, Lei and Kolhe, Gaurav and Melts, Dimitry and Klar, Jasmine and L'Heureux, Renee and Perry, Matt and Burger, Doug and Chung, Eric and Deng, Zhaoxia and Naghshineh, Sam and Park, Jongsoo and Naumov, Maxim},
  date = {2023-04-12},
  eprint = {2302.08007},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.08007},
  urldate = {2024-04-19},
  abstract = {This paper introduces Block Data Representations (BDR), a framework for exploring and evaluating a wide spectrum of narrow-precision formats for deep learning. It enables comparison of popular quantization standards, and through BDR, new formats based on shared microexponents (MX) are identified, which outperform other state-of-the-art quantization approaches, including narrow-precision floating-point and block floating-point. MX utilizes multiple levels of quantization scaling with ultra-fine scaling factors based on shared microexponents in the hardware. The effectiveness of MX is demonstrated on real-world models including large-scale generative pretraining and inferencing, and production-scale recommendation systems.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Hardware Architecture,Computer Science - Machine Learning},
  file = {C\:\\Users\\simon\\Zotero\\storage\\I58YRHNC\\Rouhani et al_2023_With Shared Microexponents, A Little Shifting Goes a Long Way.pdf;C\:\\Users\\simon\\Zotero\\storage\\38XLRHEL\\2302.html}
}

@inproceedings{salakhutdinovDeepBoltzmannMachines2009,
  title = {Deep {{Boltzmann Machines}}},
  booktitle = {Proceedings of the {{Twelth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  date = {2009-04-15},
  pages = {448--455},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v5/salakhutdinov09a.html},
  urldate = {2024-02-16},
  abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\V8LJQ6Z2\Deep Boltzmann Machines.pdf}
}

@article{sarhadiStateArtHardware2015,
  title = {State of the Art: Hardware in the Loop Modeling and Simulation with Its Applications in Design, Development and Implementation of System and Control Software},
  shorttitle = {State of the Art},
  author = {Sarhadi, Pouria and Yousefpour, Samereh},
  date = {2015-12-01},
  journaltitle = {International Journal of Dynamics and Control},
  shortjournal = {Int. J. Dynam. Control},
  volume = {3},
  number = {4},
  pages = {470--479},
  issn = {2195-2698},
  doi = {10.1007/s40435-014-0108-3},
  url = {https://doi.org/10.1007/s40435-014-0108-3},
  urldate = {2024-04-02},
  abstract = {Nowadays due to the technology development and use of digital computers in various systems, need for development of high performance and robust software is attracting great attentions. Because of increasing complexity in algorithms and implementation hardware for embedded systems, proper simulation tools are required. In sophisticated systems design, hardware in the loop (HIL) simulation is known as a prominent simulation tool before realistic tests of the system and a step after software simulation. Simultaneously it can be used for verification and validation of automation and control software. HIL has had an historical background in aerospace industries. Recently, this tool has spread in different steps of system life cycle such as design, development, implementation and test of various applications including automobile industry, shipbuilding, power lines, robotic systems and etc. Utilizing a suitable hardware in the loop laboratory, in system design stages is a practical way to increase the system reliability and efficiency as well as value of product. Also, by proper investigation in this modelling and simulation method, many errors can be avoided in design procedure of software and hardware as well as their interconnections. In this study, structure and components of an hardware in the loop laboratory for different systems are explored, also it is tried to more evaluate the applications of HIL simulations in dynamics and control engineering. At last, general structure of an hardware in the loop lab for diverse industries is proposed and discussed.},
  langid = {english},
  keywords = {Embedded systems,Hardware in the loop (HIL),Modelling & Simulation,System design},
  file = {C:\Users\simon\Zotero\storage\Z2472FGZ\Sarhadi_Yousefpour_2015_State of the art.pdf}
}

@article{sarkerMachineLearningAlgorithms2021,
  title = {Machine {{Learning}}: {{Algorithms}}, {{Real-World Applications}} and {{Research Directions}}},
  shorttitle = {Machine {{Learning}}},
  author = {Sarker, Iqbal H.},
  date = {2021-03-22},
  journaltitle = {SN Computer Science},
  shortjournal = {SN COMPUT. SCI.},
  volume = {2},
  number = {3},
  pages = {160},
  issn = {2661-8907},
  doi = {10.1007/s42979-021-00592-x},
  url = {https://doi.org/10.1007/s42979-021-00592-x},
  urldate = {2024-02-04},
  abstract = {In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated~applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study’s key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity~systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers~in various real-world situations and~application areas, particularly from the technical point of view.},
  langid = {english},
  keywords = {Artificial intelligence,Data science,Data-driven decision-making,Deep learning,Intelligent applications,Machine learning,Predictive analytics},
  file = {C:\Users\simon\Zotero\storage\2FB2ANZT\Sarker - 2021 - Machine Learning Algorithms, Real-World Applicati.pdf}
}

@article{schlammingerCoolWayMeasure2014,
  title = {A Cool Way to Measure Big {{G}}},
  author = {Schlamminger, Stephan},
  date = {2014-06},
  journaltitle = {Nature},
  volume = {510},
  number = {7506},
  pages = {478--480},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature13507},
  url = {https://www.nature.com/articles/nature13507},
  urldate = {2024-02-21},
  abstract = {Published results of the gravitational constant, a measure of the strength of gravity, have failed to converge. An approach that uses cold atoms provides a new data point in the quest to determine this fundamental constant. See Letter p.518},
  issue = {7506},
  langid = {english},
  keywords = {gelesen,Physics,Quantum physics,ungelesen,zitiert}
}

@online{ScopeEUEmissions,
  title = {Scope of the {{EU Emissions Trading System}} - {{European Commission}}},
  url = {https://climate.ec.europa.eu/eu-action/eu-emissions-trading-system-eu-ets/scope-eu-emissions-trading-system_en},
  urldate = {2024-03-10},
  abstract = {What countries, sectors and gases does the EU ETS cover?},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\7DWUCWV4\scope-eu-emissions-trading-system_en.html}
}

@article{shaikhEnablingTechnologiesGreen2017,
  title = {Enabling {{Technologies}} for {{Green Internet}} of {{Things}}},
  author = {Shaikh, Faisal Karim and Zeadally, Sherali and Exposito, Ernesto},
  date = {2017-06},
  journaltitle = {IEEE Systems Journal},
  volume = {11},
  number = {2},
  pages = {983--994},
  issn = {1937-9234},
  doi = {10.1109/JSYST.2015.2415194},
  url = {https://ieeexplore.ieee.org/abstract/document/7088546?casa_token=GN6mC9f9qGQAAAAA:yWX7AfSZgxtMd5n09pMmE6c4pXmpd7jdqDAea73K-3rkGpQzPrpseprX3dE4A-YpoZt5HcrCfQw},
  urldate = {2024-03-10},
  abstract = {Recent technological advances have led to an increase in the carbon footprint. Energy efficiency in the Internet of Things (IoT) has been attracting a lot of attention from researchers and designers over the last couple of years, paving the way for an emerging area called green IoT. There are various aspects (such as key enablers, communications, services, and applications) of IoT, where efficient utilization of energy is needed to enable a green IoT environment. We explore and discuss how the various enabling technologies (such as the Internet, smart objects, sensors, etc.) can be efficiently deployed to achieve a green IoT. Furthermore, we also review various IoT applications, projects and standardization efforts that are currently under way. Finally, we identify some of the emerging challenges that need to be addressed in the future to enable a green IoT.},
  eventtitle = {{{IEEE Systems Journal}}},
  keywords = {Air pollution,Cloud computing,green Internet of Things (IoT),Green products,Internet,machine-to-machine (M2M),Monitoring,near-field communication (NFC),radio-frequency identification (RFID),Radiofrequency identification,Wireless sensor networks,wireless sensor networks (WSNs)},
  file = {C\:\\Users\\simon\\Zotero\\storage\\J5ZTRDVP\\Shaikh et al_2017_Enabling Technologies for Green Internet of Things.pdf;C\:\\Users\\simon\\Zotero\\storage\\9IQY6YUP\\7088546.html}
}

@inproceedings{singhChatGPTGoogle2023,
  title = {Chat {{GPT}} \& {{Google Bard AI}}: {{A Review}}},
  shorttitle = {Chat {{GPT}} \& {{Google Bard AI}}},
  booktitle = {2023 {{International Conference}} on {{IoT}}, {{Communication}} and {{Automation Technology}} ({{ICICAT}})},
  author = {Singh, Shashi Kant and Kumar, Shubham and Mehra, Pawan Singh},
  date = {2023-06},
  pages = {1--6},
  doi = {10.1109/ICICAT57735.2023.10263706},
  url = {https://ieeexplore.ieee.org/abstract/document/10263706?casa_token=JMHwBzQgxnwAAAAA:70OnfYs5ECetZhuq8D_F3QXyua1Xu65rL0a_Ywve3mch00UAeSsOyVjhWCUvDuBpMX83NAbpUpM},
  urldate = {2024-02-23},
  abstract = {In today's world, Artificial Intelligence is one of the deepest and newest things to learn and research. Research on Artificial Intelligence is based on some goals and the use of some particular tools. One of the most important and latest innovations in the field of Artificial Intelligence is Chat GPT. It has created a storm in the cyber world after the launch of its prototype. In this paper, we have done a survey analysis on Chat GPT. The emphasis is on Chatbots, Chat GPT and Google Bard AI. The objective of this paper is to let the readers know about the analysis of various reviews and research work done on the topics of Chatbots, Chat GPT and Google Bard AI along with a brief comparison between them. Through this paper, we have provided a pool of knowledge about Chatbots, Chat GPT or Google Bard AI which can pave way for researchers.},
  eventtitle = {2023 {{International Conference}} on {{IoT}}, {{Communication}} and {{Automation Technology}} ({{ICICAT}})},
  keywords = {Artificial Intelligence,Automation,Chat GPT,Chatbots,Google Bard AI,Internet,Prototypes,Storms,Surveys,Technological innovation,ungelesen,zitiert},
  file = {C\:\\Users\\simon\\Zotero\\storage\\ZPZITWGX\\Singh et al_2023_Chat GPT & Google Bard AI.pdf;C\:\\Users\\simon\\Zotero\\storage\\D6YHP5BY\\10263706.html}
}

@inproceedings{sipolaArtificialIntelligenceIoT2022,
  title = {Artificial {{Intelligence}} in the {{IoT Era}}: {{A Review}} of {{Edge AI Hardware}} and {{Software}}},
  shorttitle = {Artificial {{Intelligence}} in the {{IoT Era}}},
  booktitle = {2022 31st {{Conference}} of {{Open Innovations Association}} ({{FRUCT}})},
  author = {Sipola, Tuomo and Alatalo, Janne and Kokkonen, Tero and Rantonen, Mika},
  date = {2022-04},
  pages = {320--331},
  doi = {10.23919/FRUCT54823.2022.9770931},
  url = {https://ieeexplore.ieee.org/abstract/document/9770931},
  urldate = {2024-03-18},
  abstract = {The modern trend of moving artificial intelligence computation near to the origin of data sources has increased the demand for new hardware and software suitable for such environments. We carried out a scoping study to find the current resources used when developing Edge AI applications. Due to the nature of the topic, the research combined scientific sources with product information and software project sources. The paper is structured as follows. In the first part, Edge AI applications are briefly discussed followed by hardware options and finally, the software used to develop AI models is described. There are various hardware products available, and we found as many as possible for this research to identify the best-known manufacturers. We describe the devices in the following categories: artificial intelligence accelerators and processors, field-programmable gate arrays, system-on-a-chip devices, system-on-modules, and full computers from development boards to servers. There seem to be three trends in Edge AI software development: neural network optimization, mobile device software and microcontroller software. We discussed these emerging fields and how the special challenges of low power consumption and machine learning computation are being taken into account. Our findings suggest that the Edge AI ecosystem is currently developing, and it has its own challenges to which vendors and developers are responding.},
  eventtitle = {2022 31st {{Conference}} of {{Open Innovations Association}} ({{FRUCT}})},
  keywords = {Biological system modeling,Data models,Hardware,Microcontrollers,Program processors,Software,System-on-chip},
  file = {C\:\\Users\\simon\\Zotero\\storage\\64U7JGN4\\Sipola et al_2022_Artificial Intelligence in the IoT Era.pdf;C\:\\Users\\simon\\Zotero\\storage\\CGQ6GB2N\\9770931.html}
}

@online{SklearnDatasetsLoad_digits,
  title = {Sklearn.Datasets.Load\_digits},
  url = {https://scikit-learn/stable/modules/generated/sklearn.datasets.load_digits.html},
  urldate = {2024-04-10},
  abstract = {Examples using sklearn.datasets.load\_digits: Recognizing hand-written digits Recognizing hand-written digits A demo of K-Means clustering on the handwritten digits data A demo of K-Means clustering...},
  langid = {english},
  organization = {scikit-learn},
  file = {C:\Users\simon\Zotero\storage\YB9N6MCW\sklearn.datasets.load_digits.html}
}

@article{spechtProbabilisticNeuralNetworks1990,
  title = {Probabilistic Neural Networks},
  author = {Specht, Donald F.},
  date = {1990-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {3},
  number = {1},
  pages = {109--118},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(90)90049-Q},
  url = {https://www.sciencedirect.com/science/article/pii/089360809090049Q},
  urldate = {2024-03-07},
  abstract = {By replacing the sigmoid activation function often used in neural networks with an exponential function, a probabilistic neural network (PNN) that can compute nonlinear decision boundaries which approach the Bayes optimal is formed. Alternate activation functions having similar properties are also discussed. A fourlayer neural network of the type proposed can map any input pattern to any number of classifications. The decision boundaries can be modified in real-time using new data as they become available, and can be implemented using artificial hardware “neurons” that operate entirely in parallel. Provision is also made for estimating the probability and reliability of a classification as well as making the decision. The technique offers a tremendous speed advantage for problems in which the incremental adaptation time of back propagation is a significant fraction of the total computation time. For one application, the PNN paradigm was 200,000 times faster than back-propagation.},
  keywords = {“Neuron”,Associative memory,Bayes strategy,Neural network,Parallel processor,Parzen window,Pattern recognition,Probability density function},
  file = {C:\Users\simon\Zotero\storage\E2F3DQWV\089360809090049Q.html}
}

@article{sungPerspectiveReviewMemristive2018,
  title = {Perspective: {{A}} Review on Memristive Hardware for Neuromorphic Computation},
  shorttitle = {Perspective},
  author = {Sung, Changhyuck and Hwang, Hyunsang and Yoo, In Kyeong},
  date = {2018-10-05},
  journaltitle = {Journal of Applied Physics},
  shortjournal = {Journal of Applied Physics},
  volume = {124},
  number = {15},
  pages = {151903},
  issn = {0021-8979},
  doi = {10.1063/1.5037835},
  url = {https://doi.org/10.1063/1.5037835},
  urldate = {2024-02-15},
  abstract = {Neuromorphic computation is one of the axes of parallel distributed processing, and memristor-based synaptic weight is considered as a key component of this type of computation. However, the material properties of memristors, including material related physics, are not yet matured. In parallel with memristors, CMOS based Graphics Processing Unit, Field Programmable Gate Array, and Application Specific Integrated Circuit are also being developed as dedicated artificial intelligence (AI) chips for fast computation. Therefore, it is necessary to analyze the competitiveness of the memristor-based neuromorphic device in order to position the memristor in the appropriate position of the future AI ecosystem. In this article, the status of memristor-based neuromorphic computation was analyzed on the basis of papers and patents to identify the competitiveness of the memristor properties by reviewing industrial trends and academic pursuits. In addition, material issues and challenges are discussed for implementing the memristor-based neural processor.},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\78YATYXK\Sung et al. - 2018 - Perspective A review on memristive hardware for n.pdf}
}

@article{sungPerspectiveReviewMemristive2018a,
  title = {Perspective: {{A}} Review on Memristive Hardware for Neuromorphic Computation},
  shorttitle = {Perspective},
  author = {Sung, Changhyuck and Hwang, Hyunsang and Yoo, In Kyeong},
  date = {2018-10-05},
  journaltitle = {Journal of Applied Physics},
  shortjournal = {Journal of Applied Physics},
  volume = {124},
  number = {15},
  pages = {151903},
  issn = {0021-8979},
  doi = {10.1063/1.5037835},
  url = {https://doi.org/10.1063/1.5037835},
  urldate = {2024-03-21},
  abstract = {Neuromorphic computation is one of the axes of parallel distributed processing, and memristor-based synaptic weight is considered as a key component of this type of computation. However, the material properties of memristors, including material related physics, are not yet matured. In parallel with memristors, CMOS based Graphics Processing Unit, Field Programmable Gate Array, and Application Specific Integrated Circuit are also being developed as dedicated artificial intelligence (AI) chips for fast computation. Therefore, it is necessary to analyze the competitiveness of the memristor-based neuromorphic device in order to position the memristor in the appropriate position of the future AI ecosystem. In this article, the status of memristor-based neuromorphic computation was analyzed on the basis of papers and patents to identify the competitiveness of the memristor properties by reviewing industrial trends and academic pursuits. In addition, material issues and challenges are discussed for implementing the memristor-based neural processor.},
  file = {C\:\\Users\\simon\\Zotero\\storage\\78T4GKER\\Sung et al_2018_Perspective.pdf;C\:\\Users\\simon\\Zotero\\storage\\WPML6Z4A\\Perspective-A-review-on-memristive-hardware-for.html}
}

@article{supriAsianStockIndex2023,
  title = {Asian {{Stock Index Price Prediction Analysis Using Comparison}} of {{Split Data Training}} and {{Data Testing}}},
  author = {Supri, Baharman and Rudianto and Abdurohim and Mawadah, Badriatul and Ali, Helmi},
  date = {2023-08-01},
  journaltitle = {JEMSI (Jurnal Ekonomi, Manajemen, dan Akuntansi)},
  volume = {9},
  number = {4},
  pages = {1403--1408},
  issn = {2579-5635},
  doi = {10.35870/jemsi.v9i4.1339},
  url = {http://journal.lembagakita.org/index.php/jemsi/article/view/1339},
  urldate = {2024-04-12},
  abstract = {This study implements stock index price predictions using the LSTM method, where one of the processes in data management before running with the LSTM method is data split. This study also looks for the most appropriate split data ratio in predicting stock index prices to minimize error rates and differences in forecasted prices and original prices because in previous studies there were several rules of thumb in dividing data, so it is necessary to compare the most appropriate ratios in this research. Based on the evaluation process, the error value was found from nine split data ratios that were run by five ratios which produced a predictive graph line shape that resembled the validation line. Three datasets, namely split data ratios of 80:20, 70:30, and 60:40, are the ratios that get the lowest error values based on the RMSE, MSE, MAPE, and MAE values in the five stock index datasets. The three ratios are then compared again by looking at the average percentage difference between the validation price and the predicted price for the next working day, and it is found that the ratio of 80:20 is the most suitable split data ratio for predicting the stock index price for the next working day, with a level of difference in the average value between the original price and the predicted price on the stock index of 1.3\%. While the ratio of 70:30 has an average predicted value of five stock index datasets of 1.9\% and a ratio of 60:40 of 1.8\%.},
  issue = {4},
  langid = {english},
  keywords = {data split,datasets,original price,stock index},
  file = {C:\Users\simon\Zotero\storage\6DD8AJ2T\Supri et al_2023_Asian Stock Index Price Prediction Analysis Using Comparison of Split Data.pdf}
}

@online{SurveyHardwareAccelerators,
  title = {A Survey on Hardware Accelerators: {{Taxonomy}}, Trends, Challenges, and Perspectives - {{ScienceDirect}}},
  url = {https://www.sciencedirect.com/science/article/pii/S1383762122001138},
  urldate = {2024-03-19},
  file = {C:\Users\simon\Zotero\storage\QHZEQXF9\S1383762122001138.html}
}

@online{SustainableFinanceInvestment,
  title = {Sustainable Finance and Investment: {{Review}} and Research Agenda - {{Cunha}} - 2021 - {{Business Strategy}} and the {{Environment}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/bse.2842},
  urldate = {2024-03-13},
  file = {C:\Users\simon\Zotero\storage\BKBGD3N9\bse.html}
}

@article{tanahashiApplicationIsingMachines2019,
  title = {Application of {{Ising Machines}} and a {{Software Development}} for {{Ising Machines}}},
  author = {Tanahashi, Kotaro and Takayanagi, Shinichi and Motohashi, Tomomitsu and Tanaka, Shu},
  date = {2019-06-15},
  journaltitle = {Journal of the Physical Society of Japan},
  shortjournal = {J. Phys. Soc. Jpn.},
  volume = {88},
  number = {6},
  pages = {061010},
  publisher = {The Physical Society of Japan},
  issn = {0031-9015},
  doi = {10.7566/JPSJ.88.061010},
  url = {https://journals.jps.jp/doi/full/10.7566/JPSJ.88.061010},
  urldate = {2024-03-21},
  abstract = {An online advertisement optimization, which can be represented by a combinatorial optimization problem is performed using D-Wave 2000Q, a quantum annealing machine. To optimize the online advertisement allocation optimization, we introduce a generalized version of the Markowitz mean-variance model which is a basic model of portfolio optimization. The obtained optimization performance using D-Wave 2000Q is higher than that using the greedy method which is a conventional method. Additionally, to conveniently use Ising machines including a quantum annealing machine, new software called PyQUBO is developed. The first half of the paper gives a review of several combinatorial optimization problems and how to represent them using the Ising model or the quadratic unconstrained binary optimization (QUBO) form. We show the results of the online advertisement allocation optimization and the explanation of PyQUBO in the last half of the paper.},
  file = {C:\Users\simon\Zotero\storage\7QUY6K2C\Tanahashi et al_2019_Application of Ising Machines and a Software Development for Ising Machines.pdf}
}

@online{tanakaReductionAutocorrelationHMC2017,
  title = {Towards Reduction of Autocorrelation in {{HMC}} by Machine Learning},
  author = {Tanaka, Akinori and Tomiya, Akio},
  date = {2017-12-11},
  eprint = {1712.03893},
  eprinttype = {arxiv},
  eprintclass = {cond-mat, physics:hep-lat, stat},
  doi = {10.48550/arXiv.1712.03893},
  url = {http://arxiv.org/abs/1712.03893},
  urldate = {2024-04-16},
  abstract = {In this paper we propose new algorithm to reduce autocorrelation in Markov chain Monte-Carlo algorithms for euclidean field theories on the lattice. Our proposing algorithm is the Hybrid Monte-Carlo algorithm (HMC) with restricted Boltzmann machine. We examine the validity of the algorithm by employing the phi-fourth theory in three dimension. We observe reduction of the autocorrelation both in symmetric and broken phase as well. Our proposing algorithm provides consistent central values of expectation values of the action density and one-point Green's function with ones from the original HMC in both the symmetric phase and broken phase within the statistical error. On the other hand, two-point Green's functions have slight difference between one calculated by the HMC and one by our proposing algorithm in the symmetric phase. Furthermore, near the criticality, the distribution of the one-point Green's function differs from the one from HMC. We discuss the origin of discrepancies and its improvement.},
  pubstate = {preprint},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,High Energy Physics - Lattice,Statistics - Machine Learning},
  file = {C\:\\Users\\simon\\Zotero\\storage\\UNY2Q82F\\Tanaka_Tomiya_2017_Towards reduction of autocorrelation in HMC by machine learning.pdf;C\:\\Users\\simon\\Zotero\\storage\\5VAKI9IT\\1712.html}
}

@article{teroRulesBiologicallyInspired2010,
  title = {Rules for {{Biologically Inspired Adaptive Network Design}}},
  author = {Tero, Atsushi and Takagi, Seiji and Saigusa, Tetsu and Ito, Kentaro and Bebber, Dan P. and Fricker, Mark D. and Yumiki, Kenji and Kobayashi, Ryo and Nakagaki, Toshiyuki},
  date = {2010-01-22},
  journaltitle = {Science},
  volume = {327},
  number = {5964},
  pages = {439--442},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1177894},
  url = {https://www.science.org/doi/10.1126/science.1177894},
  urldate = {2024-05-01},
  abstract = {Transport networks are ubiquitous in both social and biological systems. Robust network performance involves a complex trade-off involving cost, transport efficiency, and fault tolerance. Biological networks have been honed by many cycles of evolutionary selection pressure and are likely to yield reasonable solutions to such combinatorial optimization problems. Furthermore, they develop without centralized control and may represent a readily scalable solution for growing networks in general. We show that the slime mold Physarum polycephalum forms networks with comparable efficiency, fault tolerance, and cost to those of real-world infrastructure networks—in this case, the Tokyo rail system. The core mechanisms needed for adaptive network formation can be captured in a biologically inspired mathematical model that may be useful to guide network construction in other domains.},
  file = {C:\Users\simon\Zotero\storage\J5ESAGQ5\Tero et al_2010_Rules for Biologically Inspired Adaptive Network Design.pdf}
}

@article{tomlinsonCarbonEmissionsWriting2024,
  title = {The Carbon Emissions of Writing and Illustrating Are Lower for {{AI}} than for Humans},
  author = {Tomlinson, Bill and Black, Rebecca W. and Patterson, Donald J. and Torrance, Andrew W.},
  date = {2024-02-14},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {14},
  number = {1},
  pages = {3732},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-54271-x},
  url = {https://www.nature.com/articles/s41598-024-54271-x},
  urldate = {2024-04-28},
  abstract = {As AI systems proliferate, their greenhouse gas emissions are an increasingly important concern for human societies. In this article, we present a comparative analysis of the carbon emissions associated with AI systems (ChatGPT, BLOOM, DALL-E2, Midjourney) and human individuals performing equivalent writing and illustrating tasks. Our findings reveal that AI systems emit between 130 and 1500 times less CO2e per page of text generated compared to human writers, while AI illustration systems emit between 310 and 2900 times less CO2e per image than their human counterparts. Emissions analyses do not account for social impacts such as professional displacement, legality, and rebound effects. In addition, AI is not a substitute for all human tasks. Nevertheless, at present, the use of AI holds the potential to carry out several major activities at much lower emission levels than can humans.},
  langid = {english},
  keywords = {Climate-change mitigation,Computer science},
  file = {C:\Users\simon\Zotero\storage\STL262C2\Tomlinson et al_2024_The carbon emissions of writing and illustrating are lower for AI than for.pdf}
}

@online{UebereinkommenParisRat,
  title = {Übereinkommen von Paris: Rat übermittelt aktualisierten NDC im Namen der EU und der Mitgliedstaaten},
  shorttitle = {Übereinkommen von Paris},
  url = {https://www.consilium.europa.eu/de/press/press-releases/2023/10/16/paris-agreement-council-submits-updated-ndc-on-behalf-of-eu-and-member-states/},
  urldate = {2024-03-10},
  abstract = {Der Rat hat heute die Vorlage eines aktualisierten national festgelegten Beitrags (NDC) der EU zum UNFCCC gebilligt.},
  langid = {ngerman},
  file = {C:\Users\simon\Zotero\storage\9UJ8WPGB\paris-agreement-council-submits-updated-ndc-on-behalf-of-eu-and-member-states.html}
}

@article{upadhyaOverviewRestrictedBoltzmann2019,
  title = {An {{Overview}} of {{Restricted Boltzmann Machines}}},
  author = {Upadhya, Vidyadhar and Sastry, P.},
  date = {2019-02-18},
  journaltitle = {Journal of the Indian Institute of Science},
  shortjournal = {Journal of the Indian Institute of Science},
  volume = {99},
  doi = {10.1007/s41745-019-0102-z},
  abstract = {The restricted Boltzmann machine (RBM) is a two-layered network of stochastic units with undirected connections between pairs of units in the two layers. The two layers of nodes are called visible and hidden nodes. In an RBM, there are no connections from visible to visible or hidden to hidden nodes. RBMs are used mainly as a generative model. They can be suitably modified to perform classification tasks also. They are among the basic building blocks of other deep learning models such as deep Boltzmann machine and deep belief networks. The aim of this article is to give a tutorial introduction to the restricted Boltzmann machines and to review the evolution of this model.},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\JAGD4W2N\An Overview of Restricted Boltzmann Machines.pdf}
}

@article{uusitaloOverviewMethodsEvaluate2015,
  title = {An Overview of Methods to Evaluate Uncertainty of Deterministic Models in Decision Support},
  author = {Uusitalo, Laura and Lehikoinen, Annukka and Helle, Inari and Myrberg, Kai},
  date = {2015-01-01},
  journaltitle = {Environmental Modelling \& Software},
  shortjournal = {Environmental Modelling \& Software},
  volume = {63},
  pages = {24--31},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2014.09.017},
  url = {https://www.sciencedirect.com/science/article/pii/S1364815214002813},
  urldate = {2024-03-07},
  abstract = {There is an increasing need for environmental management advice that is wide-scoped, covering various interlinked policies, and realistic about the uncertainties related to the possible management actions. To achieve this, efficient decision support integrates the results of pre-existing models. Many environmental models are deterministic, but the uncertainty of their outcomes needs to be estimated when they are utilized for decision support. We review various methods that have been or could be applied to evaluate the uncertainty related to deterministic models' outputs. We cover expert judgement, model emulation, sensitivity analysis, temporal and spatial variability in the model outputs, the use of multiple models, and statistical approaches, and evaluate when these methods are appropriate and what must be taken into account when utilizing them. The best way to evaluate the uncertainty depends on the definitions of the source models and the amount and quality of information available to the modeller.},
  keywords = {Decision support,Deterministic,Environmental modelling,Probabilistic,Uncertainty},
  file = {C\:\\Users\\simon\\Zotero\\storage\\43USNQTT\\Uusitalo et al_2015_An overview of methods to evaluate uncertainty of deterministic models in.pdf;C\:\\Users\\simon\\Zotero\\storage\\Y4SYH9EN\\S1364815214002813.html}
}

@online{verdonQuantumHamiltonianBasedModels2019,
  title = {Quantum {{Hamiltonian-Based Models}} and the {{Variational Quantum Thermalizer Algorithm}}},
  author = {Verdon, Guillaume and Marks, Jacob and Nanda, Sasha and Leichenauer, Stefan and Hidary, Jack},
  date = {2019-10-04},
  eprint = {1910.02071},
  eprinttype = {arxiv},
  eprintclass = {quant-ph},
  url = {http://arxiv.org/abs/1910.02071},
  urldate = {2024-02-19},
  abstract = {We introduce a new class of generative quantum-neural-network-based models called Quantum Hamiltonian-Based Models (QHBMs). In doing so, we establish a paradigmatic approach for quantum-probabilistic hybrid variational learning, where we efficiently decompose the tasks of learning classical and quantum correlations in a way which maximizes the utility of both classical and quantum processors. In addition, we introduce the Variational Quantum Thermalizer (VQT) for generating the thermal state of a given Hamiltonian and target temperature, a task for which QHBMs are naturally well-suited. The VQT can be seen as a generalization of the Variational Quantum Eigensolver (VQE) to thermal states: we show that the VQT converges to the VQE in the zero temperature limit. We provide numerical results demonstrating the efficacy of these techniques in illustrative examples. We use QHBMs and the VQT on Heisenberg spin systems, we apply QHBMs to learn entanglement Hamiltonians and compression codes in simulated free Bosonic systems, and finally we use the VQT to prepare thermal Fermionic Gaussian states for quantum simulation.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Quantum Physics,ungelesen,zitiert},
  file = {C\:\\Users\\simon\\Zotero\\storage\\3HTPGVXD\\Verdon et al_2019_Quantum Hamiltonian-Based Models and the Variational Quantum Thermalizer.pdf;C\:\\Users\\simon\\Zotero\\storage\\5WMRJVH6\\1910.html}
}

@online{ViewEnvironmentalImpact,
  title = {View of {{The Environmental Impact}} of {{AI}}: {{A Case Study}} of {{Water Consumption}} by {{Chat GPT}}},
  url = {https://puiij.com/index.php/research/article/view/39/23},
  urldate = {2024-04-28},
  file = {C:\Users\simon\Zotero\storage\5PGSD4JM\23.html}
}

@online{wangOscillatorbasedIsingMachine2017,
  title = {Oscillator-Based {{Ising Machine}}},
  author = {Wang, Tianshi and Roychowdhury, Jaijeet},
  date = {2017-10-12},
  eprint = {1709.08102},
  eprinttype = {arxiv},
  eprintclass = {physics},
  doi = {10.48550/arXiv.1709.08102},
  url = {http://arxiv.org/abs/1709.08102},
  urldate = {2024-02-15},
  abstract = {Many combinatorial optimization problems can be mapped to finding the ground states of the corresponding Ising Hamiltonians. The physical systems that can solve optimization problems in this way, namely Ising machines, have been attracting more and more attention recently. Our work shows that Ising machines can be realized using almost any nonlinear self-sustaining oscillators with logic values encoded in their phases. Many types of such oscillators are readily available for large-scale integration, with potentials in high-speed and low-power operation. In this paper, we describe the operation and mechanism of oscillator-based Ising machines. The feasibility of our scheme is demonstrated through several examples in simulation and hardware, among which a simulation study reports average solutions exceeding those from state-of-art Ising machines on a benchmark combinatorial optimization problem of size 2000.},
  pubstate = {preprint},
  keywords = {Computer Science - Emerging Technologies,Physics - Computational Physics,ungelesen},
  file = {C:\Users\simon\Zotero\storage\Y6S4QDT9\Oscillator-based Ising Machine.pdf}
}

@online{wangOscillatorbasedIsingMachine2017a,
  title = {Oscillator-Based {{Ising Machine}}},
  author = {Wang, Tianshi and Roychowdhury, Jaijeet},
  date = {2017-10-12},
  eprint = {1709.08102},
  eprinttype = {arxiv},
  eprintclass = {physics},
  doi = {10.48550/arXiv.1709.08102},
  url = {http://arxiv.org/abs/1709.08102},
  urldate = {2024-03-21},
  abstract = {Many combinatorial optimization problems can be mapped to finding the ground states of the corresponding Ising Hamiltonians. The physical systems that can solve optimization problems in this way, namely Ising machines, have been attracting more and more attention recently. Our work shows that Ising machines can be realized using almost any nonlinear self-sustaining oscillators with logic values encoded in their phases. Many types of such oscillators are readily available for large-scale integration, with potentials in high-speed and low-power operation. In this paper, we describe the operation and mechanism of oscillator-based Ising machines. The feasibility of our scheme is demonstrated through several examples in simulation and hardware, among which a simulation study reports average solutions exceeding those from state-of-art Ising machines on a benchmark combinatorial optimization problem of size 2000.},
  pubstate = {preprint},
  keywords = {Computer Science - Emerging Technologies,Physics - Computational Physics},
  file = {C\:\\Users\\simon\\Zotero\\storage\\625INYTZ\\Wang_Roychowdhury_2017_Oscillator-based Ising Machine.pdf;C\:\\Users\\simon\\Zotero\\storage\\7RPKECFW\\1709.html}
}

@inproceedings{wangResearchApplicationGradient2021,
  title = {Research on the {{Application}} of {{Gradient Descent Algorithm}} in {{Machine Learning}}},
  booktitle = {2021 {{International Conference}} on {{Computer Network}}, {{Electronic}} and {{Automation}} ({{ICCNEA}})},
  author = {Wang, Xin and Yan, Liting and Zhang, Qizhi},
  date = {2021-09},
  pages = {11--15},
  doi = {10.1109/ICCNEA53019.2021.00014},
  url = {https://ieeexplore.ieee.org/document/9603742},
  urldate = {2024-03-08},
  abstract = {The gradient descent algorithm is a type of optimization algorithm that is widely used to solve machine learning algorithm model parameters. Through continuous iteration, it obtains the gradient of the objective function, gradually approaches the optimal solution of the objective function, and finally obtains the minimum loss function and related parameters. The gradient descent algorithm is frequently used in the solution process of logical regression, which is a common binary classification approach. This paper compares and analyzes the differences between batch gradient descent and its derivative algorithms — stochastic gradient descent algorithm and mini- batch gradient descent algorithm in terms of iteration number, loss function through experiments, and provides some suggestions on how to pick the best algorithm for the logistic regression binary task in machine learning.},
  eventtitle = {2021 {{International Conference}} on {{Computer Network}}, {{Electronic}} and {{Automation}} ({{ICCNEA}})},
  keywords = {Classification algorithms,Gradient Descent,Linear programming,Logistic Regression,Machine learning,Machine Learning,Machine learning algorithms,Stochastic processes,Task analysis,Training data},
  file = {C:\Users\simon\Zotero\storage\EYETDMNB\Wang et al_2021_Research on the Application of Gradient Descent Algorithm in Machine Learning.pdf}
}

@online{WhatEUETS,
  title = {What Is the {{EU ETS}}? - {{European Commission}}},
  shorttitle = {What Is the {{EU ETS}}?},
  url = {https://climate.ec.europa.eu/eu-action/eu-emissions-trading-system-eu-ets/what-eu-ets_en},
  urldate = {2024-03-10},
  abstract = {A ‘cap and trade’ system to reduce emissions via a carbon market.},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\97AF6AJM\what-eu-ets_en.html}
}

@book{wittpahlKuenstlicheIntelligenzTechnologie2019,
  title = {Künstliche Intelligenz: Technologie | Anwendung | Gesellschaft},
  shorttitle = {Künstliche Intelligenz},
  editor = {Wittpahl, Volker},
  date = {2019},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-58042-4},
  url = {http://link.springer.com/10.1007/978-3-662-58042-4},
  urldate = {2024-02-15},
  isbn = {978-3-662-58041-7 978-3-662-58042-4},
  langid = {ngerman},
  keywords = {Arbeitswelt 4.0,Breitbandausbau,Datenaufbereitung,Digitale Geschäftsmodelle,Digitales Lernen,Echtzeitvernetzung,Gesellschaftlicher Wandel,Industrie 4.0,Open Access,ungelesen,Wertschöpfung und Arbeitsmarkt,zitiert},
  file = {C:\Users\simon\Zotero\storage\IVHYC2WF\Künstliche Intelligenz.pdf}
}

@article{yaoMassivelyParallelAssociative2013,
  title = {A {{Massively Parallel Associative Memory Based}} on {{Sparse Neural Networks}}},
  author = {Yao, Zhe and Gripon, Vincent and Rabbat, Michael},
  date = {2013-03-27},
  abstract = {Associative memories store content in such a way that the content can be later retrieved by presenting the memory with a small portion of the content, rather than presenting the memory with an address as in more traditional memories. Associative memories are used as building blocks for algorithms within database engines, anomaly detection systems, compression algorithms, and face recognition systems. A classical example of an associative memory is the Hopfield neural network. Recently, Gripon and Berrou have introduced an alternative construction which builds on ideas from the theory of error correcting codes and which greatly outperforms the Hopfield network in capacity, diversity, and efficiency. In this paper we implement a variation of the Gripon-Berrou associative memory on a general purpose graphical processing unit (GPU). The work of Gripon and Berrou proposes two retrieval rules, sum-of-sum and sum-of-max. The sum-of-sum rule uses only matrix-vector multiplication and is easily implemented on the GPU. The sum-of-max rule is much less straightforward to implement because it involves non-linear operations. However, the sum-of-max rule gives significantly better retrieval error rates. We propose a hybrid rule tailored for implementation on a GPU which achieves a 880-fold speedup without sacrificing any accuracy.},
  file = {C:\Users\simon\Zotero\storage\F5FHS8MI\Yao et al_2013_A Massively Parallel Associative Memory Based on Sparse Neural Networks.pdf}
}

@inproceedings{zhaiDeepStructuredEnergy2016,
  title = {Deep {{Structured Energy Based Models}} for {{Anomaly Detection}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Zhai, Shuangfei and Cheng, Yu and Lu, Weining and Zhang, Zhongfei},
  date = {2016-06-11},
  pages = {1100--1109},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v48/zhai16.html},
  urldate = {2024-02-19},
  abstract = {In this paper, we attack the anomaly detection problem by directly modeling the data distribution with deep architectures. We hence propose deep structured energy based models (DSEBMs), where the energy function is the output of a deterministic deep neural network with structure. We develop novel model architectures to integrate EBMs with different types of data such as static data, sequential data, and spatial data, and apply appropriate model architectures to adapt to the data structure. Our training algorithm is built upon the recent development of score matching (Hyvarinen, 2005), which connects an EBM with a regularized autoencoder, eliminating the need for complicated sampling method. Statistically sound decision criterion can be derived for anomaly detection purpose from the perspective of the energy landscape of the data distribution. We investigate two decision criteria for performing anomaly detection: the energy score and the reconstruction error. Extensive empirical studies on benchmark anomaly detection tasks demonstrate that our proposed model consistently matches or outperforms all the competing methods.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\Y4E93TJV\Deep Structured Energy Based Models for Anomaly Detection.pdf}
}

@article{zhangOverviewRestrictedBoltzmann2018,
  title = {An Overview on {{Restricted Boltzmann Machines}}},
  author = {Zhang, Nan and Ding, Shifei and Zhang, Jian and Xue, Yu},
  date = {2018-01-31},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {275},
  pages = {1186--1199},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.09.065},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231217315849},
  urldate = {2024-02-15},
  abstract = {The Restricted Boltzmann Machine (RBM) has aroused wide interest in machine learning fields during the past decade. This review aims to report the recent developments in theoretical research and applications of the RBM. We first give an overview of the general RBM from the theoretical perspective, including stochastic approximation methods, stochastic gradient methods, and preventing overfitting methods. And then this review focuses on the RBM variants which further improve the learning ability of the RBM under general or specific applications. The RBM has recently been extended for representational learning, document modeling, multi-label learning, weakly supervised learning and many other tasks. The RBM and RBM variants provide powerful tools for representing dependency in the data, and they can be used as the basic building blocks to create deep networks. Apart from the Deep Belief Network (DBN) and the Deep Boltzmann Machine (DBM), the RBM can also be combined with the Convolutional Neural Network (CNN) to create deep networks. This review provides a comprehensive view of these advances in the RBM together with its future perspectives.},
  keywords = {Classification,Deep networks,Representational learning,Restricted Boltzmann Machine,ungelesen},
  file = {C:\Users\simon\Zotero\storage\739QIQAV\Zhang et al. - 2018 - An overview on Restricted Boltzmann Machines.pdf}
}

@article{zhouPhotonicMatrixMultiplication2022,
  title = {Photonic Matrix Multiplication Lights up Photonic Accelerator and Beyond},
  author = {Zhou, Hailong and Dong, Jianji and Cheng, Junwei and Dong, Wenchan and Huang, Chaoran and Shen, Yichen and Zhang, Qiming and Gu, Min and Qian, Chao and Chen, Hongsheng and Ruan, Zhichao and Zhang, Xinliang},
  date = {2022-02-03},
  journaltitle = {Light: Science \& Applications},
  shortjournal = {Light Sci Appl},
  volume = {11},
  number = {1},
  pages = {30},
  publisher = {Nature Publishing Group},
  issn = {2047-7538},
  doi = {10.1038/s41377-022-00717-8},
  url = {https://www.nature.com/articles/s41377-022-00717-8},
  urldate = {2024-03-18},
  abstract = {Matrix computation, as a fundamental building block of information processing in science and technology, contributes most of the computational overheads in modern signal processing and artificial intelligence algorithms. Photonic accelerators are designed to accelerate specific categories of computing in the optical domain, especially matrix multiplication, to address the growing demand for computing resources and capacity. Photonic matrix multiplication has much potential to expand the domain of telecommunication, and artificial intelligence benefiting from its superior performance. Recent research in photonic matrix multiplication has flourished and may provide opportunities to develop applications that are unachievable at present by conventional electronic processors. In this review, we first introduce the methods of photonic matrix multiplication, mainly including the plane light conversion method, Mach–Zehnder interferometer method and wavelength division multiplexing method. We also summarize the developmental milestones of photonic matrix multiplication and the related applications. Then, we review their detailed advances in applications to optical signal processing and artificial neural networks in recent years. Finally, we comment on the challenges and perspectives of photonic matrix multiplication and photonic acceleration.},
  langid = {english},
  keywords = {Integrated optics,Optoelectronic devices and components,Photonic devices},
  file = {C:\Users\simon\Zotero\storage\3KLKSXS8\Zhou et al_2022_Photonic matrix multiplication lights up photonic accelerator and beyond.pdf}
}

@article{zhuFutureDataCenter2023,
  title = {Future Data Center Energy-Conservation and Emission-Reduction Technologies in the Context of Smart and Low-Carbon City Construction},
  author = {Zhu, Hongyu and Zhang, Dongdong and Goh, Hui Hwang and Wang, Shuyao and Ahmad, Tanveer and Mao, Daijiafan and Liu, Tianhao and Zhao, Haisen and Wu, Thomas},
  date = {2023-02-01},
  journaltitle = {Sustainable Cities and Society},
  shortjournal = {Sustainable Cities and Society},
  volume = {89},
  pages = {104322},
  issn = {2210-6707},
  doi = {10.1016/j.scs.2022.104322},
  url = {https://www.sciencedirect.com/science/article/pii/S2210670722006266},
  urldate = {2024-04-26},
  abstract = {The energy consumption of data centers accounts for approximately 1\% of that of the world, the average power usage effectiveness is in the range of 1.4–1.6, and the associated carbon emissions account for approximately 2–4\% of the global carbon emissions. To reduce the energy consumption of data centers and promote smart, sustainable, and low-carbon city development, this study analyzes the energy conservation and emission-reduction technologies and potential decarbonization paths for data centers, compares the energy-saving situation of 20 typical data center cases, and highlights the impact of green data centers on the global carbon neutrality goal. The analysis reveals that data center energy consumption can be reduced by about 20–40\% and 15–27\% through IT equipment optimization and cooling technology improvements, respectively. Data center energy-saving strategies must consider differences in geographical location, natural resources, and economic bases. Therefore, this study examines the necessary steps for building zero-carbon data centers from the perspectives of public policy, technological innovation, and resource management. Specifically, the following aspects are explored: 1) accelerating the intelligent and unified management of data center resources; 2) building storage-computing integrated data centers that are compatible with heterogeneous resources and streamlined business models; 3) realizing large-scale and diversified use of clean energy in data centers.},
  keywords = {Carbon neutralization,Computing power,Data center,Energy-conservation and emission-reduction technology,Integration of storage and calculation,Smart and low-carbon city},
  file = {C:\Users\simon\Zotero\storage\9EZ48YI6\S2210670722006266.html}
}

@article{zioloHowDesignMore2019,
  title = {How to {{Design More Sustainable Financial Systems}}: {{The Roles}} of {{Environmental}}, {{Social}}, and {{Governance Factors}} in the {{Decision-Making Process}}},
  shorttitle = {How to {{Design More Sustainable Financial Systems}}},
  author = {Ziolo, Magdalena and Filipiak, Beata Zofia and Bąk, Iwona and Cheba, Katarzyna},
  date = {2019-01},
  journaltitle = {Sustainability},
  volume = {11},
  number = {20},
  pages = {5604},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2071-1050},
  doi = {10.3390/su11205604},
  url = {https://www.mdpi.com/2071-1050/11/20/5604},
  urldate = {2024-03-13},
  abstract = {A literature review showed that finance is a driver of sustainability. However, to achieve sustainability through finance, it is necessary to rebuild and adapt the financial system to the specifics of sustainable development. Modern financial systems can be described as one-dimensional, focusing on ensuring the economic security of transactions. Meanwhile, the growing role of risk related to non-financial factors means that the factors referred to as ESG (environmental, social, governance) become the main source threatening the stability of financial systems. Adaptation activities toward the design of so-called three-dimensional financial systems rely on incorporating ESG risk into the financial decisions of the financial institutions that make up the financial system. This is found, among other factors, in the risk assessment methodology. The general goal of the paper is to investigate which ESG criteria are incorporated into the decision-making process of financial institutions and to verify the level of sustainability of financial systems in selected OECD (Organization for Economic Cooperation and Development) countries. The main research hypothesis assumes that incorporating ESG factors into the decision-making process of financial institutions makes financial systems more sustainable. A two-stage research procedure was used to achieve the research goal. In the first stage, to determine the ESG factors that affect the level of sustainability of financial systems and identify dependencies between ESG factors incorporated by financial institutions into the decision-making process, a fuzzy cognitive map (FCM) was used. The collective map elaborating on the basis of the opinions of experts participating in the study was built using the software FCMapper\_bugfix\_27.1.2016. In the second stage, based on multiple-criteria decision analysis (MCDA) using the PROMETHEE method (Preference Ranking Organization Method of Enrichment Evaluation), 23 OECD countries that respect the Equator Principles were ranked according to seven groups of criteria defined for financial system assessment (financial depth, development, vulnerability, soundness, fragility, stability, and sustainability), based on a literature review. The ranking confirmed the strong position of Scandinavian countries for assuring best sustainability practices in financial institutions and in the economy. The added value of this paper can be considered at two levels: theoretical and empirical. From the theoretical point of view, it should be noted that it is the first of this kind of analysis which prioritizes ESG factors in financial decisions and ranks financial systems according to fulfilling sustainability criteria. The original empirical approach based on the two-stage research procedure provided analysis of 62 factors, of which 21 represented the environmental scope, 25 the social scope, and 16 the governance scope, which is the main advantage of the empirical study presented in the paper.},
  issue = {20},
  langid = {english},
  keywords = {ESG factors,fuzzy cognitive mapping,MCDA,PROMETHEE,risk,sustainable finance,sustainable financial systems},
  file = {C:\Users\simon\Zotero\storage\JCBAZLWL\Ziolo et al_2019_How to Design More Sustainable Financial Systems.pdf}
}
