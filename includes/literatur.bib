@article{ackleyLearningAlgorithmBoltzmann1985,
  title = {A Learning Algorithm for Boltzmann Machines},
  author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
  date = {1985-01-01},
  journaltitle = {Cognitive Science},
  shortjournal = {Cognitive Science},
  volume = {9},
  number = {1},
  pages = {147--169},
  issn = {0364-0213},
  doi = {10.1016/S0364-0213(85)80012-4},
  url = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
  urldate = {2024-02-16},
  abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\42UB7SBL\Ackley et al_1985_A learning algorithm for boltzmann machines.pdf}
}

@article{amariInformationGeometryBoltzmann1992,
  title = {Information Geometry of {{Boltzmann}} Machines},
  author = {Amari, S. and Kurata, K. and Nagaoka, H.},
  date = {1992-03},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {3},
  number = {2},
  pages = {260--271},
  issn = {1941-0093},
  doi = {10.1109/72.125867},
  url = {https://ieeexplore.ieee.org/abstract/document/125867},
  urldate = {2024-02-16},
  abstract = {A Boltzmann machine is a network of stochastic neurons. The set of all the Boltzmann machines with a fixed topology forms a geometric manifold of high dimension, where modifiable synaptic weights of connections play the role of a coordinate system to specify networks. A learning trajectory, for example, is a curve in this manifold. It is important to study the geometry of the neural manifold, rather than the behavior of a single network, in order to know the capabilities and limitations of neural networks of a fixed topology. Using the new theory of information geometry, a natural invariant Riemannian metric and a dual pair of affine connections on the Boltzmann neural network manifold are established. The meaning of geometrical structures is elucidated from the stochastic and the statistical point of view. This leads to a natural modification of the Boltzmann machine learning rule.{$<>$}},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Computer architecture,Information geometry,Information processing,Machine learning,Manifolds,Network topology,Neural networks,Neurons,Probability distribution,Stochastic processes,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\5Q29TWP2\Information_geometry_of_Boltzmann_machines.pdf}
}

@article{amirsoleimaniInMemoryVectorMatrixMultiplication2020,
  title = {In-{{Memory Vector-Matrix Multiplication}} in {{Monolithic Complementary Metal}}–{{Oxide}}–{{Semiconductor-Memristor Integrated Circuits}}: {{Design Choices}}, {{Challenges}}, and {{Perspectives}}},
  shorttitle = {In-{{Memory Vector-Matrix Multiplication}} in {{Monolithic Complementary Metal}}–{{Oxide}}–{{Semiconductor-Memristor Integrated Circuits}}},
  author = {Amirsoleimani, Amirali and Alibart, Fabien and Yon, Victor and Xu, Jianxiong and Pazhouhandeh, M. Reza and Ecoffey, Serge and Beilliard, Yann and Genov, Roman and Drouin, Dominique},
  date = {2020},
  journaltitle = {Advanced Intelligent Systems},
  volume = {2},
  number = {11},
  pages = {2000115},
  issn = {2640-4567},
  doi = {10.1002/aisy.202000115},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202000115},
  urldate = {2024-02-15},
  abstract = {The low communication bandwidth between memory and processing units in conventional von Neumann machines does not support the requirements of emerging applications that rely extensively on large sets of data. More recent computing paradigms, such as high parallelization and near-memory computing, help alleviate the data communication bottleneck to some extent, but paradigm-shifting concepts are required. In-memory computing has emerged as a prime candidate to eliminate this bottleneck by colocating memory and processing. In this context, resistive switching (RS) memory devices is a key promising choice, due to their unique intrinsic device-level properties, enabling both storing and computing with a small, massively-parallel footprint at low power. Theoretically, this directly translates to a major boost in energy efficiency and computational throughput, but various practical challenges remain. A qualitative and quantitative analysis of several key existing challenges in implementing high-capacity, high-volume RS memories for accelerating the most computationally demanding computation in machine learning (ML) inference, that of vector-matrix multiplication (VMM), is presented. The monolithic integration of RS memories with complementary metal–oxide–semiconductor (CMOS) integrated circuits is presented as the core underlying technology. The key existing design choices in terms of device-level physical implementation, circuit-level design, and system-level considerations is reviewed and an outlook for future directions is provided.},
  langid = {english},
  keywords = {complementary metal–oxide–semiconductor,in-memory computing,inference,memristors,redox-based random access memories,resistive switching memories,ungelesen,vector-matrix multiplications},
  file = {C:\Users\simon\Zotero\storage\SZ37ATSG\In‐Memory Vector‐Matrix Multiplication in Monolithic Complementary.pdf}
}

@article{barraEquivalenceHopfieldNetworks2012,
  title = {On the Equivalence of {{Hopfield}} Networks and {{Boltzmann Machines}}},
  author = {Barra, Adriano and Bernacchia, Alberto and Santucci, Enrica and Contucci, Pierluigi},
  date = {2012-10-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {34},
  pages = {1--9},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.06.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608012001608},
  urldate = {2024-02-16},
  abstract = {A specific type of neural networks, the Restricted Boltzmann Machines (RBM), are implemented for classification and feature detection in machine learning. They are characterized by separate layers of visible and hidden units, which are able to learn efficiently a generative model of the observed data. We study a “hybrid” version of RBMs, in which hidden units are analog and visible units are binary, and we show that thermodynamics of visible units are equivalent to those of a Hopfield network, in which the N visible units are the neurons and the P hidden units are the learned patterns. We apply the method of stochastic stability to derive the thermodynamics of the model, by considering a formal extension of this technique to the case of multiple sets of stored patterns, which may act as a benchmark for the study of correlated sets. Our results imply that simulating the dynamics of a Hopfield network, requiring the update of N neurons and the storage of N(N−1)/2 synapses, can be accomplished by a hybrid Boltzmann Machine, requiring the update of N+P neurons but the storage of only NP synapses. In addition, the well known glass transition of the Hopfield network has a counterpart in the Boltzmann Machine: it corresponds to an optimum criterion for selecting the relative sizes of the hidden and visible layers, resolving the trade-off between flexibility and generality of the model. The low storage phase of the Hopfield model corresponds to few hidden units and hence a overly constrained RBM, while the spin-glass phase (too many hidden units) corresponds to unconstrained RBM prone to overfitting of the observed data.},
  keywords = {Boltzmann Machines,Hopfield networks,Statistical mechanics,ungelesen},
  file = {C\:\\Users\\simon\\Zotero\\storage\\FBU8CDS7\\On the equivalence of Hopfield networks and Boltzmann Machines.pdf;C\:\\Users\\simon\\Zotero\\storage\\NFHUABGB\\S0893608012001608.html}
}

@article{bohmNoiseinjectedAnalogIsing2022,
  title = {Noise-Injected Analog {{Ising}} Machines Enable Ultrafast Statistical Sampling and Machine Learning},
  author = {Böhm, Fabian and Alonso-Urquijo, Diego and Verschaffelt, Guy and Van der Sande, Guy},
  date = {2022-10-04},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {5847},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-33441-3},
  url = {https://www.nature.com/articles/s41467-022-33441-3},
  urldate = {2024-02-15},
  abstract = {Ising machines are a promising non-von-Neumann computational concept for neural network training and combinatorial optimization. However, while various neural networks can be implemented with Ising machines, their inability to perform fast statistical sampling makes them inefficient for training neural networks compared to digital computers. Here, we introduce a universal concept to achieve ultrafast statistical sampling with analog Ising machines by injecting noise. With an opto-electronic Ising machine, we experimentally demonstrate that this can be used for accurate sampling of Boltzmann distributions and for unsupervised training of neural networks, with equal accuracy as software-based training. Through simulations, we find that Ising machines can perform statistical sampling orders-of-magnitudes faster than software-based methods. This enables the use of Ising machines beyond combinatorial optimization and makes them into efficient tools for machine learning and other applications.},
  issue = {1},
  langid = {english},
  keywords = {Complex networks,Computer science,Information theory and computation,Optoelectronic devices and components,ungelesen},
  file = {C:\Users\simon\Zotero\storage\CUQQWVHL\Noise-injected analog Ising machines enable ultrafast statistical sampling and machine learning.pdf}
}

@article{bohmNoiseinjectedAnalogIsing2022a,
  title = {Noise-Injected Analog {{Ising}} Machines Enable Ultrafast Statistical Sampling and Machine Learning},
  author = {Böhm, Fabian and Alonso-Urquijo, Diego and Verschaffelt, Guy and Van der Sande, Guy},
  date = {2022-10-04},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {5847},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-33441-3},
  url = {https://www.nature.com/articles/s41467-022-33441-3},
  urldate = {2024-02-15},
  abstract = {Ising machines are a promising non-von-Neumann computational concept for neural network training and combinatorial optimization. However, while various neural networks can be implemented with Ising machines, their inability to perform fast statistical sampling makes them inefficient for training neural networks compared to digital computers. Here, we introduce a universal concept to achieve ultrafast statistical sampling with analog Ising machines by injecting noise. With an opto-electronic Ising machine, we experimentally demonstrate that this can be used for accurate sampling of Boltzmann distributions and for unsupervised training of neural networks, with equal accuracy as software-based training. Through simulations, we find that Ising machines can perform statistical sampling orders-of-magnitudes faster than software-based methods. This enables the use of Ising machines beyond combinatorial optimization and makes them into efficient tools for machine learning and other applications.},
  issue = {1},
  langid = {english},
  keywords = {Complex networks,Computer science,Information theory and computation,Optoelectronic devices and components,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\JC236XT8\Noise-injected analog Ising machines enable ultrafast statistical sampling and machine learning.pdf}
}

@online{caiHarnessingIntrinsicNoise2019,
  title = {Harnessing {{Intrinsic Noise}} in {{Memristor Hopfield Neural Networks}} for {{Combinatorial Optimization}}},
  author = {Cai, Fuxi and Kumar, Suhas and Van Vaerenbergh, Thomas and Liu, Rui and Li, Can and Yu, Shimeng and Xia, Qiangfei and Yang, J. Joshua and Beausoleil, Raymond and Lu, Wei and Strachan, John Paul},
  date = {2019-04-03},
  eprint = {1903.11194},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1903.11194},
  url = {http://arxiv.org/abs/1903.11194},
  urldate = {2024-02-15},
  abstract = {We describe a hybrid analog-digital computing approach to solve important combinatorial optimization problems that leverages memristors (two-terminal nonvolatile memories). While previous memristor accelerators have had to minimize analog noise effects, we show that our optimization solver harnesses such noise as a computing resource. Here we describe a memristor-Hopfield Neural Network (mem-HNN) with massively parallel operations performed in a dense crossbar array. We provide experimental demonstrations solving NP-hard max-cut problems directly in analog crossbar arrays, and supplement this with experimentally-grounded simulations to explore scalability with problem size, providing the success probabilities, time and energy to solution, and interactions with intrinsic analog noise. Compared to fully digital approaches, and present-day quantum and optical accelerators, we forecast the mem-HNN to have over four orders of magnitude higher solution throughput per power consumption. This suggests substantially improved performance and scalability compared to current quantum annealing approaches, while operating at room temperature and taking advantage of existing CMOS technology augmented with emerging analog non-volatile memristors.},
  pubstate = {preprint},
  keywords = {Computer Science - Emerging Technologies,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\FLZE6J2J\Cai et al. - 2019 - Harnessing Intrinsic Noise in Memristor Hopfield N.pdf}
}

@article{clinchGrandChallenges2022,
  title = {Grand {{Challenges}}},
  author = {Clinch, Sarah and Intille, Stephen},
  date = {2022-07-01},
  journaltitle = {IEEE Pervasive Computing},
  volume = {21},
  number = {03},
  pages = {7--8},
  publisher = {{IEEE Computer Society}},
  issn = {1536-1268},
  doi = {10.1109/MPRV.2022.3198813},
  url = {https://www.computer.org/csdl/magazine/pc/2022/03/09903273/1GZo7D5rB8A},
  urldate = {2024-02-15},
  abstract = {The articles in this special section focus on new applications for pervasive computing.},
  langid = {english},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\VZMMDAK3\Grand Challenges.pdf}
}

@online{darioamodeiAICompute,
  title = {{{AI}} and Compute},
  author = {{Dario Amodei} and {Danny Hernandez}},
  url = {https://openai.com/research/ai-and-compute},
  urldate = {2024-02-15},
  abstract = {We’re releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore’s Law had a 2-year doubling period)[\^footnote-correction]. Since 2012, this metric has grown by more than 300,000x (a 2-year doubling period would yield only a 7x increase). Improvements in compute have been a key component of AI progress, so as long as this trend continues, it’s worth preparing for the implications of systems far outside today’s capabilities.},
  langid = {american},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\LW5E6CXF\ai-and-compute.html}
}

@book{fahlmanMassivelyParallelArchitectures1983,
  title = {Massively {{Parallel Architectures}} for {{AI}}: {{NETL}}, {{Thistle}}, and {{Boltzmann Machines}}.},
  shorttitle = {Massively {{Parallel Architectures}} for {{AI}}},
  author = {Fahlman, Scott and Hinton, Geoffrey and Sejnowski, Terrence},
  date = {1983-01-01},
  journaltitle = {[No source information available]},
  pages = {113},
  abstract = {ABSTRACT It is becoming,increasingly apparent that some,aspects of intelligent behavior require enormous ,computational power,and,that some,sort of massively parallel computing architecture is the most plausible way to deliver sueh power. Parallelism, rather than raw speed of the computing,elements. seems,to be the way that the brain gets such jobs done. But even if the need for massive parallelism is admitted, there is still the question of what kind of parallel architecture best fits the needs of various A1 tasks. In this paper we will attempt to isolate a number(\$\#\$\#\$\#CommaToBean intelligent system must perform. We will describe several families of massively parallel computing architectures, and we will see which of these computational tasks can be handled by each of these families. In particular, we will describe a new architecture, which we call the Boltzmann machine, whose abilities appear to include a number,of tasks that are inefficient},
  pagetotal = {109},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\5QXANLI6\1983 - Massively Parallel Architectures for Al NETL, Thistle, and Boltzmann Machines.pdf}
}

@incollection{gerthKuenstlicheIntelligenzZwischen2023,
  title = {Künstliche Intelligenz zwischen Utopie und Realität: Aktuelle und zukünftige Entwicklungen von KI am Beispiel von Human-Machine-Interaction, Blockchain, Green Tech und Mobilität},
  shorttitle = {Künstliche Intelligenz zwischen Utopie und Realität},
  booktitle = {Entrepreneurship der Zukunft: Voraussetzung, Implementierung und Anwendung von Künstlicher Intelligenz im Rahmen datenbasierter Geschäftsmodelle},
  author = {Gerth, Sebastian and Heim, Lars},
  editor = {Heim, Lars and Gerth, Sebastian},
  date = {2023},
  pages = {421--458},
  publisher = {{Springer Fachmedien}},
  location = {{Wiesbaden}},
  doi = {10.1007/978-3-658-42060-4_17},
  url = {https://doi.org/10.1007/978-3-658-42060-4_17},
  urldate = {2024-02-15},
  abstract = {Der vorliegende Beitrag wagt auf der Basis vergangener und aktueller Entwicklungen von Künstlicher Intelligenz (KI) einen Ausblick auf zukünftige Leistungen von KI in spezifischen Gebieten. Es wird argumentiert, dass KI ein eigenständiger Megatrend mit zahlreichen Auswirkungen auf die Gesellschaft und v.~a. auch auf die Wirtschaft in unterschiedlichen Bereichen und Ebenen mit zahlreichen Anknüpfungspunkten diverser Geschäftsbereiche und -modelle ist. Es wird gezeigt, dass die wesentliche Stärke von KI einerseits die exakte Analyse und das zur Erreichung von vordefinierten Zielen optimale Inbeziehungsetzen spezifischer Parameter ist. Andererseits vermag KI bestimmte Prognosen zu fundieren. Aus diesem Grund wird der Chatbot ChatGPT des Entwicklers OpenAI zur Zukunft von KI befragt – eine KI trifft also Vorhersagen über KI. Dieser vielversprechende Ansatz wird dennoch kritisch reflektiert und der Beitrag mit einer Beleuchtung von starker und schwacher KI im Unternehmenskontext sowie einer prüfenden Deskription der bevorstehenden KI-Evolution abgeschlossen.},
  isbn = {978-3-658-42060-4},
  langid = {ngerman},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\F8N7CZ5G\Künstliche Intelligenz zwischen Utopie und Realitä.pdf}
}

@online{hernandezMeasuringAlgorithmicEfficiency2020,
  title = {Measuring the {{Algorithmic Efficiency}} of {{Neural Networks}}},
  author = {Hernandez, Danny and Brown, Tom B.},
  date = {2020-05-08},
  eprint = {2005.04305},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2005.04305},
  url = {http://arxiv.org/abs/2005.04305},
  urldate = {2024-02-15},
  abstract = {Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning,ungelesen},
  file = {C:\Users\simon\Zotero\storage\S692LSEN\Measuring the Algorithmic Efficiency of Neural Networks.pdf}
}

@book{hintemannDataCenters20212022,
  title = {Data Centers 2021: {{Data}} Center Boom in {{Germany}} Continues - {{Cloud}} Computing Drives the Growth of the Data Center Industry and Its Energy Consumption},
  shorttitle = {Data Centers 2021},
  author = {Hintemann, Ralph and Hinterholzer, Simon},
  date = {2022-08-05},
  doi = {10.13140/RG.2.2.31826.43207},
  abstract = {The energy consumption of data centers continues to increase. At 17 billion kWh, data centers consumed 6.5 \% more electricity in 2021 than in 2020. The main reason for the growth in energy consumption is the expansion of cloud data centers in Germany and the associated increase in the number of large data centers. However, traditional data centers operated by companies themselves also continue to have a high share of data center capacities in Germany.},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\8S6LNT75\Data centers 2021 Data center boom in Germany continues.pdf}
}

@incollection{hintonPracticalGuideTraining2012,
  title = {A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {Hinton, Geoffrey E.},
  editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  date = {2012},
  volume = {7700},
  pages = {599--619},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_32},
  url = {http://link.springer.com/10.1007/978-3-642-35289-8_32},
  urldate = {2024-02-15},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\HGALEWG5\Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf}
}

@incollection{hintonPracticalGuideTraining2012a,
  title = {A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {Hinton, Geoffrey E.},
  editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  date = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {599--619},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_32},
  url = {https://doi.org/10.1007/978-3-642-35289-8_32},
  urldate = {2024-02-15},
  abstract = {Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.},
  isbn = {978-3-642-35289-8},
  langid = {english},
  keywords = {gelesen,Hide Unit,Learning Rate,Reconstruction Error,Restrict Boltzmann Machine,Training Case,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\VZ5ZDZTH\Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf}
}

@online{hizzaniMemristorbasedHardwareAlgorithms2023,
  title = {Memristor-Based Hardware and Algorithms for Higher-Order {{Hopfield}} Optimization Solver Outperforming Quadratic {{Ising}} Machines},
  author = {Hizzani, Mohammad and Heittmann, Arne and Hutchinson, George and Dobrynin, Dmitrii and Van Vaerenbergh, Thomas and Bhattacharya, Tinish and Renaudineau, Adrien and Strukov, Dmitri and Strachan, John Paul},
  date = {2023-11-02},
  eprint = {2311.01171},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.01171},
  url = {http://arxiv.org/abs/2311.01171},
  urldate = {2024-02-15},
  abstract = {Ising solvers offer a promising physics-based approach to tackle the challenging class of combinatorial optimization problems. However, typical solvers operate in a quadratic energy space, having only pair-wise coupling elements which already dominate area and energy. We show that such quadratization can cause severe problems: increased dimensionality, a rugged search landscape, and misalignment with the original objective function. Here, we design and quantify a higher-order Hopfield optimization solver, with 28nm CMOS technology and memristive couplings for lower area and energy computations. We combine algorithmic and circuit analysis to show quantitative advantages over quadratic Ising Machines (IM)s, yielding 48x and 72x reduction in time-to-solution (TTS) and energy-to-solution (ETS) respectively for Boolean satisfiability problems of 150 variables, with favorable scaling.},
  pubstate = {preprint},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Hardware Architecture,ungelesen},
  file = {C:\Users\simon\Zotero\storage\UWNBRWXG\Hizzani et al. - 2023 - Memristor-based hardware and algorithms for higher.pdf}
}

@online{luccioniPowerHungryProcessing2023,
  title = {Power {{Hungry Processing}}: {{Watts Driving}} the {{Cost}} of {{AI Deployment}}?},
  shorttitle = {Power {{Hungry Processing}}},
  author = {Luccioni, Alexandra Sasha and Jernite, Yacine and Strubell, Emma},
  date = {2023-11-28},
  eprint = {2311.16863},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.16863},
  url = {http://arxiv.org/abs/2311.16863},
  urldate = {2024-02-15},
  abstract = {Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of "generality" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\66VJIJMH\Power Hungry Processing Watts Driving the Cost of AI Deployment.pdf}
}

@online{mohseniIsingMachinesHardware2022,
  title = {Ising Machines as Hardware Solvers of Combinatorial Optimization Problems},
  author = {Mohseni, Naeimeh and McMahon, Peter L. and Byrnes, Tim},
  date = {2022-04-01},
  eprint = {2204.00276},
  eprinttype = {arxiv},
  eprintclass = {physics, physics:quant-ph},
  doi = {10.48550/arXiv.2204.00276},
  url = {http://arxiv.org/abs/2204.00276},
  urldate = {2024-02-15},
  abstract = {Ising machines are hardware solvers which aim to find the absolute or approximate ground states of the Ising model. The Ising model is of fundamental computational interest because it is possible to formulate any problem in the complexity class NP as an Ising problem with only polynomial overhead. A scalable Ising machine that outperforms existing standard digital computers could have a huge impact for practical applications for a wide variety of optimization problems. In this review, we survey the current status of various approaches to constructing Ising machines and explain their underlying operational principles. The types of Ising machines considered here include classical thermal annealers based on technologies such as spintronics, optics, memristors, and digital hardware accelerators; dynamical-systems solvers implemented with optics and electronics; and superconducting-circuit quantum annealers. We compare and contrast their performance using standard metrics such as the ground-state success probability and time-to-solution, give their scaling relations with problem size, and discuss their strengths and weaknesses.},
  pubstate = {preprint},
  keywords = {Physics - Applied Physics,Quantum Physics,ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\VWIC5SI6\Ising machines Hardware solvers for combinatorial.pdf}
}

@online{montufarRestrictedBoltzmannMachines2018,
  title = {Restricted {{Boltzmann Machines}}: {{Introduction}} and {{Review}}},
  shorttitle = {Restricted {{Boltzmann Machines}}},
  author = {Montufar, Guido},
  date = {2018-06-19},
  eprint = {1806.07066},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.1806.07066},
  url = {http://arxiv.org/abs/1806.07066},
  urldate = {2024-02-15},
  abstract = {The restricted Boltzmann machine is a network of stochastic units with undirected interactions between pairs of visible and hidden units. This model was popularized as a building block of deep learning architectures and has continued to play an important role in applied and theoretical machine learning. Restricted Boltzmann machines carry a rich structure, with connections to geometry, applied algebra, probability, statistics, machine learning, and other areas. The analysis of these models is attractive in its own right and also as a platform to combine and generalize mathematical tools for graphical models with hidden variables. This article gives an introduction to the mathematical analysis of restricted Boltzmann machines, reviews recent results on the geometry of the sets of probability distributions representable by these models, and suggests a few directions for further investigation.},
  pubstate = {preprint},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Machine Learning,ungelesen},
  file = {C:\Users\simon\Zotero\storage\3DLWASIX\Montufar - 2018 - Restricted Boltzmann Machines Introduction and Re.pdf}
}

@book{nazmbojnordiMemristiveBoltzmannMachine2016,
  title = {Memristive {{Boltzmann}} Machine: {{A}} Hardware Accelerator for Combinatorial Optimization and Deep Learning},
  shorttitle = {Memristive {{Boltzmann}} Machine},
  author = {Nazm Bojnordi, Mahdi and Ipek, Engin},
  date = {2016-03-01},
  pages = {13},
  doi = {10.1109/HPCA.2016.7446049},
  pagetotal = {1},
  keywords = {ungelesen,zitiert},
  file = {C:\Users\simon\Zotero\storage\A9DK674U\Memristive Boltzmann machine A hardware accelerator for combinatorial optimization and deep learning.pdf}
}

@inproceedings{NIPS2000_1f1baa5b,
  title = {Recognizing Hand-Written Digits Using Hierarchical Products of Experts},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mayraz, Guy and Hinton, Geoffrey E},
  editor = {Leen, T. and Dietterich, T. and Tresp, V.},
  date = {2000},
  volume = {13},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\J6D4HNWV\Mayraz und Hinton - 2000 - Recognizing hand-written digits using hierarchical.pdf}
}

@book{pandianIntelligentComputingOptimization,
  title = {Intelligent {{Computing}} and {{Optimization}}},
  author = {Pandian, Vasant and Vladimir Panchenko},
  url = {https://link.springer.com/book/9783031508868},
  urldate = {2024-02-04},
  abstract = {This book covers the 7th edition of International Conference on Intelligent Computing and Optimization took place at Baitong Hotel \& Resort},
  langid = {english},
  file = {C:\Users\simon\Zotero\storage\WPJ3SWAN\9783031508868.html}
}

@online{pedrettiXTIMEInmemoryEngine2024,
  title = {X-{{TIME}}: {{An}} in-Memory Engine for Accelerating Machine Learning on Tabular Data with {{CAMs}}},
  shorttitle = {X-{{TIME}}},
  author = {Pedretti, Giacomo and Moon, John and Bruel, Pedro and Serebryakov, Sergey and Roth, Ron M. and Buonanno, Luca and Gajjar, Archit and Ziegler, Tobias and Xu, Cong and Foltin, Martin and Faraboschi, Paolo and Ignowski, Jim and Graves, Catherine E.},
  date = {2024-02-02},
  eprint = {2304.01285},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.01285},
  url = {http://arxiv.org/abs/2304.01285},
  urldate = {2024-02-15},
  abstract = {Structured, or tabular, data is the most common format in data science. While deep learning models have proven formidable in learning from unstructured data such as images or speech, they are less accurate than simpler approaches when learning from tabular data. In contrast, modern tree-based Machine Learning (ML) models shine in extracting relevant information from structured data. An essential requirement in data science is to reduce model inference latency in cases where, for example, models are used in a closed loop with simulation to accelerate scientific discovery. However, the hardware acceleration community has mostly focused on deep neural networks and largely ignored other forms of machine learning. Previous work has described the use of an analog content addressable memory (CAM) component for efficiently mapping random forests. In this work, we focus on an overall analog-digital architecture implementing a novel increased precision analog CAM and a programmable network on chip allowing the inference of state-of-the-art tree-based ML models, such as XGBoost and CatBoost. Results evaluated in a single chip at 16nm technology show 119x lower latency at 9740x higher throughput compared with a state-of-the-art GPU, with a 19W peak power consumption.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,ungelesen},
  file = {C:\Users\simon\Zotero\storage\AG7EV9DE\Pedretti et al. - 2024 - X-TIME An in-memory engine for accelerating machi.pdf}
}

@inproceedings{salakhutdinovDeepBoltzmannMachines2009,
  title = {Deep {{Boltzmann Machines}}},
  booktitle = {Proceedings of the {{Twelth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  date = {2009-04-15},
  pages = {448--455},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v5/salakhutdinov09a.html},
  urldate = {2024-02-16},
  abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\V8LJQ6Z2\Deep Boltzmann Machines.pdf}
}

@article{sarkerMachineLearningAlgorithms2021,
  title = {Machine {{Learning}}: {{Algorithms}}, {{Real-World Applications}} and {{Research Directions}}},
  shorttitle = {Machine {{Learning}}},
  author = {Sarker, Iqbal H.},
  date = {2021-03-22},
  journaltitle = {SN Computer Science},
  shortjournal = {SN COMPUT. SCI.},
  volume = {2},
  number = {3},
  pages = {160},
  issn = {2661-8907},
  doi = {10.1007/s42979-021-00592-x},
  url = {https://doi.org/10.1007/s42979-021-00592-x},
  urldate = {2024-02-04},
  abstract = {In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated~applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study’s key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity~systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers~in various real-world situations and~application areas, particularly from the technical point of view.},
  langid = {english},
  keywords = {Artificial intelligence,Data science,Data-driven decision-making,Deep learning,Intelligent applications,Machine learning,Predictive analytics},
  file = {C:\Users\simon\Zotero\storage\2FB2ANZT\Sarker - 2021 - Machine Learning Algorithms, Real-World Applicati.pdf}
}

@article{sungPerspectiveReviewMemristive2018,
  title = {Perspective: {{A}} Review on Memristive Hardware for Neuromorphic Computation},
  shorttitle = {Perspective},
  author = {Sung, Changhyuck and Hwang, Hyunsang and Yoo, In Kyeong},
  date = {2018-10-05},
  journaltitle = {Journal of Applied Physics},
  shortjournal = {Journal of Applied Physics},
  volume = {124},
  number = {15},
  pages = {151903},
  issn = {0021-8979},
  doi = {10.1063/1.5037835},
  url = {https://doi.org/10.1063/1.5037835},
  urldate = {2024-02-15},
  abstract = {Neuromorphic computation is one of the axes of parallel distributed processing, and memristor-based synaptic weight is considered as a key component of this type of computation. However, the material properties of memristors, including material related physics, are not yet matured. In parallel with memristors, CMOS based Graphics Processing Unit, Field Programmable Gate Array, and Application Specific Integrated Circuit are also being developed as dedicated artificial intelligence (AI) chips for fast computation. Therefore, it is necessary to analyze the competitiveness of the memristor-based neuromorphic device in order to position the memristor in the appropriate position of the future AI ecosystem. In this article, the status of memristor-based neuromorphic computation was analyzed on the basis of papers and patents to identify the competitiveness of the memristor properties by reviewing industrial trends and academic pursuits. In addition, material issues and challenges are discussed for implementing the memristor-based neural processor.},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\78YATYXK\Sung et al. - 2018 - Perspective A review on memristive hardware for n.pdf}
}

@article{upadhyaOverviewRestrictedBoltzmann2019,
  title = {An {{Overview}} of {{Restricted Boltzmann Machines}}},
  author = {Upadhya, Vidyadhar and Sastry, P.},
  date = {2019-02-18},
  journaltitle = {Journal of the Indian Institute of Science},
  shortjournal = {Journal of the Indian Institute of Science},
  volume = {99},
  doi = {10.1007/s41745-019-0102-z},
  abstract = {The restricted Boltzmann machine (RBM) is a two-layered network of stochastic units with undirected connections between pairs of units in the two layers. The two layers of nodes are called visible and hidden nodes. In an RBM, there are no connections from visible to visible or hidden to hidden nodes. RBMs are used mainly as a generative model. They can be suitably modified to perform classification tasks also. They are among the basic building blocks of other deep learning models such as deep Boltzmann machine and deep belief networks. The aim of this article is to give a tutorial introduction to the restricted Boltzmann machines and to review the evolution of this model.},
  keywords = {ungelesen},
  file = {C:\Users\simon\Zotero\storage\JAGD4W2N\An Overview of Restricted Boltzmann Machines.pdf}
}

@online{wangOscillatorbasedIsingMachine2017,
  title = {Oscillator-Based {{Ising Machine}}},
  author = {Wang, Tianshi and Roychowdhury, Jaijeet},
  date = {2017-10-12},
  eprint = {1709.08102},
  eprinttype = {arxiv},
  eprintclass = {physics},
  doi = {10.48550/arXiv.1709.08102},
  url = {http://arxiv.org/abs/1709.08102},
  urldate = {2024-02-15},
  abstract = {Many combinatorial optimization problems can be mapped to finding the ground states of the corresponding Ising Hamiltonians. The physical systems that can solve optimization problems in this way, namely Ising machines, have been attracting more and more attention recently. Our work shows that Ising machines can be realized using almost any nonlinear self-sustaining oscillators with logic values encoded in their phases. Many types of such oscillators are readily available for large-scale integration, with potentials in high-speed and low-power operation. In this paper, we describe the operation and mechanism of oscillator-based Ising machines. The feasibility of our scheme is demonstrated through several examples in simulation and hardware, among which a simulation study reports average solutions exceeding those from state-of-art Ising machines on a benchmark combinatorial optimization problem of size 2000.},
  pubstate = {preprint},
  keywords = {Computer Science - Emerging Technologies,Physics - Computational Physics,ungelesen},
  file = {C:\Users\simon\Zotero\storage\Y6S4QDT9\Oscillator-based Ising Machine.pdf}
}

@book{wittpahlKuenstlicheIntelligenzTechnologie2019,
  title = {Künstliche Intelligenz: Technologie | Anwendung | Gesellschaft},
  shorttitle = {Künstliche Intelligenz},
  editor = {Wittpahl, Volker},
  date = {2019},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-58042-4},
  url = {http://link.springer.com/10.1007/978-3-662-58042-4},
  urldate = {2024-02-15},
  isbn = {978-3-662-58041-7 978-3-662-58042-4},
  langid = {ngerman},
  keywords = {Arbeitswelt 4.0,Breitbandausbau,Datenaufbereitung,Digitale Geschäftsmodelle,Digitales Lernen,Echtzeitvernetzung,Gesellschaftlicher Wandel,Industrie 4.0,Open Access,ungelesen,Wertschöpfung und Arbeitsmarkt,zitiert},
  file = {C:\Users\simon\Zotero\storage\IVHYC2WF\Künstliche Intelligenz.pdf}
}

@article{zhangOverviewRestrictedBoltzmann2018,
  title = {An Overview on {{Restricted Boltzmann Machines}}},
  author = {Zhang, Nan and Ding, Shifei and Zhang, Jian and Xue, Yu},
  date = {2018-01-31},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {275},
  pages = {1186--1199},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.09.065},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231217315849},
  urldate = {2024-02-15},
  abstract = {The Restricted Boltzmann Machine (RBM) has aroused wide interest in machine learning fields during the past decade. This review aims to report the recent developments in theoretical research and applications of the RBM. We first give an overview of the general RBM from the theoretical perspective, including stochastic approximation methods, stochastic gradient methods, and preventing overfitting methods. And then this review focuses on the RBM variants which further improve the learning ability of the RBM under general or specific applications. The RBM has recently been extended for representational learning, document modeling, multi-label learning, weakly supervised learning and many other tasks. The RBM and RBM variants provide powerful tools for representing dependency in the data, and they can be used as the basic building blocks to create deep networks. Apart from the Deep Belief Network (DBN) and the Deep Boltzmann Machine (DBM), the RBM can also be combined with the Convolutional Neural Network (CNN) to create deep networks. This review provides a comprehensive view of these advances in the RBM together with its future perspectives.},
  keywords = {Classification,Deep networks,Representational learning,Restricted Boltzmann Machine,ungelesen},
  file = {C:\Users\simon\Zotero\storage\739QIQAV\Zhang et al. - 2018 - An overview on Restricted Boltzmann Machines.pdf}
}
